{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "ae40a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai\n",
    "from google.oauth2 import service_account\n",
    "from google.auth import load_credentials_from_file\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber  # for improved OCR if needed\n",
    "import timeit\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "import tiktoken  # OpenAI's tokenization library\n",
    "import json\n",
    "import openai\n",
    "from googlesearch import search\n",
    "import unicodedata\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907a022",
   "metadata": {},
   "source": [
    "<h1>Chunk data from lecture presentations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b45ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_pdf(input_pdf_path,file_name, max_pages=1):\n",
    "    \"\"\"\n",
    "    Split a PDF into smaller chunks of max_pages.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(input_pdf_path)\n",
    "    chunks = []\n",
    "    for i in range(0, len(reader.pages), max_pages):\n",
    "        writer = PdfWriter()\n",
    "        for j in range(i, min(i + max_pages, len(reader.pages))):\n",
    "            writer.add_page(reader.pages[j])\n",
    "        chunk_path = f\"./chunks/chunk_{i // max_pages + 1}_{file_name.split('.')[0]}.pdf\"\n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            writer.write(f)\n",
    "        chunks.append(chunk_path)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "163d817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file('coms-6998-applied-llm-class-4e98f4f7a361.json')\n",
    "client = documentai.DocumentProcessorServiceClient(credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a708ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_to_extract_data_from = os.listdir('./lecture_pdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "21a5361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "for file_name in all_files_to_extract_data_from:\n",
    "    file_directory = \"./lecture_pdfs\"\n",
    "    pdf_path = os.path.join(file_directory, file_name)\n",
    "    chunks = split_pdf(pdf_path,file_name)\n",
    "    all_chunks = all_chunks + chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21d848",
   "metadata": {},
   "source": [
    "<h1>Extract text and links from chunks from lectures</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6c8f9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_links(text):\n",
    "    links = []\n",
    "    text = text.replace('-\\n',\"\")\n",
    "    page_links = re.findall(r'(https?://\\S+)', text)\n",
    "    links.extend(page_links)\n",
    "    page_links = re.findall(r'(http?://\\S+)', text)\n",
    "    links.extend(page_links)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "475cff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_extraction(file_name,project_id = \"coms-6998-applied-llm-class\",location = \"us\",processor_id = \"398fd74279aa6748\"):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    raw_document = documentai.RawDocument(content=content, mime_type=\"application/pdf\")\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "    # Make the request\n",
    "    request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
    "    response = client.process_document(request=request)\n",
    "    document = response.document\n",
    "    text = document.text\n",
    "    links = extract_text_links(text)\n",
    "    return text, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5f8370b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_clean_text(url):\n",
    "    \"\"\"\n",
    "    Fetches and cleans text from the given URL.\n",
    "    :param url: The URL to fetch text from.\n",
    "    :return: Cleaned text or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make an HTTP GET request\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract the main text content\n",
    "        # We can focus on specific tags (e.g., <p>, <div>) or use the whole text\n",
    "        text_elements = soup.find_all([\"p\", \"div\"])\n",
    "        text = \" \".join(element.get_text() for element in text_elements)\n",
    "        \n",
    "        # Clean the text\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "        text = text.strip()  # Remove leading/trailing whitespace\n",
    "        \n",
    "        # Handle empty text scenario\n",
    "        if not text:\n",
    "            return f\"Error: No extractable text found at {url}\"\n",
    "        return text\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle HTTP and connection errors\n",
    "        return f\"Error: Unable to fetch content from {url}. Exception: {e}\"\n",
    "    except Exception as e:\n",
    "        # Handle other unexpected errors\n",
    "        return f\"Error: Unexpected error while processing {url}. Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d9d38c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_links(links):\n",
    "    \"\"\"\n",
    "    Processes a list of links, extracting and cleaning text content.\n",
    "    :param links: List of URLs.\n",
    "    :return: Dictionary with URLs as keys and cleaned text (or error messages) as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for url in links:\n",
    "        print(f\"Processing: {url}\")\n",
    "        text = fetch_and_clean_text(url)\n",
    "        results[url] = text\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3e78e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts_with_links = [value['text'] for key,value in all_data.items() if len(value['links'])>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "61de4ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 6.449538166999446 ./chunks/chunk_46_Lecture-12-Columbia.pdf\n",
      "50 7.764267583000219 ./chunks/chunk_51_Lecture-12-Columbia.pdf\n",
      "55 7.30011899999954 ./chunks/chunk_56_Lecture-12-Columbia.pdf\n",
      "60 8.476970249999795 ./chunks/chunk_61_Lecture-12-Columbia.pdf\n",
      "65 8.502008792000197 ./chunks/chunk_66_Lecture-12-Columbia.pdf\n",
      "70 8.57982845800052 ./chunks/chunk_71_Lecture-12-Columbia.pdf\n",
      "75 7.840684208000312 ./chunks/chunk_76_Lecture-12-Columbia.pdf\n",
      "80 7.657551166999838 ./chunks/chunk_81_Lecture-12-Columbia.pdf\n",
      "85 7.173694291999709 ./chunks/chunk_86_Lecture-12-Columbia.pdf\n",
      "90 8.033632041000601 ./chunks/chunk_91_Lecture-12-Columbia.pdf\n",
      "95 8.131241334000151 ./chunks/chunk_96_Lecture-12-Columbia.pdf\n",
      "100 8.513085292000142 ./chunks/chunk_101_Lecture-12-Columbia.pdf\n",
      "105 7.703719500000261 ./chunks/chunk_106_Lecture-12-Columbia.pdf\n",
      "110 9.373608582999623 ./chunks/chunk_111_Lecture-12-Columbia.pdf\n",
      "115 8.92286362499999 ./chunks/chunk_116_Lecture-12-Columbia.pdf\n",
      "120 7.5701725419994546 ./chunks/chunk_121_Lecture-12-Columbia.pdf\n",
      "125 8.885581458999695 ./chunks/chunk_126_Lecture-12-Columbia.pdf\n",
      "130 9.089975707999656 ./chunks/chunk_131_Lecture-12-Columbia.pdf\n",
      "135 8.26253962499959 ./chunks/chunk_2_Lecture-13-Columbia.pdf\n",
      "140 8.236549958999603 ./chunks/chunk_7_Lecture-13-Columbia.pdf\n",
      "145 7.548319667000214 ./chunks/chunk_12_Lecture-13-Columbia.pdf\n",
      "150 7.327647166000133 ./chunks/chunk_17_Lecture-13-Columbia.pdf\n",
      "155 7.944368416000543 ./chunks/chunk_22_Lecture-13-Columbia.pdf\n",
      "160 7.962167457999385 ./chunks/chunk_27_Lecture-13-Columbia.pdf\n",
      "165 7.763841124999999 ./chunks/chunk_32_Lecture-13-Columbia.pdf\n",
      "170 8.528474917000494 ./chunks/chunk_37_Lecture-13-Columbia.pdf\n",
      "175 8.435918166000192 ./chunks/chunk_42_Lecture-13-Columbia.pdf\n",
      "180 8.159709958999883 ./chunks/chunk_47_Lecture-13-Columbia.pdf\n",
      "185 7.086496833000638 ./chunks/chunk_52_Lecture-13-Columbia.pdf\n",
      "190 7.783497625000564 ./chunks/chunk_57_Lecture-13-Columbia.pdf\n",
      "195 7.406822457999624 ./chunks/chunk_62_Lecture-13-Columbia.pdf\n",
      "200 7.861424958000498 ./chunks/chunk_67_Lecture-13-Columbia.pdf\n",
      "205 7.928558624999823 ./chunks/chunk_72_Lecture-13-Columbia.pdf\n",
      "210 7.78217662499992 ./chunks/chunk_77_Lecture-13-Columbia.pdf\n",
      "215 7.780537042000105 ./chunks/chunk_82_Lecture-13-Columbia.pdf\n",
      "220 7.842029082999943 ./chunks/chunk_87_Lecture-13-Columbia.pdf\n",
      "225 8.237499332999505 ./chunks/chunk_92_Lecture-13-Columbia.pdf\n",
      "230 8.330287833000511 ./chunks/chunk_97_Lecture-13-Columbia.pdf\n",
      "235 8.31084012500014 ./chunks/chunk_102_Lecture-13-Columbia.pdf\n",
      "240 8.75368141600029 ./chunks/chunk_107_Lecture-13-Columbia.pdf\n",
      "245 7.9195339589996365 ./chunks/chunk_3_Lecture-5-columbia-Fall2024.pdf\n",
      "250 7.279840749999494 ./chunks/chunk_8_Lecture-5-columbia-Fall2024.pdf\n",
      "255 7.964158249999855 ./chunks/chunk_13_Lecture-5-columbia-Fall2024.pdf\n",
      "260 7.67915075000019 ./chunks/chunk_18_Lecture-5-columbia-Fall2024.pdf\n",
      "265 7.593029124999703 ./chunks/chunk_23_Lecture-5-columbia-Fall2024.pdf\n",
      "270 7.654824750000444 ./chunks/chunk_28_Lecture-5-columbia-Fall2024.pdf\n",
      "275 7.408023332999619 ./chunks/chunk_33_Lecture-5-columbia-Fall2024.pdf\n",
      "280 7.650419833000342 ./chunks/chunk_38_Lecture-5-columbia-Fall2024.pdf\n",
      "285 7.761835332999908 ./chunks/chunk_43_Lecture-5-columbia-Fall2024.pdf\n",
      "290 8.191603917000066 ./chunks/chunk_4_Lecture-7-Columbia.pdf\n",
      "295 9.278614374999961 ./chunks/chunk_9_Lecture-7-Columbia.pdf\n",
      "300 8.957823749999989 ./chunks/chunk_14_Lecture-7-Columbia.pdf\n",
      "305 8.682208458000787 ./chunks/chunk_19_Lecture-7-Columbia.pdf\n",
      "310 8.939435625000442 ./chunks/chunk_24_Lecture-7-Columbia.pdf\n",
      "315 8.399405582999862 ./chunks/chunk_29_Lecture-7-Columbia.pdf\n",
      "320 8.849013416999696 ./chunks/chunk_34_Lecture-7-Columbia.pdf\n",
      "325 8.45350845899975 ./chunks/chunk_39_Lecture-7-Columbia.pdf\n",
      "330 8.78551137499926 ./chunks/chunk_44_Lecture-7-Columbia.pdf\n",
      "335 9.074276291000388 ./chunks/chunk_49_Lecture-7-Columbia.pdf\n",
      "340 8.83377904200006 ./chunks/chunk_54_Lecture-7-Columbia.pdf\n",
      "345 8.709281207999993 ./chunks/chunk_59_Lecture-7-Columbia.pdf\n",
      "350 8.70278800000051 ./chunks/chunk_64_Lecture-7-Columbia.pdf\n",
      "355 7.782674582999789 ./chunks/chunk_69_Lecture-7-Columbia.pdf\n",
      "360 7.945757624999715 ./chunks/chunk_5_Lecture-3-Columbia (1).pdf\n",
      "365 9.017627958000048 ./chunks/chunk_10_Lecture-3-Columbia (1).pdf\n",
      "370 8.849826124999709 ./chunks/chunk_15_Lecture-3-Columbia (1).pdf\n",
      "375 8.68846129099984 ./chunks/chunk_20_Lecture-3-Columbia (1).pdf\n",
      "380 8.775227000000086 ./chunks/chunk_25_Lecture-3-Columbia (1).pdf\n",
      "385 7.722368167000241 ./chunks/chunk_30_Lecture-3-Columbia (1).pdf\n",
      "390 7.860786915999597 ./chunks/chunk_35_Lecture-3-Columbia (1).pdf\n",
      "395 8.501044999999976 ./chunks/chunk_40_Lecture-3-Columbia (1).pdf\n",
      "400 8.740583500000866 ./chunks/chunk_45_Lecture-3-Columbia (1).pdf\n",
      "405 9.020848333000686 ./chunks/chunk_50_Lecture-3-Columbia (1).pdf\n",
      "410 8.964991791000102 ./chunks/chunk_55_Lecture-3-Columbia (1).pdf\n",
      "415 8.706264833000205 ./chunks/chunk_60_Lecture-3-Columbia (1).pdf\n",
      "420 8.27922204099923 ./chunks/chunk_65_Lecture-3-Columbia (1).pdf\n",
      "425 9.838630708999517 ./chunks/chunk_70_Lecture-3-Columbia (1).pdf\n",
      "430 9.201253500000348 ./chunks/chunk_75_Lecture-3-Columbia (1).pdf\n",
      "435 7.668966584000373 ./chunks/chunk_80_Lecture-3-Columbia (1).pdf\n",
      "440 7.781664375000219 ./chunks/chunk_5_Lecture-2-columbia-Fall2024.pdf\n",
      "445 7.8346565419997205 ./chunks/chunk_10_Lecture-2-columbia-Fall2024.pdf\n",
      "450 7.589046707999842 ./chunks/chunk_15_Lecture-2-columbia-Fall2024.pdf\n",
      "455 7.356198208999558 ./chunks/chunk_20_Lecture-2-columbia-Fall2024.pdf\n",
      "460 7.774766458000158 ./chunks/chunk_25_Lecture-2-columbia-Fall2024.pdf\n",
      "465 8.031287833000533 ./chunks/chunk_30_Lecture-2-columbia-Fall2024.pdf\n",
      "470 8.357353708999653 ./chunks/chunk_35_Lecture-2-columbia-Fall2024.pdf\n",
      "475 9.569471917000556 ./chunks/chunk_40_Lecture-2-columbia-Fall2024.pdf\n",
      "480 7.439694833999965 ./chunks/chunk_45_Lecture-2-columbia-Fall2024.pdf\n",
      "485 7.687130541999977 ./chunks/chunk_50_Lecture-2-columbia-Fall2024.pdf\n",
      "490 8.185709874999702 ./chunks/chunk_55_Lecture-2-columbia-Fall2024.pdf\n",
      "495 7.911954541999876 ./chunks/chunk_60_Lecture-2-columbia-Fall2024.pdf\n",
      "500 8.784260291999999 ./chunks/chunk_65_Lecture-2-columbia-Fall2024.pdf\n",
      "505 7.441585542000212 ./chunks/chunk_70_Lecture-2-columbia-Fall2024.pdf\n",
      "510 8.117459291999694 ./chunks/chunk_75_Lecture-2-columbia-Fall2024.pdf\n",
      "515 7.629976332999831 ./chunks/chunk_80_Lecture-2-columbia-Fall2024.pdf\n",
      "520 8.041072209000049 ./chunks/chunk_85_Lecture-2-columbia-Fall2024.pdf\n",
      "525 7.595909167000173 ./chunks/chunk_1_Lecture-9-Columbia.pdf\n",
      "530 7.698650000000271 ./chunks/chunk_6_Lecture-9-Columbia.pdf\n",
      "535 7.467916333000176 ./chunks/chunk_11_Lecture-9-Columbia.pdf\n",
      "540 7.911277707999943 ./chunks/chunk_16_Lecture-9-Columbia.pdf\n",
      "545 7.580332708000242 ./chunks/chunk_21_Lecture-9-Columbia.pdf\n",
      "550 7.750810916999399 ./chunks/chunk_26_Lecture-9-Columbia.pdf\n",
      "555 6.80767829099932 ./chunks/chunk_31_Lecture-9-Columbia.pdf\n",
      "560 7.032660541000041 ./chunks/chunk_36_Lecture-9-Columbia.pdf\n",
      "565 8.225823832999595 ./chunks/chunk_41_Lecture-9-Columbia.pdf\n",
      "570 7.221644125000239 ./chunks/chunk_46_Lecture-9-Columbia.pdf\n",
      "575 7.746955707999405 ./chunks/chunk_51_Lecture-9-Columbia.pdf\n",
      "580 8.89356320800016 ./chunks/chunk_56_Lecture-9-Columbia.pdf\n",
      "585 8.760759707999568 ./chunks/chunk_61_Lecture-9-Columbia.pdf\n",
      "590 7.985754457999974 ./chunks/chunk_66_Lecture-9-Columbia.pdf\n",
      "595 7.885314541999833 ./chunks/chunk_71_Lecture-9-Columbia.pdf\n",
      "600 8.285354124999685 ./chunks/chunk_76_Lecture-9-Columbia.pdf\n",
      "605 7.432501375000356 ./chunks/chunk_81_Lecture-9-Columbia.pdf\n",
      "610 8.597793083000397 ./chunks/chunk_1_Lecture-11-columbia.pdf\n",
      "615 7.085838958999375 ./chunks/chunk_6_Lecture-11-columbia.pdf\n",
      "620 7.750360541999726 ./chunks/chunk_11_Lecture-11-columbia.pdf\n",
      "625 8.704311041999972 ./chunks/chunk_16_Lecture-11-columbia.pdf\n",
      "630 8.437389249999796 ./chunks/chunk_21_Lecture-11-columbia.pdf\n",
      "635 8.01620370799992 ./chunks/chunk_26_Lecture-11-columbia.pdf\n",
      "640 8.339264958000058 ./chunks/chunk_31_Lecture-11-columbia.pdf\n",
      "645 7.960805708999942 ./chunks/chunk_36_Lecture-11-columbia.pdf\n",
      "650 7.983526250000068 ./chunks/chunk_41_Lecture-11-columbia.pdf\n",
      "655 7.528581624999788 ./chunks/chunk_1_Lecture-6-columbia-Fall2024.pdf\n",
      "660 7.995284208000157 ./chunks/chunk_6_Lecture-6-columbia-Fall2024.pdf\n",
      "665 7.4731590829997 ./chunks/chunk_11_Lecture-6-columbia-Fall2024.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670 6.929717415999221 ./chunks/chunk_16_Lecture-6-columbia-Fall2024.pdf\n",
      "675 7.669259667000006 ./chunks/chunk_21_Lecture-6-columbia-Fall2024.pdf\n",
      "680 7.963793916999748 ./chunks/chunk_26_Lecture-6-columbia-Fall2024.pdf\n",
      "685 7.476365084000463 ./chunks/chunk_4_Lecture-10-Columbia.pdf\n",
      "690 8.1687207089999 ./chunks/chunk_9_Lecture-10-Columbia.pdf\n",
      "695 8.90650420899965 ./chunks/chunk_14_Lecture-10-Columbia.pdf\n",
      "700 9.140342749999945 ./chunks/chunk_19_Lecture-10-Columbia.pdf\n",
      "705 7.98733508299938 ./chunks/chunk_24_Lecture-10-Columbia.pdf\n",
      "710 8.115165457999865 ./chunks/chunk_29_Lecture-10-Columbia.pdf\n",
      "715 8.156138875000579 ./chunks/chunk_34_Lecture-10-Columbia.pdf\n",
      "720 9.232960582999112 ./chunks/chunk_39_Lecture-10-Columbia.pdf\n",
      "725 8.515497958000196 ./chunks/chunk_44_Lecture-10-Columbia.pdf\n",
      "730 7.679936417000135 ./chunks/chunk_49_Lecture-10-Columbia.pdf\n",
      "735 7.841302666000047 ./chunks/chunk_54_Lecture-10-Columbia.pdf\n",
      "740 8.02238770799977 ./chunks/chunk_59_Lecture-10-Columbia.pdf\n",
      "745 7.95550374999948 ./chunks/chunk_64_Lecture-10-Columbia.pdf\n",
      "750 8.129068791000464 ./chunks/chunk_69_Lecture-10-Columbia.pdf\n",
      "755 8.778868874999716 ./chunks/chunk_74_Lecture-10-Columbia.pdf\n",
      "760 7.308207125000081 ./chunks/chunk_79_Lecture-10-Columbia.pdf\n",
      "765 7.450056874999973 ./chunks/chunk_84_Lecture-10-Columbia.pdf\n",
      "770 8.137261249999938 ./chunks/chunk_89_Lecture-10-Columbia.pdf\n",
      "775 7.93935566699929 ./chunks/chunk_94_Lecture-10-Columbia.pdf\n",
      "780 7.570794291999846 ./chunks/chunk_99_Lecture-10-Columbia.pdf\n",
      "785 7.668086333000247 ./chunks/chunk_104_Lecture-10-Columbia.pdf\n",
      "790 7.2911123750000115 ./chunks/chunk_109_Lecture-10-Columbia.pdf\n",
      "795 7.571943208999983 ./chunks/chunk_4_Lecture-4-columbia-Fall2024.pdf\n",
      "800 8.164453834000597 ./chunks/chunk_9_Lecture-4-columbia-Fall2024.pdf\n",
      "805 7.55943654100065 ./chunks/chunk_14_Lecture-4-columbia-Fall2024.pdf\n",
      "810 10.460296374999416 ./chunks/chunk_19_Lecture-4-columbia-Fall2024.pdf\n",
      "815 8.346888082999612 ./chunks/chunk_24_Lecture-4-columbia-Fall2024.pdf\n",
      "820 7.89074837499993 ./chunks/chunk_29_Lecture-4-columbia-Fall2024.pdf\n",
      "825 8.620286416000454 ./chunks/chunk_34_Lecture-4-columbia-Fall2024.pdf\n",
      "830 7.123980625000513 ./chunks/chunk_39_Lecture-4-columbia-Fall2024.pdf\n",
      "835 7.343548041999384 ./chunks/chunk_44_Lecture-4-columbia-Fall2024.pdf\n",
      "840 7.287329791999582 ./chunks/chunk_49_Lecture-4-columbia-Fall2024.pdf\n",
      "845 7.550609832999726 ./chunks/chunk_54_Lecture-4-columbia-Fall2024.pdf\n",
      "850 8.078799249999975 ./chunks/chunk_59_Lecture-4-columbia-Fall2024.pdf\n",
      "855 7.282098333000249 ./chunks/chunk_64_Lecture-4-columbia-Fall2024.pdf\n",
      "860 7.505362832999708 ./chunks/chunk_69_Lecture-4-columbia-Fall2024.pdf\n",
      "865 7.529766540999844 ./chunks/chunk_74_Lecture-4-columbia-Fall2024.pdf\n",
      "870 9.03695824999977 ./chunks/chunk_79_Lecture-4-columbia-Fall2024.pdf\n",
      "875 8.04511787499996 ./chunks/chunk_5_Lecture-8-Columbia.pdf\n",
      "880 7.8007027499998 ./chunks/chunk_10_Lecture-8-Columbia.pdf\n",
      "885 8.117723082999873 ./chunks/chunk_15_Lecture-8-Columbia.pdf\n",
      "890 8.800097041999834 ./chunks/chunk_20_Lecture-8-Columbia.pdf\n",
      "895 9.241167666000365 ./chunks/chunk_25_Lecture-8-Columbia.pdf\n",
      "900 7.645130375000008 ./chunks/chunk_30_Lecture-8-Columbia.pdf\n",
      "905 7.796405208000579 ./chunks/chunk_35_Lecture-8-Columbia.pdf\n",
      "910 8.15077137499975 ./chunks/chunk_40_Lecture-8-Columbia.pdf\n",
      "915 8.70919533400047 ./chunks/chunk_45_Lecture-8-Columbia.pdf\n",
      "920 9.176570124999671 ./chunks/chunk_50_Lecture-8-Columbia.pdf\n",
      "925 8.926850458000445 ./chunks/chunk_55_Lecture-8-Columbia.pdf\n",
      "930 8.929297542000313 ./chunks/chunk_60_Lecture-8-Columbia.pdf\n",
      "935 8.783865000000333 ./chunks/chunk_65_Lecture-8-Columbia.pdf\n",
      "940 9.315037708999625 ./chunks/chunk_70_Lecture-8-Columbia.pdf\n",
      "945 11.75281829100004 ./chunks/chunk_75_Lecture-8-Columbia.pdf\n"
     ]
    }
   ],
   "source": [
    "all_processed_chunks = list(all_data.keys())\n",
    "start = timeit.default_timer()\n",
    "for i,chunk in enumerate(all_chunks):\n",
    "    if chunk not in all_processed_chunks:\n",
    "        text, links = get_document_extraction(chunk)\n",
    "        all_data[chunk] = {'text':text,'links':links}\n",
    "        if i%5 ==0:\n",
    "            end = timeit.default_timer()\n",
    "            print(i, end-start, chunk)\n",
    "            start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a6ae10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cleaned = {}\n",
    "for key,value in all_data.items():\n",
    "    if len(value['links'])>0:\n",
    "        all_data_cleaned[key] = {'text':value['text'],'links':extract_text_links(value['text'])}\n",
    "    else:\n",
    "        all_data_cleaned[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "77951cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the file name of the JSON file\n",
    "# file_name = \"data_from_presentations.json\"\n",
    "\n",
    "# # Load the JSON file\n",
    "# with open(file_name, \"r\") as json_file:\n",
    "#     data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f7849ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_chunks = list(all_data_cleaned.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397b337",
   "metadata": {},
   "source": [
    "<h1>Aggregating all links from class presentations and HW</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4b701340",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = []\n",
    "for extracted_data in list(list(all_data_cleaned.values())):\n",
    "    all_links = all_links + extracted_data['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ae9cadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_extracted_data = {key:value for key, value in extracted_data.items() if len(value)>=1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ff0f6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_to_extract_data_from = os.listdir('./HWs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5f99c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hw_chunks = []\n",
    "for file_name in all_files_to_extract_data_from:\n",
    "    file_directory = \"./HWs\"\n",
    "    pdf_path = os.path.join(file_directory, file_name)\n",
    "    chunks = split_pdf(pdf_path,file_name, max_pages = 15)\n",
    "    all_hw_chunks = all_hw_chunks + chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f8112a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_hw_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "dfc60108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.847637875000146 ./chunks/chunk_1_HW4-PDF.pdf\n"
     ]
    }
   ],
   "source": [
    "all_processed_chunks = list(all_hw_data.keys())\n",
    "start = timeit.default_timer()\n",
    "for i,chunk in enumerate(all_hw_chunks):\n",
    "    if chunk not in all_processed_chunks:\n",
    "        text, links = get_document_extraction(chunk)\n",
    "        all_hw_data[chunk] = {'text':text,'links':links}\n",
    "        if i%5 ==0:\n",
    "            end = timeit.default_timer()\n",
    "            print(i, end-start, chunk)\n",
    "            start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "aac9f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hw_links = []\n",
    "for extracted_data in list(all_hw_data.values()):\n",
    "    all_hw_links = all_hw_links + extracted_data['links']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab590e8",
   "metadata": {},
   "source": [
    "<h1>Finding new relevant links, by mining topics from the syllabus and finding relevant blog posts links</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3566e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "text,links = get_document_extraction('./Syllabus/Fall 2024 Syllabus-columbia-110524.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "4e9ed1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    start = timeit.default_timer()\n",
    "    all_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"I am currently taking a class called Introduction to Deep Learning and LLM based Generative AI Systems\"},\n",
    "    {\"role\": \"user\", \"content\": f\"I want you to extract all topics I will learn from this class: {text}.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Please make sure to only extract topics related to Machine Learning, Large Language Models, Computer Science, and Software Engineering topics\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please format the output as a list topics. Here is an example: ['model parallelism','Devops principles in machine learning']\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Please return nothing else other than a string version of the list\"}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o\",\n",
    "    max_tokens = 8000,\n",
    "    messages=all_messages\n",
    "    )\n",
    "    course_topics = response['choices'][0]['message']['content']\n",
    "    course_topics_cleaned = clean_q_a_string_json(course_topics)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "5f4f0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_google_search_results_html(response):\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Parse the JSON response\n",
    "        html_content = data.get(\"body\", \"\")  # Get the raw HTML from the \"body\" key\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        # Dictionary to hold the results\n",
    "        results_dict = {}\n",
    "        # Loop through search result elements - adjust as necessary\n",
    "        for result in soup.find_all(\"div\", class_=\"g\"):  # \"g\" is the common class for Google search results\n",
    "            link_tag = result.find(\"a\", href=True)\n",
    "            title_tag = result.find(\"h3\")\n",
    "            if link_tag and title_tag:\n",
    "                url = link_tag[\"href\"]\n",
    "                title = title_tag.get_text()\n",
    "                results_dict[url] = title\n",
    "        return results_dict\n",
    "    else:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "93f066a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_search_results(query,api_token = \"0fbec085971dc1ca50b111c6433d49bd989a57b81344bfb508754d9687d19efa\"):\n",
    "    search_url = f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n",
    "    url = \"https://api.brightdata.com/request\"\n",
    "    payload = {\n",
    "        \"zone\": \"serp_api3\",  # Replace with your actual zone if different\n",
    "        \"url\": search_url,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_token}\"\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    all_search_results = parse_google_search_results_html(response)\n",
    "    return all_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "8efab0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_query_string(query):\n",
    "    # Normalize the query to decompose special characters\n",
    "    normalized = unicodedata.normalize(\"NFD\", query)\n",
    "    # Encode to ASCII, ignoring any non-ASCII characters\n",
    "    ascii_encoded = normalized.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    # URL-encode the sanitized query string\n",
    "    return urllib.parse.quote_plus(ascii_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "18d3b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"Blog post explaining {course_topics_cleaned[1]} in Deep Learning, Machine Learning, Computer Science, or Software Engineering \"\n",
    "linkedin_url = None\n",
    "#     print(query)\n",
    "sanitized_query = sanitize_query_string(query)\n",
    "results = get_google_search_results(sanitized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "5c525c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8749121250002645 5\n",
      "5 17.543654875000357 29\n",
      "10 31.217416375002358 54\n",
      "15 63.02728124999703 79\n",
      "20 73.60367716699693 100\n",
      "25 88.57625995900162 123\n",
      "30 99.52590387500095 148\n",
      "35 110.69694012500258 171\n",
      "40 139.45502395900257 195\n",
      "45 155.12103224999737 215\n",
      "50 164.55748754199885 239\n",
      "55 179.83256741699734 262\n",
      "60 190.92883874999825 283\n",
      "65 202.32500008399802 305\n",
      "70 212.8582381669985 329\n",
      "75 225.31525770900043 354\n",
      "80 237.91018229199835 373\n",
      "85 250.4848908749991 391\n",
      "90 260.39152995900076 415\n",
      "95 274.1067696250029 437\n",
      "100 283.8554647089986 459\n",
      "105 298.89148462499725 480\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "start = timeit.default_timer()\n",
    "for i,topic in enumerate(course_topics_cleaned):\n",
    "    query = f\"Blog post explaining {topic} in Deep Learning, Machine Learning, Computer Science, or Software Engineering \"\n",
    "    linkedin_url = None\n",
    "    #     print(query)\n",
    "    sanitized_query = sanitize_query_string(query)\n",
    "    results = get_google_search_results(sanitized_query)\n",
    "    num_articles= 0\n",
    "    for key,value in results.items():\n",
    "        if num_articles<=4:\n",
    "            all_results[key] = value\n",
    "            num_articles+=1\n",
    "        else:\n",
    "            break\n",
    "    end = timeit.default_timer()\n",
    "    if i%5 ==0:\n",
    "        print(i,end-start,len(list(all_results.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "23df6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_google_blog_links = list(all_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8016f3",
   "metadata": {},
   "source": [
    "<h1>Extracting all text from links</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f555eb5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model\n",
      "Processing: https://arxiv.org/abs/2205.14135\n",
      "Processing: https://ai.stanford.edu/blog/longer-sequencesnext-leap-ai/\n",
      "Processing: https://github.com/vllm-project/vllm\n",
      "Processing: https://vllm.ai\n",
      "Processing: https://arxiv.org/abs/2309.06180\n",
      "Processing: https://discord.gg/jz7wjKhh6g\n",
      "Processing: https://docs.nvidia.com/datacenter/tesla/mig-userguide/index.html\n",
      "Processing: https://huggingface.co/blog/trl-peft\n",
      "Processing: https://arxiv.org/pdf/2202.05924\n",
      "Processing: https://splab.sdu.edu.cn/G\n",
      "Processing: https://research.google/blog/pathways-languagemodel-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/\n",
      "Processing: https://arxiv.org/pdf/2202.05924\n",
      "Processing: https://www.youtube.com/watch?v=EnJ7qX9fkcU\n",
      "Processing: https://jvns.ca/blog/2016/10/10/what-even-is-a-container/\n",
      "Processing: https://kubernetes.io/\n",
      "Processing: https://cloud.google.com/kubernetesengine/\n",
      "Processing: https://www.alibabacloud.com/product/kubernetes\n",
      "Processing: https://aws.amazon.com/eks/\n",
      "Processing: https://azure.microsoft.com/enus/services/kubernetes-service/\n",
      "Processing: https://github.com/IBM/FfDL\n",
      "Processing: https://www.ibm.com/products/watson-studio\n",
      "Processing: https://aws.amazon.com/sagemaker\n",
      "Processing: https://azure.com/ml\n",
      "Processing: https://cloud.google.com/vertex-ai/\n",
      "Processing: https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n",
      "Processing: https://arxiv.org/pdf/2202.05924\n",
      "Processing: https://website-754fwhahs-humanloopml.vercel.app/blog/open_ai_talk\n",
      "Processing: https://arxiv.org/pdf/2202.05924\n",
      "Processing: https://splab.sdu.edu.cn/GPT3.pdf\n",
      "Processing: https://research.google/blog/pathw\n",
      "Processing: https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/\n",
      "Processing: https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/\n",
      "Processing: https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html\n",
      "Processing: https://arxiv.org/pdf/1811.05233.pdf\n",
      "Processing: https://cloud.google.com/tpu/docs/systemarchitecture\n",
      "Processing: https://medium.com/mlreview/a-guide-to-receptive-fieldarithmetic-for-convolutional-neural-networks-e0f514068807\n",
      "Processing: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Processing: https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n",
      "Processing: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Processing: https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n",
      "Processing: https://arxiv.org/pdf/2005.14165\n",
      "Processing: https://gluebenchmark.com/leaderboard\n",
      "Processing: https://super.gluebenchmark.com/leaderboard/\n",
      "Processing: https://crfm.stanford.edu/helm/\n",
      "Processing: https://crfm.stanford.edu/helm/\n",
      "Processing: https://www.anyscale.com/blog/reproducible-performance-metrics-for-llm-inference\n",
      "Processing: https://github.com/ray-project/LLMPerf\n",
      "Processing: https://github.com/ray-project/llmperf-leaderboard\n",
      "Processing: https://github.com/ray-project/llmperf-leaderboard\n",
      "Processing: https://www.kubeflow.org/docs/about/kubeflow/\n",
      "Processing: https://www.kubeflow.org/docs/pipelines/pipelines-quickstart/\n",
      "Processing: https://www.kubeflow.org/docs/pi\n",
      "Processing: https://www.kubeflow.org/docs/pipelines/overview/concepts/comp\n",
      "Processing: https://www.kubeflow.org/docs/pipelines/refe\n",
      "Processing: https://mlcommons.org\n",
      "Processing: https://mlcommons.org/benchmarks/storage/\n",
      "Processing: https://github.com/bigscience-workshop/promptsource/blob/main/promptsource/templates/amazon_polarity/templates.yaml\n",
      "Processing: https://huggingface.co/datasets/samsum,\n",
      "Processing: https://github.com/google-research/FLAN/blob/2c79a31/flan/v2/templates.py\n",
      "Processing: https://huggingface.co/datasets/knkarthick/dialogsum/viewer/knkarthick--dialo\n",
      "Processing: https://arxiv.org/pdf/1806.09055.pdf\n",
      "Processing: http://onnx.ai\n",
      "Processing: http://onnx.ai\n",
      "Processing: http://onnx.ai/supported-tools\n",
      "Processing: https://github.com/onnx/tutorials\n",
      "Processing: https://github.com/onnx/models\n",
      "Processing: http://onnx.ai/supported-tools\n",
      "Processing: https://github.com/microsoft/onnxruntime\n",
      "Processing: https://github.com/huggingface/notebooks/blob/main/examples/onnx\n",
      "Processing: https://github.com/tensorflow/models\n",
      "Processing: https://github.com/pytorch/vision\n",
      "Processing: https://www.restack.io/p/retrieval-augmented-generation-answer-rag-vs-semantic-cat-ai\n",
      "Processing: https://www.geeksforgeeks.org/keywordsearching-algorithms-for-search-engines/\n",
      "Processing: https://en.wikipedia.org/wiki/Okapi_BM25\n",
      "Processing: https://drive.google.com/file/d/1UCb_ED3anGlfUvqm19ZKvVdBTBJb4KM/view?usp=sharing\n",
      "Processing: https://api.open-meteo.com/v1/forecast\"\n"
     ]
    }
   ],
   "source": [
    "extracted_data = process_links(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "448c7ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://dustinstansbury.github.io/theclevermachine/bias-variance-tradeoff.\n",
      "Processing: https://arxiv.org/pdf/1611.03530.pdf.\n",
      "Processing: https://arxiv.org/abs/1506.01186.\n",
      "Processing: https://arxiv.org/pdf/1611.03530.pdf\n",
      "Processing: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutionalneural-networks.pdf\n",
      "Processing: https://arxiv.org/pdf/1409.1556.pdf\n",
      "Processing: https://arxiv.org/pdf/1409.4842.pdf\n",
      "Processing: https://github.com/qfgaohao/pytorch-ssd\n",
      "Processing: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n",
      "Processing: https://github.com/onnx/tutorials/blob/master/tutorials/OnnxRuntimeServerSSDModel.ipynb\n",
      "Processing: https://storage.googleapis.com/openimages/web/index.html\n",
      "Processing: http://host.robots.ox.ac.uk/pascal/VOC/voc2007/\n",
      "Processing: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
      "Processing: https://cs231n.github.io/transfer-learning/\n",
      "Processing: http://host.robots.ox.ac.uk/pascal/VOC/voc2007/\n"
     ]
    }
   ],
   "source": [
    "extracted_hw_data = process_links(all_hw_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "5ee1936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://arize.com/blog/understanding-bias-in-ml-models/\n",
      "Processing: https://medium.com/@sruthy.sn91/addressing-bias-in-machine-learning-techniques-and-ethical-considerations-fe9d9532d657\n",
      "Processing: https://www.scalablepath.com/machine-learning/bias-machine-learning\n",
      "Processing: https://www.wovenware.com/blog/2020/07/3-bias-machine-learning/\n",
      "Processing: https://www.encora.com/insights/a-short-discussion-on-bias-in-machine-learning\n",
      "Processing: https://www.simplilearn.com/tutorials/machine-learning-tutorial/bias-and-variance\n",
      "Processing: https://www.bmc.com/blogs/bias-variance-machine-learning/\n",
      "Processing: https://data-science-blog.com/blog/2020/11/02/bias-and-variance-in-machine-learning/\n",
      "Processing: http://varianceexplained.org/r/ds-ml-ai/\n",
      "Processing: https://towardsai.net/p/l/mastering-the-bias-variance-dilemma-a-guide-for-machine-learning-practitioners\n",
      "Processing: http://research.google/blog/a-new-lens-on-understanding-generalization-in-deep-learning/\n",
      "Processing: https://dominicm73.blogspot.com/2021/03/regularization-and-generalization-in.html\n",
      "Processing: https://magnimindacademy.com/blog/what-is-generalization-in-machine-learning/\n",
      "Processing: https://www.reddit.com/r/MachineLearning/comments/8mpxmm/d_what_do_we_currently_know_about_generalization/\n",
      "Processing: https://queentechsolutions.net/blog/software/software-engineering-vs-machine-learning/\n",
      "Processing: https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning\n",
      "Processing: https://www.sprintzeal.com/blog/machine-learning-regularization\n",
      "Processing: https://www.geeksforgeeks.org/regularization-in-machine-learning/\n",
      "Processing: https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\n",
      "Processing: https://levity.ai/blog/difference-machine-learning-deep-learning\n",
      "Processing: https://www.zendesk.com/blog/machine-learning-and-deep-learning/\n",
      "Processing: https://sunscrapers.com/blog/machine-learning-vs-deep-learning/\n",
      "Processing: https://kareemai.com/blog/posts/ds_and_algo/master_ds.html\n",
      "Processing: https://www.netguru.com/blog/machine-learning-vs-deep-learning\n",
      "Processing: https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\n",
      "Processing: https://serokell.io/blog/understanding-backpropagation\n",
      "Processing: https://medium.com/@tam.tamanna18/backpropagation-in-neural-networks-a-comprehensive-guide-3d36151b8fb4\n",
      "Processing: https://vinodsblog.com/2019/02/17/deep-learning-backpropagation-algorithm-basics/\n",
      "Processing: https://www.reddit.com/r/MachineLearning/comments/9ddg3y/d_what_do_you_think_is_the_best_way_to_understand/\n",
      "Processing: https://towardsdatascience.com/gradient-descent-a-beginners-guide-fa0b5d0a1db8\n",
      "Processing: https://www.reddit.com/r/learnmachinelearning/comments/t5pz3z/the_magic_of_machine_learning_gradient_descent/\n",
      "Processing: https://www.datacamp.com/tutorial/tutorial-gradient-descent\n",
      "Processing: https://medium.com/quantyca/gradient-descent-in-deep-learning-b1077b89af81\n",
      "Processing: https://graphite-note.com/understanding-gradient-descent/\n",
      "Processing: https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html\n",
      "Processing: https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/\n",
      "Processing: https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253\n",
      "Processing: https://blog.roboflow.com/activation-function-computer-vision/\n",
      "Processing: https://medium.com/@shaomukherjee/understanding-activation-functions-a-comprehensive-overview-d3e7b0cd2e39\n",
      "Processing: https://medium.com/@shivansh20128/what-are-vanishing-gradients-and-exploding-gradients-54d9e32c9b99\n",
      "Processing: https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing\n",
      "Processing: https://www.geeksforgeeks.org/vanishing-and-exploding-gradients-problems-in-deep-learning/\n",
      "Processing: https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/\n",
      "Processing: https://programmathically.com/understanding-the-exploding-and-vanishing-gradients-problem/\n",
      "Processing: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
      "Processing: https://medium.com/@akshayhitendrashah/cliches-of-deep-learning-part-i-5206a17c3264\n",
      "Processing: https://medium.com/@juanc.olamendy/weight-initialization-for-deep-learning-neural-networks-6047cbe27297\n",
      "Processing: https://www.linkedin.com/advice/0/what-best-weight-initialization-techniques-deep-3xinf\n",
      "Processing: https://www.reddit.com/r/deeplearning/comments/1evwa3d/why_do_we_initialize_the_neural_networks_randomly/\n",
      "Processing: https://www.mygreatlearning.com/blog/understanding-learning-rate-in-machine-learning/\n",
      "Processing: https://www.machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
      "Processing: https://medium.com/@ach.chathuranga/the-art-and-science-of-learning-rates-in-deep-learning-826fe4e85b07\n",
      "Processing: https://spotintelligence.com/2024/02/19/learning-rate-machine-learning/\n",
      "Processing: https://towardsdatascience.com/deep-learning-personal-notes-part-1-lesson-2-8946fe970b95\n",
      "Processing: https://medium.com/@juanc.olamendy/real-world-ml-understanding-batch-size-train-faster-and-better-deep-learning-models-2b24c353e292\n",
      "Processing: https://medium.com/geekculture/how-does-batch-size-impact-your-model-learning-2dd34d9fb1fa\n",
      "Processing: https://www.linkedin.com/pulse/power-batch-size-comprehensive-guide-gradient-descent-juan-carlos-dg5de\n",
      "Processing: https://www.bacancytechnology.com/qanda/qa-automation/batch-size-in-background-of-deep-reinforcement-learning\n",
      "Processing: https://www.sabrepc.com/blog/Deep-Learning-and-AI/Epochs-Batch-Size-Iterations?srsltid=AfmBOoqiQ5cmo_fDNWZLv8VLRlftrCmcxef2e2vRDCJfiSsNb9XfH4I6\n",
      "Processing: https://medium.com/@piyushkashyap045/understanding-sgd-with-momentum-in-deep-learning-a-beginner-friendly-guide-0252ede605b4\n",
      "Processing: https://blog.dailydoseofds.com/p/momentum-explained-visually-and-intuitively\n",
      "Processing: https://karan3-zoh.medium.com/paper-summary-on-the-importance-of-initialization-and-momentum-in-deep-learning-8b8121d21aa9\n",
      "Processing: https://www.digitalocean.com/community/tutorials/intro-to-optimization-momentum-rmsprop-adam\n",
      "Processing: https://stackoverflow.com/questions/56482528/what-is-momentum-in-machine-learning\n",
      "Processing: https://medium.com/@ngneha090/batch-normalization-in-deep-learning-5f200f6f7733\n",
      "Processing: https://medium.com/@utsavraj.ptn04/demystifying-batch-normalization-in-deep-learning-a-beginners-guide-3aa916390875\n",
      "Processing: https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/\n",
      "Processing: https://www.geeksforgeeks.org/what-is-batch-normalization-in-deep-learning/\n",
      "Processing: https://graphite-note.com/the-impact-of-batch-normalization-in-machine-learning/\n",
      "Processing: https://medium.com/@sujathamudadla1213/weight-decay-in-deep-learning-8fb8b5dd825c\n",
      "Processing: https://programmathically.com/weight-decay-in-neural-networks/\n",
      "Processing: https://spotintelligence.com/2024/05/02/weight-decay/\n",
      "Processing: https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8\n",
      "Processing: https://www.linkedin.com/posts/skphd_why-do-we-need-weight-decay-in-modern-deep-activity-7261978555968831490-R44j\n",
      "Processing: https://medium.com/@utsavraj.ptn04/dropping-the-knowledge-bomb-understanding-dropout-layers-in-deep-learning-0612f517269d\n",
      "Processing: https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5\n",
      "Processing: https://www.geeksforgeeks.org/dropout-regularization-in-deep-learning/\n",
      "Processing: https://spotintelligence.com/2023/08/15/dropout-in-neural-network/\n",
      "Processing: https://www.analyticsvidhya.com/blog/2022/08/dropout-regularization-in-deep-learning/\n",
      "Processing: https://medium.com/@juanc.olamendy/real-world-ml-early-stopping-in-deep-learning-a-comprehensive-guide-fabb1e69f8cc\n",
      "Processing: https://www.sabrepc.com/blog/deep-learning-and-ai/what-is-early-stopping-in-deep-learning?srsltid=AfmBOoqWyvvtVwKedR6dHmYPyb0jP7OdzfJOgetD8cbTriNlQ6RC1IqJ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://cyborgcodes.medium.com/what-is-early-stopping-in-deep-learning-eeb1e710a3cf\n",
      "Processing: https://www.machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
      "Processing: https://towardsdatascience.com/early-stopping-why-did-your-machine-learning-model-stop-training-c6b1d64e009e\n",
      "Processing: https://insights.daffodilsw.com/blog/what-is-data-augmentation-in-deep-learning\n",
      "Processing: https://aws.amazon.com/what-is/data-augmentation/\n",
      "Processing: https://www.f22labs.com/blogs/what-is-data-augmentation/\n",
      "Processing: https://medium.com/@saiwadotai/the-essential-guide-to-data-augmentation-in-deep-learning-f66e0907cdc8\n",
      "Processing: https://gretel.ai/technical-glossary/what-is-data-augmentation\n",
      "Processing: https://medium.com/udemy-engineering/delivering-ai-ml-products-efficiently-the-single-node-machine-learning-workflow-bad1389410af\n",
      "Processing: https://www.enthought.com/blog/a-beginners-guide-to-deep-learning/\n",
      "Processing: https://medium.com/@Coursesteach/deep-learning-part-1-86757cf5a0c3\n",
      "Processing: https://www.ml4devs.com/articles/machine-learning-intro-for-developers/\n",
      "Processing: https://afmck.in/posts/2023-02-26-parallelism/\n",
      "Processing: https://medium.com/@dnyaneshwalwadkar/harnessing-the-power-of-parallelism-for-faster-deep-learning-model-training-with-tensorflow-a4f0d05718\n",
      "Processing: https://www.blopig.com/blog/2023/10/understanding-gpu-parallelization-in-deep-learning/\n",
      "Processing: https://www.purestorage.com/knowledge/what-is-model-parallelism.html\n",
      "Processing: https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214\n",
      "Processing: https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4\n",
      "Processing: https://www.run.ai/blog/parallelism-strategies-for-distributed-training\n",
      "Processing: https://neptune.ai/blog/distributed-training\n",
      "Processing: https://criss-wang.com/post/blogs/mlops/distributed-training/\n",
      "Processing: https://medium.com/@rachittayal7/a-gentle-introduction-to-distributed-training-of-ml-models-81295a7057de\n",
      "Processing: https://medium.com/cracking-the-data-science-interview/datacast-episode-58-deep-learning-meets-distributed-systems-with-jim-dowling-e14e19538059\n",
      "Processing: https://betterprogramming.pub/parallel-and-distributed-training-in-deep-learning-for-beginners-part-1-introduction-612a4534a117\n",
      "Processing: https://d2l.ai/chapter_computational-performance/parameterserver.html\n",
      "Processing: https://shivambharuka.medium.com/deep-learning-a-primer-on-distributed-training-part-1-d0ae0054bb1c\n",
      "Processing: https://medium.com/coinmonks/parameter-server-for-distributed-machine-learning-fd79d99f84c3\n",
      "Processing: https://github.com/Jokeren/Notes/blob/master/Deep%20Learning/Scaling%20Distributed%20Machine%20Learning%20with%20the%20Parameter%20Server.md\n",
      "Processing: https://medium.com/@mpchang17/making-the-leap-from-hardware-to-machine-learning-part-2-eb172c2e9d8e\n",
      "Processing: https://csweb.rice.edu/academics/graduate-programs/online-mcs/blog/computer-science-vs-artificial-intelligence-and-machine-learning\n",
      "Processing: https://engineering.deptagency.com/machine-learning-explain-it-like-im-five-podcast\n",
      "Processing: https://news.ycombinator.com/item?id=30432987\n",
      "Processing: https://buseyaren.medium.com/what-is-a-gpu-are-they-needed-for-deep-learning-94dd4aeb45f6\n",
      "Processing: https://blogs.nvidia.com/blog/why-gpus-are-great-for-ai/\n",
      "Processing: https://www.run.ai/guides/gpu-deep-learning\n",
      "Processing: https://goonline.io/blog/making-the-leap-why-gpus-are-essential-for-machine-learning-and-deep-learning/\n",
      "Processing: https://www.weka.io/learn/glossary/ai-ml/gpus-for-machine-learning/\n",
      "Processing: https://www.digitalocean.com/community/tutorials/understanding-tensor-cores\n",
      "Processing: https://acecloud.ai/resources/blog/cuda-cores-vs-tensor-cores/\n",
      "Processing: https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/\n",
      "Processing: https://developer.nvidia.com/blog/tag/tensor-cores/\n",
      "Processing: https://stackoverflow.com/questions/47335027/what-is-the-difference-between-cuda-vs-tensor-cores\n",
      "Processing: https://developer.nvidia.com/blog/scaling-deep-learning-training-nccl/\n",
      "Processing: https://medium.com/@akp83540/nvidia-collective-communications-library-nccl-5c325c41df25\n",
      "Processing: https://medium.com/@pranay.janupalli/introduction-to-nccl-communication-operators-the-backbone-of-efficient-distributed-training-d8b4b2f990a6\n",
      "Processing: https://forums.developer.nvidia.com/t/scaling-deep-learning-training-with-nccl/148629\n",
      "Processing: https://www.youtube.com/watch?v=GjbsCzYwh24\n",
      "Processing: https://www.analyticsvidhya.com/blog/2021/05/convolutional-neural-networks-cnn/\n",
      "Processing: https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns\n",
      "Processing: https://medium.com/@tam.tamanna18/exploring-convolutional-neural-networks-architecture-steps-use-cases-and-pros-and-cons-b0d3b7d46c71\n",
      "Processing: https://vinodsblog.com/2018/10/15/everything-you-need-to-know-about-convolutional-neural-networks/\n",
      "Processing: https://www.linkedin.com/pulse/understanding-convolutional-neural-networks-cnns-deep-aritra-pain\n",
      "Processing: https://medium.com/@utsavraj.ptn04/unraveling-the-wonders-of-recurrent-neural-networks-rnns-a-deep-dive-into-sequential-learning-27d5e74344d3\n",
      "Processing: https://vinodsblog.com/2019/01/07/deep-learning-introduction-to-recurrent-neural-networks/\n",
      "Processing: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
      "Processing: https://aws.amazon.com/what-is/recurrent-neural-network/\n",
      "Processing: https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
      "Processing: https://blog.mlreview.com/understanding-lstm-and-its-diagrams-37e2f46f1714\n",
      "Processing: https://www.reddit.com/r/learnmachinelearning/comments/e6x22x/a_very_good_blog_post_to_learn_about_lstm_networks/\n",
      "Processing: https://shiyan.medium.com/materials-to-understand-lstm-34387d6454c1\n",
      "Processing: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
      "Processing: https://www.analyticsvidhya.com/blog/2022/03/an-overview-on-long-short-term-memory-lstm/\n",
      "Processing: https://www.machinelearningmastery.com/what-are-generative-adversarial-networks-gans/\n",
      "Processing: https://www.analyticsvidhya.com/blog/2021/10/an-end-to-end-introduction-to-generative-adversarial-networksgans/\n",
      "Processing: https://viso.ai/deep-learning/generative-adversarial-networks-gan/\n",
      "Processing: https://vinodsblog.com/2018/11/23/generative-adversarial-networks-gans-the-basics-you-need-to-know/\n",
      "Processing: https://www.proxet.com/blog/introduction-to-generative-adversarial-networks\n",
      "Processing: https://www.superannotate.com/blog/diffusion-models\n",
      "Processing: https://encord.com/blog/diffusion-models/\n",
      "Processing: https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/\n",
      "Processing: https://towardsai.net/p/machine-learning/ai-ml-diffusion-models-a-beginners-guide-to-math-behind-stable-diffusion-and-dall-e\n",
      "Processing: https://www.reddit.com/r/deeplearning/comments/1fw3m3h/resources_to_better_under_diffusion_model/\n",
      "Processing: https://neptune.ai/blog/best-practices-docker-for-machine-learning\n",
      "Processing: https://medium.com/@diogeneswallis/docker-for-machine-learning-abca15eaadc6\n",
      "Processing: https://aws.amazon.com/blogs/opensource/why-use-docker-containers-for-machine-learning-development/\n",
      "Processing: https://www.reddit.com/r/MachineLearning/comments/iq8i4f/d_using_docker_for_ml_development/\n",
      "Processing: https://cnvrg.io/docker-for-machine-learning-and-reproducible-data-science/\n",
      "Processing: https://medium.com/@datasciencewizards/why-do-we-hear-kubernetes-and-machine-learning-together-so-often-e73bc72a278a\n",
      "Processing: https://www.index.dev/blog/kubernetes-for-software-engineers-what-no-one-tells-you-but-you-need-to-know\n",
      "Processing: https://hamel.dev/blog/posts/k8s/\n",
      "Processing: https://medium.com/@somnath.2301/role-of-kubernetes-based-engineering-in-ai-9540b994ff37\n",
      "Processing: https://overcast.blog/mastering-kubernetes-for-machine-learning-ml-ai-in-2024-26f0cb509d81\n",
      "Processing: https://medium.com/@tenyks_blogger/ml-vs-mlops-engineer-key-differences-similarities-43d612bacdd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://cloud.google.com/discover/deep-learning-vs-machine-learning\n",
      "Processing: https://medium.com/@markpalatucci/deep-learning-in-the-cloud-vs-on-premises-machines-d9707ddfec22\n",
      "Processing: https://aws.amazon.com/blogs/machine-learning/\n",
      "Processing: https://aws.amazon.com/what-is/deep-learning/\n",
      "Processing: https://aws.amazon.com/blogs/architecture/lets-architect-learn-about-machine-learning-on-aws/\n",
      "Processing: https://www.whizlabs.com/blog/aws-deep-learning/\n",
      "Processing: https://k21academy.com/amazon-web-services/aws-ml/deep-learning/\n",
      "Processing: https://techcommunity.microsoft.com/tag/software%20engineering?nodeId=board%3AEducatorDeveloperBlog\n",
      "Processing: https://opensource.microsoft.com/blog/topic/deep-learning/\n",
      "Processing: https://learn.microsoft.com/en-us/community/content/get-started-machine-learning\n",
      "Processing: https://blog.acolyer.org/2019/07/08/software-engineering-for-machine-learning/\n",
      "Processing: https://www.microsoft.com/en-us/research/project/deep-program-understanding/\n",
      "Processing: http://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead/\n",
      "Processing: https://itcraftapps.com/blog/google-machine-learning/\n",
      "Processing: https://developers.google.com/machine-learning/crash-course\n",
      "Processing: http://research.google/blog/using-machine-learning-to-explore-neural-network-architecture/\n",
      "Processing: https://medium.com/geekculture/deep-learning-with-pytorch-part-1-what-is-deep-learning-9759d3fd46d4\n",
      "Processing: https://medium.com/@zacharypollatsek/pytorch-my-first-foray-into-deep-learning-faba8f2cdc44\n",
      "Processing: https://www.altexsoft.com/blog/pytorch-library/\n",
      "Processing: https://www.reddit.com/r/learnmachinelearning/comments/1ajtyso/a_simple_explanation_of_pytorch/\n",
      "Processing: https://www.youtube.com/watch?v=V_xro1bcAuA\n",
      "Processing: https://medium.com/samsara-engineering/building-a-modern-machine-learning-platform-with-ray-eb0271f9cbcf\n",
      "Processing: https://www.anyscale.com/blog/why-you-should-build-your-ai-applications-with-ray\n",
      "Processing: https://www.infoq.com/presentations/ray-ml/\n",
      "Processing: https://medium.com/@erfan.loghmani/from-frustration-to-fast-using-ray-for-parallel-computing-on-a-single-machine-or-a-cluster-26233b2faabd\n",
      "Processing: https://www.reddit.com/r/mlops/comments/1bsuknq/opinions_of_ray_framework/\n",
      "Processing: https://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks\n",
      "Processing: https://www.ibm.com/topics/deep-learning\n",
      "Processing: https://www.ibm.com/topics/machine-learning\n",
      "Processing: https://admin02.prod.blogs.cis.ibm.net/blogs/think/category/machine-learning/\n",
      "Processing: https://developer.ibm.com/technologies/machine-learning/blogs/\n",
      "Processing: https://medium.com/codex/machine-learning-development-in-the-cloud-part-6-jobs-and-automation-2874dbd126b5\n",
      "Processing: https://www.reddit.com/r/MachineLearning/comments/ifn7ua/d_what_are_the_untold_truths_of_being_a_machine/\n",
      "Processing: https://www.fullstackacademy.com/blog/career-roadmap-to-get-into-ai-ml\n",
      "Processing: https://www.quora.com/Is-machine-learning-and-deep-learning-a-better-career-than-web-development-now\n",
      "Processing: https://medium.com/@vinodvamanbhat/devops-for-machine-learning-mlops-4fd280ca2ffd\n",
      "Processing: https://medium.com/oolooroo/role-of-ai-and-machine-learning-in-devops-c06c0035cf59\n",
      "Processing: https://www.icertglobal.com/how-devops-is-shaping-ai-ml-development-pipelines-blog/detail\n",
      "Processing: https://www.napkyn.com/blog/mlops-the-devops-of-machine-learning-systems\n",
      "Processing: https://www.linkedin.com/pulse/evolution-machine-learning-devops-bridging-gap-between-rajaram-j-lhvqc\n",
      "Processing: https://medium.com/@zakariasaif/demystifying-ai-and-ml-models-from-training-to-deployment-38179135d3e8\n",
      "Processing: https://synoptek.com/insights/it-blogs/data-insights/ai-ml-dl-and-generative-ai-face-off-a-comparative-analysis/\n",
      "Processing: https://nebius.com/blog/posts/what-is-automl\n",
      "Processing: https://www.clicdata.com/blog/ai-ml-data-science-deep-learning/\n",
      "Processing: https://online-engineering.case.edu/blog/advancements-in-artificial-intelligence-and-machine-learning\n",
      "Processing: https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained\n",
      "Processing: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html\n",
      "Processing: https://h2o.ai/blog/2019/a-deep-dive-into-h2os-automl/\n",
      "Processing: https://towardsdatascience.com/h2o-for-inexperienced-users-7bc064124264\n",
      "Processing: https://savemyleads.com/blog/useful/automl-automating-machine-learning\n",
      "Processing: https://www.analyticsvidhya.com/blog/2021/05/a-step-by-step-guide-to-automl-with-h2o-flow/\n",
      "Processing: https://neptune.ai/blog/mlops\n",
      "Processing: https://one2n.io/blog/understanding-mlops-from-a-software-engineers-perspective\n",
      "Processing: https://insights.sei.cmu.edu/blog/introduction-to-mlops-bridging-machine-learning-and-operations/\n",
      "Processing: https://www.acldigital.com/blogs/journey-machine-learning-towards-mlops\n",
      "Processing: https://medium.com/@faheemrustamy/machine-learning-platforms-using-kubeflow-a0a9be98f57f\n",
      "Processing: https://www.arrikto.com/blog/kubeflow-fundamentals-machine-learning-workflows-part-2/\n",
      "Processing: https://ubuntu.com/blog/deep-dive-kubeflow-pipelines\n",
      "Processing: https://medium.com/@saschagrunert/data-science-on-steroids-with-kubeflow-60fc3ba92b06\n",
      "Processing: https://blog.kubeflow.org/\n",
      "Processing: https://mlflow.org/blog/deep-learning-part-1\n",
      "Processing: https://www.run.ai/guides/machine-learning-operations/mlflow\n",
      "Processing: https://cloud4scieng.org/2022/07/08/understanding-mlops-a-review-of-practical-deep-learning-at-scale-with-mlflow-by-yong-liu/\n",
      "Processing: https://viso.ai/deep-learning/mlflow-machine-learning-experimentation/\n",
      "Processing: https://mlflow.org/blog/deep-learning-part-2\n",
      "Processing: https://medium.com/@shb8086/tutorial-series-onnx-a7044297991d\n",
      "Processing: https://medium.com/@hassini.abir/onnx-bridging-the-gap-between-different-machine-learning-frameworks-246593da3f09\n",
      "Processing: https://www.splunk.com/en_us/blog/learn/open-neural-network-exchange-onnx.html\n",
      "Processing: https://viso.ai/computer-vision/onnx-explained/\n",
      "Processing: https://www.linkedin.com/pulse/what-onnx-machine-learning-model-why-should-you-care-bhattiprolu\n",
      "Processing: https://neptune.ai/blog/tensorboard-tutorial\n",
      "Processing: https://medium.com/dscutsg/a-brief-introduction-to-tensorflow-for-machine-learning-aed3d19d1f55\n",
      "Processing: https://www.springboard.com/blog/data-science/tensorflow-tutorial-beginners/\n",
      "Processing: https://research.google/blog/build-your-own-machine-learning-visualizations-with-the-new-tensorboard-api/\n",
      "Processing: https://towardsdatascience.com/vibing-out-tensorflow-e91c04cc3872\n",
      "Processing: https://blogs.nvidia.com/deep-learning-fundamentals-explained/\n",
      "Processing: https://developer.nvidia.com/blog/profiling-and-optimizing-deep-neural-networks-with-dlprof-and-pyprof/\n",
      "Processing: https://medium.com/geekculture/deep-learning-gpu-setup-from-scratch-75f730c49c01\n",
      "Processing: https://developer.nvidia.com/blog/minimizing-dl-inference-latency-with-mig/\n",
      "Processing: https://neptune.ai/blog/machine-learning-approach-to-log-analytics\n",
      "Processing: https://medium.com/xenonstack-ai/automatic-log-analysis-using-deep-learning-and-ai-398759d01b2f\n",
      "Processing: https://sciencelogic.com/blog/log-analysis-with-machine-learning-an-automated-approach-to-analyzing-logs-using-ml-ai\n",
      "Processing: https://edgedelta.com/company/blog/how-log-analysis-is-evolving-with-ai-and-ml\n",
      "Processing: https://www.evidentlyai.com/ml-in-production/data-drift\n",
      "Processing: https://superwise.ai/blog/everything-you-need-to-know-about-drift-in-machine-learning/\n",
      "Processing: https://medium.com/@sachinsoni600517/understanding-and-detecting-drift-in-ml-models-58253f7968fe\n",
      "Processing: https://spotintelligence.com/2024/04/08/data-drift-in-machine-learning/\n",
      "Processing: https://medium.com/@gfcristhian98/understanding-model-drift-and-how-to-detect-it-effectively-305f27c734b2\n",
      "Processing: https://community.cadence.com/cadence_blogs_8/b/breakfast-bytes/posts/mlperf\n",
      "Processing: https://odsc.medium.com/what-is-mlperf-bf24ee72c309\n",
      "Processing: https://blogs.nvidia.com/blog/mlperf-training-blackwell/\n",
      "Processing: https://developer.nvidia.com/blog/leading-mlperf-training-2-1-with-full-stack-optimizations-for-ai/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://vente.medium.com/mlperf-vs-my-neural-net-training-time-nightmare-1a0a5ee624b6?source=post_internal_links---------4----------------------------\n",
      "Processing: https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/\n",
      "Processing: https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html\n",
      "Processing: https://medium.com/@prakhargannu/attention-mechanism-in-deep-learning-simplified-d6a5830a079d\n",
      "Processing: https://www.unthinkable.co/blog/exploring-the-concept-of-attention-mechanism-in-deep-learning/\n",
      "Processing: https://insights.daffodilsw.com/blog/what-is-the-attention-mechanism-in-deep-learning\n",
      "Processing: https://blogs.nvidia.com/blog/what-is-a-transformer-model/\n",
      "Processing: https://www.datacamp.com/tutorial/how-transformers-work\n",
      "Processing: https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power\n",
      "Processing: https://blog.nelhage.com/post/transformers-for-software-engineers/\n",
      "Processing: https://www.reddit.com/r/learnmachinelearning/comments/12r27jp/understanding_transformer_architecture/\n",
      "Processing: https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n",
      "Processing: https://medium.com/@farzad.karami/decoding-the-magic-of-self-attention-a-deep-dive-into-its-intuition-and-mechanisms-394aa98f34c5\n",
      "Processing: https://www.geeksforgeeks.org/self-attention-in-nlp/\n",
      "Processing: https://www.reddit.com/r/deeplearning/comments/k5wn5k/resourcespapers_to_understand_transformers_and/\n",
      "Processing: https://www.scaler.com/topics/deep-learning/attention-mechanism-deep-learning/\n",
      "Processing: https://medium.com/@kramiknakrani100/deep-dive-into-multi-head-attention-revolutionizing-deep-learning-f9270eb5f30d\n",
      "Processing: https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/\n",
      "Processing: https://theaisummer.com/self-attention/\n",
      "Processing: https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\n",
      "Processing: https://www.reddit.com/r/learnmachinelearning/comments/lu9spi/multihead_attention_is_changing_deep_learning_in/\n",
      "Processing: https://medium.com/data-science-community-srm/understanding-encoders-decoders-with-attention-based-mechanism-c1eb7164c581\n",
      "Processing: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
      "Processing: https://huggingface.co/blog/bert-101\n",
      "Processing: https://medium.com/@igniobydigitate/bert-a-beginner-friendly-explanation-876549f0ece2\n",
      "Processing: https://www.braveriver.com/blog/how-googles-bert-changed-natural-language-understanding/\n",
      "Processing: https://www.machinelearningmastery.com/a-brief-introduction-to-bert/\n",
      "Processing: https://blog.gregbrockman.com/how-i-became-a-machine-learning-practitioner\n",
      "Processing: https://www.springboard.com/blog/data-science/machine-learning-gpt-3-open-ai/\n",
      "Processing: https://www.einfochips.com/blog/openai-gpt-3-the-most-powerful-language-model-an-overview/\n",
      "Processing: https://aws.amazon.com/what-is/gpt/\n",
      "Processing: https://www.grammarly.com/blog/ai/what-is-gpt-3/\n",
      "Processing: https://medium.com/codecontent/introduction-to-llama-a-paradigm-shift-in-ai-language-models-0836c6048a05\n",
      "Processing: https://ai.meta.com/blog/large-language-model-llama-meta-ai/\n",
      "Processing: https://www.datacamp.com/blog/introduction-to-meta-ai-llama\n",
      "Processing: https://www.geeksforgeeks.org/what-is-llama/\n",
      "Processing: https://pauldeepakraj-r.medium.com/unraveling-the-limitations-of-llama-v2-an-in-depth-exploration-63a29bb3f723\n",
      "Processing: https://blog.google/technology/ai/google-gemini-ai/\n",
      "Processing: https://www.wovenware.com/blog/2024/02/gemini-ai-google-computer-vision-revolution/\n",
      "Processing: https://www.vlinkinfo.com/blog/gemini-ai-everything-you-need-to-know/\n",
      "Processing: https://www.zdnet.com/article/i-asked-gemini-and-gpt-4-to-explain-deep-learning-ai-and-gemini-won-hands-down/\n",
      "Processing: https://unfoldai.com/lessons-from-googles-gemini/\n",
      "Processing: https://medium.com/@tomskiecke/claude-ai-revolutionizing-web-development-fd675b52a05b\n",
      "Processing: https://www.reddit.com/r/ClaudeAI/comments/1e9nmkl/software_devs_how_are_you_preparingupskilling_for/\n",
      "Processing: https://618media.com/en/blog/the-science-behind-claude-ais-models/\n",
      "Processing: https://www.linkedin.com/pulse/cracking-code-how-claude-helping-release-my-inner-developer-moran-7rcbe\n",
      "Processing: https://idratherbewriting.com/blog/writing-full-length-articles-with-claude-ai\n",
      "Processing: https://research.ibm.com/blog/Granite-adapter-experiments\n",
      "Processing: https://syncedreview.com/2024/05/13/ibms-granite-code-powering-enterprise-software-development-with-ai-precision/\n",
      "Processing: https://www.datacamp.com/blog/what-is-prompt-engineering-the-future-of-ai-communication\n",
      "Processing: https://github.blog/ai-and-ml/generative-ai/prompt-engineering-guide-generative-ai-llms/\n",
      "Processing: https://www.altexsoft.com/blog/prompt-engineering/\n",
      "Processing: https://www.scrums.com/blog/the-differences-between-ai-prompt-and-software-engineers\n",
      "Processing: https://digitate.com/blog/what-is-prompt-engineering/\n",
      "Processing: https://medium.com/@mattchinnock/llms-and-machine-learning-for-software-engineers-a7634fab109a\n",
      "Processing: https://insights.sei.cmu.edu/blog/application-of-large-language-models-llms-in-software-engineering-overblown-hype-or-disruptive-change/\n",
      "Processing: https://dev.to/wesen/llms-will-fundamentally-change-software-engineering-3oj8\n",
      "Processing: https://blogs.nvidia.com/blog/what-are-large-language-models-used-for/\n",
      "Processing: https://zahere.com/demystifying-large-language-models-a-guide-for-software-developers\n",
      "Processing: https://www.machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/\n",
      "Processing: https://www.ibm.com/topics/zero-shot-learning\n",
      "Processing: https://www.datacamp.com/tutorial/zero-shot-prompting\n",
      "Processing: https://www.vellum.ai/blog/zero-shot-vs-few-shot-prompting-a-guide-with-examples\n",
      "Processing: https://www.digital-adoption.com/zero-shot-prompting/\n",
      "Processing: https://softwareguide.medium.com/mastering-few-shot-prompting-a-comprehensive-guide-6eda3761538c\n",
      "Processing: https://learnprompting.org/docs/basics/few_shot?srsltid=AfmBOoq_hwuxpq2DVanTRoplAwEoZkUQvTA5HOyjl1RqBf14r-yOxg5w\n",
      "Processing: https://www.digital-adoption.com/what-is-few-shot-prompting-examples-uses/\n",
      "Processing: https://www.datacamp.com/tutorial/few-shot-prompting\n",
      "Processing: https://serokell.io/blog/chain-of-thought-prompting-llms\n",
      "Processing: https://towardsai.net/p/artificial-intelligence/understanding-chain-of-thought-cot-reasoning-the-core-behind-openais-o1-model\n",
      "Processing: http://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/\n",
      "Processing: https://annotationbox.com/chain-of-thought-prompting/\n",
      "Processing: https://www.prompthub.us/blog/chain-of-thought-prompting-guide\n",
      "Processing: https://medium.com/@vikrampande783/introduction-to-langchain-9e09aae37e62\n",
      "Processing: https://aws.amazon.com/what-is/langchain/\n",
      "Processing: https://www.useready.com/blog/building-better-llm-applications-with-langchain\n",
      "Processing: https://towardsai.net/p/l/understanding-langchain-%EF%B8%8F-part2\n",
      "Processing: https://medium.com/@gurinderjeetkaurnatt/generative-ai-with-langchain-ee9cc5078080\n",
      "Processing: https://medium.com/llamaindex-blog/llamaindex-metaphor-towards-automating-knowledge-work-with-llms-5520a32efa2f\n",
      "Processing: https://towardsai.net/p/artificial-intelligence/a-complete-guide-to-rag-and-llamaindex\n",
      "Processing: https://www.llamaindex.ai/blog/tag/machine-learning\n",
      "Processing: https://towardsai.net/p/machine-learning/unlocking-data-science-how-gemini-pro-and-llama-index-will-transform-your-workflow\n",
      "Processing: https://www.useready.com/blog/rag-wars-llama-index-vs-langchain-showdown\n",
      "Processing: https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\n",
      "Processing: https://machine-learning-made-simple.medium.com/an-overview-of-how-to-do-retrieval-augmented-generation-3075292c0bed\n",
      "Processing: https://aws.amazon.com/what-is/retrieval-augmented-generation/\n",
      "Processing: https://medium.com/@ceo_44783/what-ive-learned-in-10-months-of-doing-rag-retrieval-augmented-generation-0520563ad256\n",
      "Processing: https://research.ibm.com/blog/retrieval-augmented-generation-RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://medium.com/pinterest-engineering/understanding-pins-through-keyword-extraction-40cf94214c18\n",
      "Processing: https://www.seoclarity.net/blog/machine-learning-and-seo-16591/\n",
      "Processing: https://blog.google/products/search/search-language-understanding-bert/\n",
      "Processing: https://softwaredoug.com/blog/2024/06/25/what-ai-engineers-need-to-know-search\n",
      "Processing: https://www.quora.com/What-is-a-great-blog-for-machine-learning\n",
      "Processing: https://encord.com/blog/embeddings-machine-learning/\n",
      "Processing: https://medium.com/@alok.g.v/understanding-embedding-machine-learning-6b0712242bef\n",
      "Processing: https://developers.google.com/machine-learning/crash-course/embeddings\n",
      "Processing: https://aws.amazon.com/what-is/embeddings-in-machine-learning/\n",
      "Processing: https://www.reddit.com/r/learnmachinelearning/comments/tfpl7c/a_deep_dive_into_word_embeddings_nlp/\n",
      "Processing: https://medium.com/@aikho/deep-learning-in-information-retrieval-part-ii-dense-retrieval-1f9fecb47de9\n",
      "Processing: https://www.amazon.science/blog/from-structured-search-to-learning-to-rank-and-retrieve\n",
      "Processing: https://github.com/sebastian-hofstaetter/teaching/blob/master/advanced-information-retrieval/Lecture%2010%20-%20Closed%20Captions.md\n",
      "Processing: https://news.ycombinator.com/item?id=39109469\n",
      "Processing: https://medium.com/womenintechnology/ai-c3412c5aa0ac\n",
      "Processing: https://towardsdatascience.com/deep-learning-and-machine-learning-c1101debe0c\n",
      "Processing: https://www.fullstackacademy.com/blog/what-is-deep-learning\n",
      "Processing: https://medium.com/cracking-the-data-science-interview/datacast-e117-vector-databases-the-embeddings-revolution-and-working-in-china-with-frank-liu-ebd7a157b49d\n",
      "Processing: https://lakefs.io/blog/what-is-vector-databases/\n",
      "Processing: https://zilliz.com/learn/what-is-vector-database\n",
      "Processing: https://www.pinecone.io/learn/vector-database/\n",
      "Processing: https://www.reddit.com/r/learnmachinelearning/comments/1gte2j4/vector_databases_explained_in_2_minutes/\n",
      "Processing: https://machinelearningmastery.com/building-graph-rag-system-step-by-step-approach/\n",
      "Processing: https://medium.com/@sahin.samia/graph-rag-in-ai-what-is-it-and-how-does-it-work-d719d814e610\n",
      "Processing: https://www.linkedin.com/posts/elena-kohlwey-00924a14b_graphrag-field-guide-navigating-the-world-activity-7242436090630946816-wy0f\n",
      "Processing: https://towardsai.net/p/l/graphrag-is-the-logical-step-from-rag-so-why-the-sudden-hype\n",
      "Processing: https://markovate.com/agentic-rag/\n",
      "Processing: https://iamshobhitagarwal.medium.com/agentic-retrieval-augmented-generation-rag-a-comprehensive-guide-2872683fa773\n",
      "Processing: https://www.reddit.com/r/Rag/comments/1gqv7ei/rant_are_we_really_going_with_agentic_rag_now/\n",
      "Processing: https://blogs.nvidia.com/blog/what-is-a-pretrained-ai-model/\n",
      "Processing: https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/\n",
      "Processing: https://soon-yau.medium.com/speeding-up-deep-learning-with-quantization-3fe3538cbb9\n",
      "Processing: https://www.reddit.com/r/learnmachinelearning/comments/1dkkg7z/one_of_my_first_blog_posts_quantization_basics/\n",
      "Processing: https://deepganteam.medium.com/three-flavors-of-quantization-cc5be18e7ab4\n",
      "Processing: https://towardsai.net/p/machine-learning/llm-quantization-intuition-simple-explaination\n",
      "Processing: https://sertiscorp.medium.com/machine-learning-engineer-vs-software-engineer-what-are-the-differences-a4047a8a8c2e\n",
      "Processing: https://parallelstaff.com/deep-learning-vs-machine-learning/\n",
      "Processing: https://www.edge-ai-vision.com/2024/05/fully-sharded-data-parallelism-fsdp/\n",
      "Processing: https://engineering.fb.com/2021/07/15/open-source/fsdp/\n",
      "Processing: https://medium.com/@siddharthashrestha/an-introduction-to-fsdp-fully-sharded-data-parallel-for-distributed-training-5e67adfa1712\n",
      "Processing: https://www.linkedin.com/posts/dr-akash-sri_from-deepspeed-to-fsdp-and-back-again-with-activity-7207125223622545408-pzXF\n",
      "Processing: https://www.linkedin.com/posts/chiravdave_distributed-training-demystified-a-beginner-activity-7263501075175882752-DY38\n",
      "Processing: https://medium.com/@sujathamudadla1213/zero-redundancy-optimization-zero-in-deep-learning-895a60f06a8c\n",
      "Processing: https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/\n",
      "Processing: https://oracle-oci-ocas.medium.com/zero-redundancy-optimizers-a-method-for-training-machine-learning-models-with-billion-parameter-472e8f4e7a5b\n",
      "Processing: https://pub.towardsai.net/the-zero-redundancy-optimizer-zero-a-short-introduction-with-python-8db4fd07601d\n",
      "Processing: https://www.amazon.science/blog/making-deepspeed-zero-run-efficiently-on-more-affordable-hardware\n",
      "Processing: https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications\n",
      "Processing: https://www.linkedin.com/posts/barralexandra_chinchilla-explainedhow-to-read-deepminds-activity-7075517400284057600-6TT0\n",
      "Processing: https://medium.com/@ronnyh/research-paper-summary-chinchilla-training-compute-optimal-large-language-models-6073e0c83eb4\n",
      "Processing: https://www.graphcore.ai/posts/great-teachers-and-beyond-chinchilla-papers-of-the-month-jan-2024\n",
      "Processing: https://www.turing.com/kb/deepminds-chinchilla-ai\n",
      "Processing: https://medium.com/@genedarocha/what-is-the-development-of-bloomberggpt-860c0ab0d292\n",
      "Processing: https://snorkel.ai/blog/bloomberg-s-gideon-mann-on-the-power-of-domain-specialist-llms-bloomberggpt/\n",
      "Processing: https://www.linkedin.com/posts/pyquant-news_bloomberggpt-a-large-language-model-for-activity-7197991023124353025-dCX4\n",
      "Processing: https://medium.com/codex/bloomberggpt-the-first-large-language-model-for-finance-61cc92075075\n",
      "Processing: https://towardsai.net/p/l/bloomberggpt-the-first-gpt-for-finance\n",
      "Processing: https://medium.com/@amanatulla1606/fine-tuning-the-model-what-why-and-how-e7fa52bc8ddf\n",
      "Processing: https://www.ibm.com/topics/fine-tuning\n",
      "Processing: https://www.multimodal.dev/post/understanding-fine-tuning-in-deep-learning\n",
      "Processing: https://www.kdnuggets.com/2016/05/explain-machine-learning-software-engineer.html\n",
      "Processing: https://www.leewayhertz.com/parameter-efficient-fine-tuning/\n",
      "Processing: https://softwaremind.com/blog/parameter-efficient-fine-tuning-peft-benefits-and-techniques/\n",
      "Processing: https://www.ibm.com/think/topics/parameter-efficient-fine-tuning\n",
      "Processing: https://www.calibraint.com/blog/what-is-parameter-efficient-fine-tuning\n",
      "Processing: https://medium.com/intro-to-artificial-intelligence/parameter-efficient-finetuning-peft-of-llm-710831c0ffb3\n",
      "Processing: https://medium.com/@zhonghong9998/multi-task-learning-enhancing-model-efficiency-and-generalization-4d6f5ffd2fa7\n",
      "Processing: https://careersatdoordash.com/blog/improving-etas-with-multi-task-models-deep-learning-and-probabilistic-forecasts/\n",
      "Processing: https://www.ruder.io/multi-task/\n",
      "Processing: https://adasci.org/fine-tuning-pre-trained-multitask-llms-a-comprehensive-guide/\n",
      "Processing: https://towardsai.net/p/data-science/single-vs-multi-task-llm-instruction-fine-tuning\n",
      "Processing: https://www.ml6.eu/blogpost/low-rank-adaptation-a-technical-deep-dive\n",
      "Processing: https://www.linkedin.com/posts/zainhas_explanation-of-low-rank-adaptation-lora-activity-7223369220862922752-v0B4\n",
      "Processing: https://medium.com/@Shrishml/lora-low-rank-adaptation-from-the-first-principle-7e1adec71541\n",
      "Processing: https://datascientest.com/en/low-rank-adaptation-understanding-definition-applications-and-challenges\n",
      "Processing: https://medium.com/@adimodi96/low-rank-adaptation-lora-explained-9e64b7b0a5f1\n",
      "Processing: https://developer.nvidia.com/blog/an-introduction-to-large-language-models-prompt-engineering-and-p-tuning/\n",
      "Processing: https://dev.to/avinashvagh/understanding-the-concept-of-natural-language-processing-nlp-and-prompt-engineering-35hg\n",
      "Processing: https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766\n",
      "Processing: https://www.reddit.com/r/MachineLearning/comments/15lnfbh/a_blog_on_lora_and_qlora_finetuning_techniques_p/\n",
      "Processing: https://www.brev.dev/blog/how-qlora-works\n",
      "Processing: https://towardsdatascience.com/leveraging-qlora-for-fine-tuning-of-task-fine-tuned-models-without-catastrophic-forgetting-d9bcd594cff4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://www.linkedin.com/posts/optimumai_peft-newsletter-ai-activity-7201972096032272384-uGEa\n",
      "Processing: https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
      "Processing: https://www.traceloop.com/blog/evaluating-model-performance-with-the-rouge-metric-a-comprehensive-guide\n",
      "Processing: https://www.linkedin.com/advice/1/what-rouge-score-how-can-you-use-evaluate-nlp-euj9e\n",
      "Processing: https://towardsdatascience.com/to-rouge-or-not-to-rouge-6a5f3552ea45\n",
      "Processing: https://medium.com/free-code-camp/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840\n",
      "Processing: https://kantanmtblog.com/2015/07/14/understanding-bleu-for-machine-translation/\n",
      "Processing: https://www.traceloop.com/blog/demystifying-the-bleu-metric\n",
      "Processing: https://kvashee.medium.com/understanding-mt-quality-bleu-scores-9a19ed20526d\n",
      "Processing: https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213\n",
      "Processing: https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb\n",
      "Processing: https://encord.com/blog/f1-score-in-machine-learning/\n",
      "Processing: https://arize.com/blog-course/f1-score/\n",
      "Processing: https://serokell.io/blog/a-guide-to-f1-score\n",
      "Processing: https://www.v7labs.com/blog/f1-score-guide\n",
      "Processing: https://www.geeksforgeeks.org/f1-score-in-machine-learning/\n",
      "Processing: https://www.noidea.dog/glue\n",
      "Processing: https://h2o.ai/wiki/glue/\n",
      "Processing: https://medium.com/@pradoshkumar.jena/understanding-benchmarking-in-nlp-glue-superglue-helm-mmlu-and-big-bench-2e0a55b57d3b\n",
      "Processing: https://aws.amazon.com/blogs/machine-learning/category/analytics/aws-glue/\n",
      "Processing: https://venturebeat.com/ai/ai-researchers-launch-superglue-a-rigorous-benchmark-for-language-understanding/\n",
      "Processing: https://www.interviewquery.com/p/software-engineering-vs-machine-learning\n",
      "Processing: https://newsletter.pragmaticengineer.com/p/what-is-ml-engineering\n",
      "Processing: https://christiangrech.medium.com/unlock-faster-llm-serving-with-vllm-a-step-by-step-guide-331afc2f5bf5\n",
      "Processing: https://medium.com/@asimsultan2/vllm-a-deep-dive-into-efficient-llm-inference-and-serving-17804bf047df\n",
      "Processing: http://muratbuffalo.blogspot.com/2016/12/learning-machine-learning-beginners.html\n",
      "Processing: https://encord.com/blog/vision-language-models-guide/\n",
      "Processing: https://community.nasscom.in/index.php/communities/ai/understanding-vllm-virtual-large-language-model-revolution\n",
      "Processing: https://towardsdatascience.com/deepspeed-deep-dive-model-implementations-for-inference-mii-b02aa5d5e7f7\n",
      "Processing: https://www.ideas2it.com/blogs/deepspeed-mii-made-easy\n",
      "Processing: https://www.deepspeed.ai/\n",
      "Processing: https://medium.com/design-bootcamp/advancing-machine-learning-with-deepspeed-mii-and-stable-diffusion-c65f3960ac4b\n",
      "Processing: https://www.microsoft.com/en-us/research/project/deepspeed/microsoft-research-blog/\n",
      "Processing: https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/\n",
      "Processing: https://medium.com/@abhilashkrish/deep-dive-into-nvidia-tensorrt-model-parsing-optimization-and-high-performance-inference-07af563d0f8d\n",
      "Processing: https://blog.roboflow.com/what-is-tensorrt/\n",
      "Processing: https://medium.com/the-techlife/tensorrt-an-overview-2023-ce32cb9509dc\n",
      "Processing: https://developer.nvidia.com/blog/optimizing-and-serving-models-with-nvidia-tensorrt-and-nvidia-triton/\n",
      "Processing: https://www.datacamp.com/tutorial/hugging-faces-text-generation-inference-toolkit-for-llms\n",
      "Processing: https://huggingface.co/blog\n",
      "Processing: https://www.ideas2it.com/blogs/deploying-llm-powered-applications-in-production-using-tgi\n",
      "Processing: https://www.linkedin.com/posts/jeffboudier_github-huggingfacetext-generation-inference-activity-7090755444129861632-NWj2\n",
      "Processing: https://huggingface.co/blog/community\n",
      "Processing: https://blogs.rstudio.com/tensorflow/posts/2023-06-22-understanding-lora/\n",
      "Processing: https://medium.com/@meghanheintz/gentle-introduction-to-lora-low-rank-adaptation-for-finetuning-167be61731a6\n",
      "Processing: https://www.machinelearningmastery.com/using-lora-in-stable-diffusion/\n",
      "Processing: https://mlsys.stanford.edu/\n",
      "Processing: https://www.quora.com/Do-deep-learning-machine-learning-professionals-test-run-their-codes-on-their-own-laptop-or-on-a-remote-computer-cloud\n",
      "Processing: https://aws.amazon.com/blogs/machine-learning/efficient-and-cost-effective-multi-tenant-lora-serving-with-amazon-sagemaker/\n",
      "Processing: https://huggingface.co/blog/rlhf\n",
      "Processing: https://codingscape.com/blog/what-is-rlhf-reinforcement-learning-from-human-feedback\n",
      "Processing: https://blog.pangeanic.com/what-is-reinforcement-learning-from-human-feedback-rlhf-how-it-works\n",
      "Processing: https://www.lakera.ai/blog/reinforcement-learning-from-human-feedback\n",
      "Processing: https://aws.amazon.com/blogs/machine-learning/improving-your-llms-with-rlhf-on-amazon-sagemaker/\n",
      "Processing: https://www.reddit.com/r/reinforcementlearning/comments/gs2mj5/blog_series_on_proximal_policy_optimization/\n",
      "Processing: https://medium.com/@chris.p.hughes10/understanding-ppo-a-game-changer-in-ai-decision-making-explained-for-rl-newcomers-913a0bc98d2b\n",
      "Processing: https://medium.com/intro-to-artificial-intelligence/proximal-policy-optimization-ppo-a-policy-based-reinforcement-learning-algorithm-3cf126a7562d\n",
      "Processing: https://datascientest.com/en/proximal-policy-optimization-all-about-the-algorithm-created-by-openai\n",
      "Processing: https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149\n",
      "Processing: https://medium.com/@jonnyndavis/understanding-constitutional-ai-dd9d783ef712\n",
      "Processing: https://www.solventum.com/en-us/home/health-information-technology/resources-education/blog/2023/6/ai-talk-naturalness-of-software-and-constitutional-ai/\n",
      "Processing: https://medium.com/@lekefbi/constitutional-ai-for-harmless-ai-a3d76cb79149\n",
      "Processing: https://www.cornelllawreview.org/wp-content/uploads/2020/12/Huq-final.pdf\n",
      "Processing: https://arxiv.org/abs/2212.08073\n"
     ]
    }
   ],
   "source": [
    "extracted_google_blog_data = process_links(all_google_blog_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "3391565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = defaultdict(lambda: \"\")\n",
    "for key,value in cleaned_extracted_data.items():\n",
    "    links_dict[key] = value\n",
    "for key,value in cleaned_extracted_hw_data.items():\n",
    "    links_dict[key] = value\n",
    "for key,value in extracted_google_blog_data.items():\n",
    "    links_dict[key] = value\n",
    "cleaned_links_dict = {key:value for key, value in links_dict.items() if len(value)>=1000}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa9688",
   "metadata": {},
   "source": [
    "<h1>Chunking scraped data from links for VDB</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "e866eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using a regex-based sentence tokenizer.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'(?<=[.!?]) +')  # Match end of sentence followed by space\n",
    "    return sentence_endings.split(text)\n",
    "\n",
    "def chunk_text_by_sentence(text: str, max_tokens: int, tokenizer) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk text into pieces of max_tokens length, ensuring chunks do not cut sentences.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to chunk.\n",
    "        max_tokens (int): The maximum number of tokens per chunk.\n",
    "        tokenizer: The tokenizer instance for tokenizing the text.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    current_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.encode(sentence)\n",
    "        if current_tokens + len(sentence_tokens) <= max_tokens:\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += len(sentence_tokens)\n",
    "        else:\n",
    "            # Complete the current chunk\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            # Start a new chunk\n",
    "            current_chunk = [sentence]\n",
    "            current_tokens = len(sentence_tokens)\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_documents_by_sentence(documents: Dict[str, str], max_tokens: int = 500) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Chunk the text of multiple documents into smaller pieces, ensuring no sentence is cut.\n",
    "    \n",
    "    Args:\n",
    "        documents (Dict[str, str]): A dictionary with document IDs as keys and text as values.\n",
    "        max_tokens (int): The maximum number of tokens per chunk.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]: A dictionary with document IDs as keys and lists of chunked text as values.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Use the tokenizer compatible with OpenAI models\n",
    "    chunked_documents = {}\n",
    "    \n",
    "    for doc_id, text in documents.items():\n",
    "        chunked_documents[doc_id] = chunk_text_by_sentence(text, max_tokens, tokenizer)\n",
    "    \n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "1b501cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_links_dict = chunk_documents_by_sentence(cleaned_links_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "140db7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data_from_embedded_links.json\"\n",
    "with open(file_name, \"w\") as json_file:\n",
    "    json.dump(chunked_links_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52cc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2003452",
   "metadata": {},
   "source": [
    "<h1>Pulling Q and A docs from Quizlet</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "76cec1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q_and_a_docs_final = all_q_and_a_docs + all_q_and_a_docs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5d3a0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"all_q_and_a_docs_final.json\"\n",
    "with open(file_name, \"w\") as json_file:\n",
    "    json.dump(all_q_and_a_docs_final, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "727398f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'Large Language Model',\n",
       "  'output': 'A type of foundation model applied specifically to text with the ability to understand and generate human language, enabling applications such as translation, summarization, and question-answering. Foundation Model: Pre-trained on large amounts of unlabeled and self-supervised data for very general use cases.'},\n",
       " {'input': 'Transformer',\n",
       "  'output': 'A type of neural network architecture designed for handling sequences of data, particularly in natural language processing tasks. Transformers are known for their self-attention mechanism, which allows them to weigh the importance of different parts of an input sequence. They learn context and track relationships in sequential data like words in a sentence.'},\n",
       " {'input': 'Pretraining',\n",
       "  'output': 'The initial phase of training a large language model, during which the model learns general language patterns and structures from a vast corpus of text data.'},\n",
       " {'input': 'Fine tuning',\n",
       "  'output': 'The second phase of training a large language model, during which the model is fine-tuned on a smaller, domain-specific dataset to specialize in a particular task or field.'},\n",
       " {'input': 'Tokenization',\n",
       "  'output': 'The process of breaking down text into individual words or subwords, called tokens, which are then used as input for a language model.'},\n",
       " {'input': 'Vocabulary',\n",
       "  'output': 'The set of unique tokens (words or sub-words) recognized by a large language model, used for both input and output text generation.'},\n",
       " {'input': 'Context Window',\n",
       "  'output': 'The maximum number of tokens a language model can consider from the input text when generating a response or prediction.'},\n",
       " {'input': 'Zero Shot Learning',\n",
       "  'output': 'The ability of a pre-trained language model to perform a task without any additional fine-tuning or task-specific training, relying only on its general understanding of language.'},\n",
       " {'input': 'Few Shot Learning',\n",
       "  'output': 'The ability of a pre-trained language model to perform a task with minimal fine-tuning or exposure to task-specific examples.'},\n",
       " {'input': 'Transfer Learning',\n",
       "  'output': 'The process of leveraging the knowledge acquired by a model during pre-training on one task to improve performance on a different, but related, task.'},\n",
       " {'input': 'Model Size',\n",
       "  'output': 'The number of parameters (weights and biases) in a neural network, often used as a measure of the complexity and capacity of a language model.'},\n",
       " {'input': 'Bias',\n",
       "  'output': \"The presence of unfair or unjustified assumptions in a language model's output, often resulting from biases present in the training data.\"},\n",
       " {'input': 'Overfitting',\n",
       "  'output': 'A situation in which a model becomes too specialized to its training data, leading to poor performance on new or unseen data.'},\n",
       " {'input': 'Generalization',\n",
       "  'output': 'The ability of a model to perform well on new, unseen data, by learning the underlying patterns and structures of the training data without memorizing specific examples.'},\n",
       " {'input': 'Embedding',\n",
       "  'output': 'Expressing words/sentences as vectors, or an array of real values that represent characteristics of the word or sentence.'},\n",
       " {'input': 'Multitask Learning',\n",
       "  'output': 'Collect a dataset of training/test/development data for a range of different tasks, training examples are of the form (dataset, objective) sampled from the distribution of dataset & objectives, in a probabilistic framework, task: estimate a conditional distribution: p(output|input, task).'},\n",
       " {'input': 'Positional Embedding', 'output': 'Capturing word order.'},\n",
       " {'input': 'One-Shot',\n",
       "  'output': 'In addition to the task description, the model sees the a single example of the task.'},\n",
       " {'input': 'RAG (Retrieval Augmented Generation)',\n",
       "  'output': \"Stores knowledge in a database and if it's knowledge that the LLM can't answer, searches this database and processes it into the LLM. - Consists of vector database and embedding technology (to convert text into vectors).\"},\n",
       " {'input': 'Seq2Seq model',\n",
       "  'output': 'A special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc.'},\n",
       " {'input': 'Attention head',\n",
       "  'output': 'A specialized mini-brain within the AI model that helps it selectively focus on certain aspects of the input data. In the context of NLP, attention heads aid in understanding the relationships between words in a sentence or a sequence of text.'},\n",
       " {'input': 'Hallucination',\n",
       "  'output': 'Incorrect information is learned and given by the LLM as a confident answer.'},\n",
       " {'input': 'Recurrent layer',\n",
       "  'output': \"A type of deep neural network where both input data and prior hidden states are fed into the network's layers, giving the network a state and hence memory. RNNs are commonly used for sequence-based or time-based data.\"},\n",
       " {'input': 'Autoregressive',\n",
       "  'output': 'A model that learns from a series of timed steps and takes measurements from previous actions as inputs, in order to predict the value of the next time step.'},\n",
       " {'input': 'Machine learning',\n",
       "  'output': 'A type of artificial intelligence that leverages massive amounts of data so that computers can improve the accuracy of actions and predictions on their own without additional programming.'},\n",
       " {'input': 'Deep Learning',\n",
       "  'output': 'A subset of machine learning, which is essentially a neural network with three or more layers. These neural networks attempt to simulate the behavior of the human brain—albeit far from matching its ability—allowing it to \"learn\" from large amounts of data.'},\n",
       " {'input': 'Decoder-only transformer architecture',\n",
       "  'output': 'Designed to generate/create new text. Produces contextually relevant, coherent text. They receive input and they generate text relevant to that input. During pre-training, its task is to predict the next word in each sequence of text giving it the ability to understand and generate human-like text.**Tokens look at previous tokens.'},\n",
       " {'input': 'Encoder-only transformer architecture',\n",
       "  'output': \"Encoder-only models find their place in scenarios where understanding context is paramount but autoregressive generation isn't necessary (previous text doesn't really matter). By excelling in capturing contextual information, they thrive in tasks such as sentiment analysis, where interpreting the sentiment of a text requires a holistic grasp of its context. Additionally, they excel in tasks like named entity recognition, where identifying entities like names, dates, and locations demands a comprehensive understanding of the input.**Tokens look at each other.\"},\n",
       " {'input': 'Encoder-decoder transformer architecture',\n",
       "  'output': 'Encoder-decoder models are typically used for natural language processing tasks that involve understanding input sequences and generating output sequences, often with different lengths and structures. They are particularly good at tasks where there is a complex mapping between the input and output sequences and where it is crucial to capture the relationships between the elements in both sequences. Some common use cases for encoder-decoder models include text translation and summarization. Good at analyzing text and somewhat good at generating.'},\n",
       " {'input': 'Embedding layer', 'output': 'Creates embeddings from input text.'},\n",
       " {'input': 'Feedforward layer',\n",
       "  'output': \"Multiple connected layers transform the input embeddings to glean higher-level abstractions and understand the user's intent with the text input.\"},\n",
       " {'input': 'Agents',\n",
       "  'output': 'System that uses an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools. They consist of an agent core, a memory module, tools, and a planning module.'},\n",
       " {'input': 'Agent core',\n",
       "  'output': 'Foundational component built around an LLM. Decision-making module that manages behavioral characteristics of the agent. Contains overall objectives, tools for execution, explanation of planning modules, memory of past questions.'},\n",
       " {'input': 'Memory module',\n",
       "  'output': 'Store of internal logs and interactions. Both short-term (sentence by sentence) memory and long-term (conversation history) memory.'},\n",
       " {'input': 'Tools',\n",
       "  'output': 'External resources, services, or third-party APIs that agents can use to execute tasks and enhance capabilities. This includes databases, knowledge bases, external models. Ex. using a RAG pipeline to generate context-aware answers, API to search information online.'},\n",
       " {'input': 'Planning module',\n",
       "  'output': 'Plans out nuanced approaches for complicated questions. -Task and question decomposition: Breaking down one question into multiple subparts-Reflection/critic: Techniques to refine execution plan.'},\n",
       " {'input': 'Structured data',\n",
       "  'output': 'Data that fits neatly into data tables and includes discrete data types such as numbers, short text, and dates.'},\n",
       " {'input': 'Unstructured data',\n",
       "  'output': \"Data that doesn't fit neatly into a data table because its size or nature: for example, audio and video files and large text documents. Also, sentences.\"},\n",
       " {'input': 'Knowledge graph',\n",
       "  'output': 'Well suited for handling complex, multi-part collection since they store data as a network of nodes and the relationship between them. This connected data structure allows RAG apps to navigate from one piece of information to another efficiently, accessing all related information.'},\n",
       " {'input': 'Information extraction pipeline',\n",
       "  'output': 'Transformation of unstructured text into structured information. 1. Run input text through a coreference resolution model: Find all expressions that refer to a specific entity. 2. Entity disambiguation step: Accurately identifying and distinguishing between entities with similar names or references. 3. Identify relationships between entities. When combined with knowledge graphs, you can process each document individually and interconnect the different documents.'},\n",
       " {'input': 'Multi-hop question-answering task',\n",
       "  'output': \"LLM needs information from multiple documents/chunks of text to generate an answer. Chunking + embedding documents doesn't work because: 1. Provided documents might not necessarily contain all information to answer question fully. 2. Missing reference information: Some chunks may not contain the full context and there could be missing references. 3. Hard to identify ideal number of retrieved documents. Solution: Knowledge graphs. They're great with sorting and aggregating unstructured text data.\"},\n",
       " {'input': 'Knowledge graph nodes', 'output': 'Represent entities.'},\n",
       " {'input': 'Knowledge graph edges', 'output': 'Represent relationships.'},\n",
       " {'input': 'Why do we use a knowledge graph for RAG applications?',\n",
       "  'output': '1. Reduced workload during query time, improving latency. 2. Easier traversal and navigation through interconnected documents, enabling multi-hop reasoning. 3. Can easily absorb all types of data.'},\n",
       " {'input': 'Which in-context learning method involves creating an initial prompt that states the task to be completed and includes a single example question with answer followed by a second question to be answered by the LLM?',\n",
       "  'output': 'd. One Shot. One shot inference involves providing an example question with answer followed by a second question to be answered by the LLM. Few shot inference provides multiple example prompts and answers while zero shot provides only one prompt to be answered by the LLM.'},\n",
       " {'input': 'Which configuration parameter for inference can be adjusted to either increase or decrease randomness within the model output layer?',\n",
       "  'output': 'c. Temperature. Temperature is used to affect the randomness of the output of the softmax layer. A lower temperature results in reduced variability while a higher temperature results in increased randomness of the output.'},\n",
       " {'input': 'Which of the following best describes the role of data parallelism in the context of training Large Language Models (LLMs) with GPUs?',\n",
       "  'output': 'd. Data parallelism allows for the use of multiple GPUs to process different parts of the same data simultaneously, speeding up training time. Data parallelism is a strategy that splits the training data across multiple GPUs. Each GPU processes a different subset of the data simultaneously, which can greatly speed up the overall training time.'},\n",
       " {'input': 'Which of the following statements about pretraining scaling laws are correct? Select all that apply.',\n",
       "  'output': \"a, b & c. a. To scale our model, we need to jointly increase dataset size and model size, or they can become a bottleneck for each other. b. There is a relationship between model size (in number of parameters) and the optimal number of tokens to train the model with. c. When measuring compute budget, we can use 'PetaFlops per second-Day' as a metric.\"},\n",
       " {'input': 'Interacting with Large Language Models (LLMs) differs from traditional machine learning models. Working with LLMs involves natural language input, known as a _____, resulting in output from the Large Language Model, known as the ______.',\n",
       "  'output': 'd. prompt, completion'},\n",
       " {'input': 'Large Language Models (LLMs) are capable of performing multiple tasks supporting a variety of use cases. Which of the following tasks supports the use case of converting code comments into executable code?',\n",
       "  'output': 'c. Translation'},\n",
       " {'input': 'What is the self-attention that powers the transformer architecture?',\n",
       "  'output': 'a. A mechanism that allows a model to focus on different parts of the input sequence during computation.'},\n",
       " {'input': 'Which of the following stages are part of the generative AI model lifecycle mentioned in the course? (Select all that apply)',\n",
       "  'output': 'b, c, d & e. b. Selecting a candidate model and potentially pre-training a custom model. c. Manipulating the model to align with specific project needs. d. Defining the problem and identifying relevant datasets. e. Deploying the model into the infrastructure and integrating it with the application.'},\n",
       " {'input': \"'RNNs are better than Transformers for generative AI Tasks.' Is this true or false?\",\n",
       "  'output': 'False'},\n",
       " {'input': 'Which transformer-based model architecture has the objective of guessing a masked token based on the previous sequence of tokens by building bidirectional representations of the input sequence?',\n",
       "  'output': 'c. Autoencoder'},\n",
       " {'input': 'Which transformer-based model architecture is well-suited to the task of text translation?',\n",
       "  'output': 'b. Sequence-to-sequence'},\n",
       " {'input': 'Do we always need to increase the model size to improve its performance?',\n",
       "  'output': 'False'},\n",
       " {'input': 'Scaling laws for pre-training large language models consider several aspects to maximize performance of a model within a set of constraints and available scaling choices. Select all alternatives that should be considered for scaling when performing model pre-training?',\n",
       "  'output': 'a, c & d. a. Compute budget: Compute constraints. c. Model size: Number of parameters. d. Dataset size: Number of tokens.'},\n",
       " {'input': \"'You can combine data parallelism with model parallelism to train LLMs.' Is this true or false?\",\n",
       "  'output': 'True'},\n",
       " {'input': 'Which of the following are true in respect to Catastrophic Forgetting? Select all that apply.',\n",
       "  'output': 'b, c & d. b. Catastrophic forgetting occurs when a machine learning model forgets previously learned information as it learns new information. c. Catastrophic forgetting is a common problem in machine learning, especially in deep learning models. d. One way to mitigate catastrophic forgetting is by using regularization techniques to limit the amount of change that can be made to the weights of the model during training.'},\n",
       " {'input': 'What is the purpose of fine-tuning with prompt datasets?',\n",
       "  'output': 'd. To improve the performance and adaptability of a pre-trained language model for specific tasks.'},\n",
       " {'input': \"'Parameter Efficient Fine-Tuning (PEFT) updates only a small subset of parameters. This helps prevent catastrophic forgetting.' True or False?\",\n",
       "  'output': 'True'},\n",
       " {'input': 'Parameter Efficient Fine-Tuning (PEFT) methods specifically attempt to address some of the challenges of performing full fine-training. Which of the following options describe challenges that PEFT tries to overcome?',\n",
       "  'output': 'a, b & c. a. Computational constraints. b. Catastrophic forgetting. c. Storage requirements.'},\n",
       " {'input': 'Fill in the blanks: __________ involves using many prompt-completion examples as the labeled training dataset to continue training the model by updating its weights. This is different from _________ where you provide prompt-completion examples during inference.',\n",
       "  'output': 'd. Instruction fine-tuning, In-context learning'},\n",
       " {'input': 'Fine-tuning a model on a single task can improve model performance specifically on that task; however, it can also degrade the performance of other tasks as a side effect. This phenomenon is known as:',\n",
       "  'output': 'd. Catastrophic forgetting'},\n",
       " {'input': 'Which evaluation metric below focuses on precision in matching generated output to the reference text and is used for text translation?',\n",
       "  'output': 'b. BLEU'},\n",
       " {'input': 'Which of the following statements about multi-task finetuning is correct? Select all that apply.',\n",
       "  'output': 'a & d. a. FLAN-T5 was trained with multi-task finetuning. d. Multi-task finetuning can help prevent catastrophic forgetting.'},\n",
       " {'input': \"'Smaller LLMs can struggle with one-shot and few-shot inference:' Is this true or false?\",\n",
       "  'output': 'True'},\n",
       " {'input': 'Which of the following are Parameter Efficient Fine-Tuning (PEFT) methods? Select all that apply.',\n",
       "  'output': 'a, b & d. a. Reparameterization. b. Additive. d. Selective.'},\n",
       " {'input': 'Which of the following best describes how LoRA works?',\n",
       "  'output': 'c. LoRA decomposes weights into two smaller rank matrices and trains those instead of the full model weights.'},\n",
       " {'input': 'What is a soft prompt in the context of LLMs (Large Language Models)?',\n",
       "  'output': 'a. A set of trainable tokens that are added to a prompt and whose values are updated during additional training to improve performance on specific tasks.'},\n",
       " {'input': \"'Prompt Tuning is a technique used to adjust all hyperparameters of a language model.' Is this true or false?\",\n",
       "  'output': 'False'},\n",
       " {'input': \"'PEFT methods can reduce the memory needed for fine-tuning dramatically, sometimes to just 12-20% of the memory needed for full fine-tuning.' Is this true or false?\",\n",
       "  'output': 'True'},\n",
       " {'input': 'When using Reinforcement Learning with Human Feedback (RLHF) to align large language models with human preferences, what is the role of human labelers?',\n",
       "  'output': 'b. To score prompt completions, so that this score is used to train the reward model component of the RLHF process.'},\n",
       " {'input': 'How can RLHF align the performance of large language models with human preferences? Select all that apply',\n",
       "  'output': 'b & c. b. RLHF can help reduce model toxicity and misinformation. c. RLHF can enhance the interpretability of generated text.'}]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_q_and_a_docs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2231116",
   "metadata": {},
   "source": [
    "<h1>Building new Q and A set from scraped links text</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "9dd0566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "10f8e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedded_blogs = list(cleaned_links_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b2d0d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_q_a_string_json(text):\n",
    "    clean_response = text.strip('```python\\n').strip('```')\n",
    "    try:\n",
    "        quiz_data = ast.literal_eval(clean_response)\n",
    "        return quiz_data\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing the response:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abaa58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_q_a = []\n",
    "for key,value in links_dict.items():\n",
    "    try:\n",
    "        start = timeit.default_timer()\n",
    "        all_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"I am trying to create a dataset of quiz questions and answers I can use to fine-tune a model. I want you to create that set of up to 10 quiz questions and answers using the data I give you below\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Here is the data I want you to make quiz questions and answers from: {value}.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please make sure to only make questions related to Machine Learning, Large Language Models, Computer Science, and Software Engineering topics\"},\n",
    "        {\"role\": \"user\", \"content\": \"Please format the output as a list of python dictionaries where each dictionary represents one question answer pair. Here is an example of the structure [{'question':extracted question, 'answer':extracted answer}]\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please return nothing else other than a string version of the python dictionary\"}\n",
    "        ]\n",
    "        response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens = 8000,\n",
    "        messages=all_messages\n",
    "        )\n",
    "        q_a_json_text = response['choices'][0]['message']['content']\n",
    "        q_a_list = clean_q_a_string_json(q_a_json_text)\n",
    "        all_q_a = all_q_a + q_a_list\n",
    "        end = timeit.default_timer()\n",
    "        print(end-start,key,q_a_list,len(all_q_a))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "4b7d8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_q_and_a_docs_final = []\n",
    "for q_a in all_q_a:\n",
    "    all_keys = q_a.keys()\n",
    "    if ('question' in all_keys)&('answer' in all_keys):\n",
    "        all_q_and_a_docs_final.append({'input':q_a['question'],'output':q_a['answer']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "74c4ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q_and_a_docs_final_cleaned = np.array([q_a if \"?\" in q_a['input'] else {\"input\":f\"What is {q_a['input']}?\",\"output\":q_a['output']} for q_a in all_q_and_a_docs_final ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "0ba5e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(0,len(all_q_and_a_docs_final_cleaned))\n",
    "train_indices = np.random.choice(all_indices, size = int(len(all_q_and_a_docs_final_cleaned)*.7))\n",
    "test_indices = np.array([index for index in all_indices if index not in train_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "dd5b0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = all_q_and_a_docs_final_cleaned[train_indices]\n",
    "test_data = all_q_and_a_docs_final_cleaned[test_indices]\n",
    "test_data_list = list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "426933de",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./Test Data/test_data.json\"\n",
    "with open(file_name, \"w\") as json_file:\n",
    "    json.dump(test_data_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c7cab926",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"./Fine Tuning Data/training_data.jsonl\"\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for line in training_data:\n",
    "        try:\n",
    "            # Parse the JSON line\n",
    "            # Create the required structure\n",
    "            transformed = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [{\"text\": line.get(\"input\", \"\")}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"model\",\n",
    "                        \"parts\": [{\"text\": line.get(\"output\", \"\")}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            # Write the transformed JSON object as a line\n",
    "            outfile.write(json.dumps(transformed) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line: {line.strip()}\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "80ad7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"./Fine Tuning Data/test_data.jsonl\"\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for line in test_data:\n",
    "        try:\n",
    "            # Parse the JSON line\n",
    "            # Create the required structure\n",
    "            transformed = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [{\"text\": line.get(\"input\", \"\")}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"model\",\n",
    "                        \"parts\": [{\"text\": line.get(\"output\", \"\")}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            # Write the transformed JSON object as a line\n",
    "            outfile.write(json.dumps(transformed) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line: {line.strip()}\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4989d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "c62f01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_formatted_data = []\n",
    "for entry in training_data:\n",
    "    formatted_entry = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful tutor who answers questions about a class called Introduction to Deep Learning and LLM based Generative AI Systems\"},\n",
    "            {\"role\": \"user\", \"content\": entry[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": entry[\"output\"]}\n",
    "        ]\n",
    "    }\n",
    "    openai_formatted_data.append(formatted_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "a07bf9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Fine Tuning Data/openai_training_data.jsonl\", \"w\") as f:\n",
    "    for entry in openai_formatted_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "cef20ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.File.create(\n",
    "    file=open(\"./Fine Tuning Data/openai_training_data.jsonl\", \"rb\"),\n",
    "    purpose='fine-tune'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "8c68306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = response['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "c7a91360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-TM899BA8CGhM4sLZNiaaQE'"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "7d4af1db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAI' from 'openai' (/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/openai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/54/86chlskx09z1cnlrwbx1g6g00000gp/T/ipykernel_33992/3790563971.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OpenAI' from 'openai' (/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/openai/__init__.py)"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "a1c9a079",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Unknown request URL: POST /v1/fine-tunes. Please check the URL for typos, or see the docs at https://platform.openai.com/docs/api-reference/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/54/86chlskx09z1cnlrwbx1g6g00000gp/T/ipykernel_33992/1665233426.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m fine_tune_response = openai.FineTune.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtraining_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini-2024-07-18\"\u001b[0m  \u001b[0;31m# Or another base model like 'curie', 'babbage', 'ada'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/openai/api_resources/abstract/createable_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         )\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         )\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             return (\n\u001b[0;32m--> 619\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             )\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Unknown request URL: POST /v1/fine-tunes. Please check the URL for typos, or see the docs at https://platform.openai.com/docs/api-reference/."
     ]
    }
   ],
   "source": [
    "fine_tune_response = openai.FineTune.create(\n",
    "    training_file=file_id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\"  # Or another base model like 'curie', 'babbage', 'ada'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d59c3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"I am trying to create a dataset of quiz questions and answers I can use to fine-tune a model. I want you to create that set of up to 10 quiz questions and answers using the data I give you below\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here is the data I want you to make quiz questions and answers from: {all_embedded_blogs[0]}.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please format the output as a list of python dictionaries where each dictionary represents one question answer pair. Here is an example of the structure [{'question':extracted question, 'answer':extracted answer}]\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Please return nothing else other than a string version of the python dictionary\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "35047b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o\",\n",
    "    max_tokens = 8000,\n",
    "    messages=all_messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "babaaea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a_json_text = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4dd0ed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'What is the average token-to-word ratio for a 750-word English document in LLMs?', 'answer': '1.3:1, meaning a 750-word document is approximately 1000 tokens.'}, {'question': 'How much can be saved by appending \"Be Concise\" to a prompt when using an LLM?', 'answer': '40-90% of the tokens can be saved.'}, {'question': 'What is the typical cost ratio of using GPT-4 compared to GPT-3.5 Turbo?', 'answer': 'The cost ratio is approximately 50:1.'}, {'question': 'What is the typical cost ratio of generating text with GPT-3.5 Turbo versus looking it up with OpenAI embedding?', 'answer': 'The cost ratio is 5:1.'}, {'question': 'What is the cost ratio of OpenAI embedding services to self-hosted embedding?', 'answer': 'The cost ratio is approximately 10:1.'}, {'question': 'What is the cost ratio of serving a fine-tuned model versus a base model on OpenAI?', 'answer': 'The cost ratio is 6:1.'}, {'question': 'How much does it typically cost to train a 13 billion parameter model on 1.4 trillion tokens?', 'answer': 'Approximately $1 million.'}, {'question': 'What is the cost ratio of fine-tuning compared to training a model from scratch?', 'answer': 'The cost ratio is less than 0.001.'}, {'question': 'What are the typical GPU memory capacities for various GPUs like V100, A10G, and A100 used in LLM inference?', 'answer': 'V100: 16GB, A10G: 24GB, A100: 40/80GB.'}, {'question': 'How much GPU memory is typically required for 1 token of output with a 13B parameter model?', 'answer': 'Approximately 1 MB of GPU memory.'}]\n"
     ]
    }
   ],
   "source": [
    "clean_response = q_a_json_text.strip('```python\\n').strip('```')\n",
    "\n",
    "# Step 2: Safely parse the string into a Python list\n",
    "try:\n",
    "    quiz_data = ast.literal_eval(clean_response)\n",
    "    print(quiz_data)\n",
    "except Exception as e:\n",
    "    print(\"Error parsing the response:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10b5e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from vertexai.generative_models import GenerativeModel, SafetySetting, Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61137430",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7a55258",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Explain the concept of gradient descent in simple terms.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1586c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials, project_id = load_credentials_from_file(\"./GSuite Text Extraction Creds/vertex_ai_key.json\")\n",
    "vertexai.init(credentials=credentials,project=\"90458358443\", location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5f2899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\n",
    "    \"projects/90458358443/locations/us-central1/endpoints/326380131100655616\",\n",
    "    system_instruction=[\"You are a helpful tutor for the class - Applied Large Language Models and Natural Language Processing\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "831a029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat(response_validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91fd4993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      text: \"RAG stands for Retrieval Augmented Generation.  It\\'s a technique that combines the strengths of large language models (LLMs) with the ability to access and process external knowledge.\\n\\nHere\\'s a breakdown of the process:\\n\\n1. **Retrieval:** The RAG process begins with a user query.  The query is then used to search a knowledge base.  The knowledge base can be a database, a file system, or a cloud storage service.  The search results are then used to retrieve relevant documents.\\n\\n2. **Augmentation:** The retrieved documents are then used to augment the user query.  This means that the user query is modified to include information from the retrieved documents.  This is done by adding the retrieved documents to the user query.\\n\\n3. **Generation:** The augmented query is then used to generate a response.  This is done by using a large language model (LLM).  The LLM is used to generate a response that is based on the augmented query.\\n\\n4. **Output:** The output is a response that is based on the augmented query.  This means that the response is based on the user query and the retrieved documents.\\n\\n**Example:**\\n\\nLet\\'s say you have a knowledge base of documents about the history of the United States.  You want to know about the history of the United States.  You can use a RAG process to generate a response that is based on the user query and the retrieved documents.\\n\\n**Benefits of RAG:**\\n\\n* **Improved accuracy:** RAG can improve the accuracy of LLMs by providing them with access to external knowledge.\\n* **Improved efficiency:** RAG can improve the efficiency of LLMs by providing them with access to external knowledge.\\n* **Improved scalability:** RAG can improve the scalability of LLMs by providing them with access to external knowledge.\\n\\n**Limitations of RAG:**\\n\\n* **Cost:** RAG can be expensive to implement.\\n* **Complexity:** RAG can be complex to implement.\\n* **Scalability:** RAG can be difficult to scale.\\n\\n**In short:**\\n\\nRAG is a technique that combines the strengths of LLMs with the ability to access and process external knowledge.  This can improve the accuracy, efficiency, and scalability of LLMs.\\n\\n**In the context of LLMs and NLP:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\\n\\n**In the context of Applied Large Language Models and Natural Language Processing:**\\n\\nRAG is a powerful technique that can be used to improve the accuracy, efficiency, and scalability of LLMs.  It can also be used to improve the accuracy, efficiency, and scalability of NLP tasks.\"\n",
       "    }\n",
       "  }\n",
       "  finish_reason: MAX_TOKENS\n",
       "  avg_logprobs: -0.20825053751468658\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 23\n",
       "  candidates_token_count: 1024\n",
       "  total_token_count: 1047\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.send_message(\n",
    "        [\"\"\"What is a RAG process?\"\"\"],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dce33b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiturn_generate_content():\n",
    "    vertexai.init(project=\"90458358443\", location=\"us-central1\")\n",
    "    model = GenerativeModel(\n",
    "        \"projects/90458358443/locations/us-central1/endpoints/326380131100655616\",\n",
    "        system_instruction=[\"You are a helpful tutor for the class - Applied Large Language Models and Natural Language Processing\"]\n",
    "    )\n",
    "    chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0ac9d2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 The Model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned Gemini model using get_tuned_model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tuned_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojects/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/locations/us-central1/models/326380131100655616\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m gemini_model \u001b[38;5;241m=\u001b[39m \u001b[43mTextGenerationModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tuned_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuned_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuned_model_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/vertexai/language_models/_language_models.py:187\u001b[0m, in \u001b[0;36m_GetTunedModelMixin.get_tuned_model\u001b[0;34m(cls, tuned_model_name)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tuned_model\u001b[39m(\u001b[38;5;28mcls\u001b[39m, tuned_model_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_LanguageModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads the specified tuned language model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     tuned_vertex_model \u001b[38;5;241m=\u001b[39m \u001b[43maiplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuned_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     tuned_model_labels \u001b[38;5;241m=\u001b[39m tuned_vertex_model\u001b[38;5;241m.\u001b[39mlabels\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _TUNING_BASE_MODEL_ID_LABEL_KEY \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tuned_model_labels:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/cloud/aiplatform/models.py:4562\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model_name, project, location, credentials, version)\u001b[0m\n\u001b[1;32m   4560\u001b[0m \u001b[38;5;66;03m# Create a versioned model_name, if it exists, for getting the GCA model\u001b[39;00m\n\u001b[1;32m   4561\u001b[0m versioned_model_name \u001b[38;5;241m=\u001b[39m ModelRegistry\u001b[38;5;241m.\u001b[39m_get_versioned_name(model_name, version)\n\u001b[0;32m-> 4562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gca_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversioned_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4564\u001b[0m \u001b[38;5;66;03m# Create ModelRegistry with the unversioned resource name\u001b[39;00m\n\u001b[1;32m   4565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry \u001b[38;5;241m=\u001b[39m ModelRegistry(\n\u001b[1;32m   4566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_name,\n\u001b[1;32m   4567\u001b[0m     location\u001b[38;5;241m=\u001b[39mlocation,\n\u001b[1;32m   4568\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[1;32m   4569\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m   4570\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/cloud/aiplatform/base.py:692\u001b[0m, in \u001b[0;36mVertexAiResourceNoun._get_gca_resource\u001b[0;34m(self, resource_name, parent_resource_name_fields)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns GAPIC service representation of client class resource.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m        Should not include project and location.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m resource_name \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfull_resource_name(\n\u001b[1;32m    682\u001b[0m     resource_name\u001b[38;5;241m=\u001b[39mresource_name,\n\u001b[1;32m    683\u001b[0m     resource_noun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_noun,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     resource_id_validator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_id_validator,\n\u001b[1;32m    690\u001b[0m )\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getter_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_RETRY\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/cloud/aiplatform_v1/services/model_service/client.py:1048\u001b[0m, in \u001b[0;36mModelServiceClient.get_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 The Model does not exist."
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned Gemini model using get_tuned_model\n",
    "tuned_model_name = f\"projects/{project_id}/locations/us-central1/models/326380131100655616\"\n",
    "gemini_model = TextGenerationModel.get_tuned_model(tuned_model_name=tuned_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "414eb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_id = \"326380131100655616\"\n",
    "endpoint = aiplatform.Endpoint(endpoint_name=f\"projects/{project_id}/locations/us-central1/endpoints/{endpoint_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9d2e22a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Gemini cannot be accessed through Vertex Predict/RawPredict API. Please follow https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal for Gemini usage.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"Gemini cannot be accessed through Vertex Predict/RawPredict API. Please follow https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal for Gemini usage.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.41.10:443 {grpc_message:\"Gemini cannot be accessed through Vertex Predict/RawPredict API. Please follow https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal for Gemini usage.\", grpc_status:9, created_time:\"2024-12-04T17:39:13.412476-05:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Retrieval Augmented Generation (RAG)?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m instances \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]  \u001b[38;5;66;03m# Ensure the input format matches your model's schema\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/cloud/aiplatform/models.py:2341\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict, use_dedicated_endpoint)\u001b[0m\n\u001b[1;32m   2332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   2333\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2334\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2337\u001b[0m         model_version_id\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelVersionId\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2338\u001b[0m     )\n\u001b[1;32m   2340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2341\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[1;32m   2348\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:853\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 853\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/vertexai_env/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Gemini cannot be accessed through Vertex Predict/RawPredict API. Please follow https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal for Gemini usage."
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "prompt = \"What is Retrieval Augmented Generation (RAG)?\"\n",
    "instances = [{\"content\": prompt}]  # Ensure the input format matches your model's schema\n",
    "\n",
    "response = endpoint.predict(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a84821",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "        threshold=1,  # 1 is the most restrictive; adjust as needed\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ee62ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_gapic_compute_tokens',\n",
       " '_gapic_compute_tokens_async',\n",
       " '_gapic_count_tokens',\n",
       " '_gapic_count_tokens_async',\n",
       " '_generate_content',\n",
       " '_generate_content_async',\n",
       " '_generate_content_streaming',\n",
       " '_generate_content_streaming_async',\n",
       " '_generation_config',\n",
       " '_labels',\n",
       " '_llm_utility_async_client',\n",
       " '_llm_utility_client',\n",
       " '_location',\n",
       " '_model_name',\n",
       " '_parse_response',\n",
       " '_prediction_async_client',\n",
       " '_prediction_client',\n",
       " '_prediction_resource_name',\n",
       " '_prepare_request',\n",
       " '_safety_settings',\n",
       " '_system_instruction',\n",
       " '_tool_config',\n",
       " '_tools',\n",
       " 'compute_tokens',\n",
       " 'compute_tokens_async',\n",
       " 'count_tokens',\n",
       " 'count_tokens_async',\n",
       " 'generate_content',\n",
       " 'generate_content_async',\n",
       " 'start_chat']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(gemini_model)[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61abf1d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_GenerativeModel.generate_content() got an unexpected keyword argument 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgemini_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Controls randomness; lower is less random\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Limit on output length\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Controls diversity via nucleus sampling\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Controls diversity via token sampling\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optional, set this if needed\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: _GenerativeModel.generate_content() got an unexpected keyword argument 'prompt'"
     ]
    }
   ],
   "source": [
    "response = gemini_model.generate_content(\n",
    "    prompt=input_text,\n",
    "    temperature=0.7,  # Controls randomness; lower is less random\n",
    "    max_output_tokens=256,  # Limit on output length\n",
    "    top_p=0.8,  # Controls diversity via nucleus sampling\n",
    "    top_k=40,  # Controls diversity via token sampling\n",
    "    safety_settings=safety_settings  # Optional, set this if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "05b69aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model does not support deployment. See https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1#google.cloud.aiplatform.v1.Model.FIELDS.repeated.google.cloud.aiplatform.v1.Model.DeploymentResourcesType.google.cloud.aiplatform.v1.Model.supported_deployment_resources_types\n"
     ]
    }
   ],
   "source": [
    "endpoint = tuned_model.deploy(\n",
    "    machine_type=\"n1-standard-4\",  # Choose an appropriate machine type\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "593de2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deployed to endpoint: 3785355751153729536\n"
     ]
    }
   ],
   "source": [
    "print(\"Model deployed to endpoint:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "ac101736",
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Gemini cannot be accessed through Vertex Predict/RawPredict API. Please follow https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal for Gemini usage.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         )\n\u001b[0;32m-> 1181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"Gemini cannot be accessed through Vertex Predict/RawPredict API. Please follow https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal for Gemini usage.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.64.74:443 {created_time:\"2024-12-04T15:18:52.572003-05:00\", grpc_status:9, grpc_message:\"Gemini cannot be accessed through Vertex Predict/RawPredict API. Please follow https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal for Gemini usage.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/54/86chlskx09z1cnlrwbx1g6g00000gp/T/ipykernel_33992/46594611.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1562\u001b[0m             )\n\u001b[1;32m   1563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m             prediction_response = self._prediction_client.predict(\n\u001b[0m\u001b[1;32m   1565\u001b[0m                 \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    605\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Gemini cannot be accessed through Vertex Predict/RawPredict API. Please follow https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal for Gemini usage."
     ]
    }
   ],
   "source": [
    "input_text = \"What is Retrieval Augmented Generation (RAG)?\"\n",
    "instances = [{\"content\": input_text}]\n",
    "\n",
    "response = endpoint.predict(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3cb826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
