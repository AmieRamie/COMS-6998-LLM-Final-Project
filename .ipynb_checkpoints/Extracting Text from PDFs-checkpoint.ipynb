{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f1422ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai\n",
    "from google.oauth2 import service_account\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber  # for improved OCR if needed\n",
    "import timeit\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "import tiktoken  # OpenAI's tokenization library\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api_key = \"pcsk_68gSai_Jvtbix4qBZ8Z3ccEr6HincaEb4ewKM5GzK23pbEkfNZ6UoNAN2jJimXGg3iyYpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac739dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_pdf(input_pdf_path,file_name, max_pages=1):\n",
    "    \"\"\"\n",
    "    Split a PDF into smaller chunks of max_pages.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(input_pdf_path)\n",
    "    chunks = []\n",
    "    for i in range(0, len(reader.pages), max_pages):\n",
    "        writer = PdfWriter()\n",
    "        for j in range(i, min(i + max_pages, len(reader.pages))):\n",
    "            writer.add_page(reader.pages[j])\n",
    "        chunk_path = f\"./chunks/chunk_{i // max_pages + 1}_{file_name.split('.')[0]}.pdf\"\n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            writer.write(f)\n",
    "        chunks.append(chunk_path)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "119a1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file('coms-6998-applied-llm-class-4e98f4f7a361.json')\n",
    "client = documentai.DocumentProcessorServiceClient(credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fef436f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_to_extract_data_from = os.listdir('./lecture_pdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2747cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "for file_name in all_files_to_extract_data_from:\n",
    "    file_directory = \"./lecture_pdfs\"\n",
    "    pdf_path = os.path.join(file_directory, file_name)\n",
    "    chunks = split_pdf(pdf_path,file_name)\n",
    "    all_chunks = all_chunks + chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b1bafc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_links(text):\n",
    "    links = []\n",
    "    text = text.replace('-\\n',\"\")\n",
    "    page_links = re.findall(r'(https?://\\S+)', text)\n",
    "    links.extend(page_links)\n",
    "    page_links = re.findall(r'(http?://\\S+)', text)\n",
    "    links.extend(page_links)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b3d9133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_extraction(file_name,project_id = \"coms-6998-applied-llm-class\",location = \"us\",processor_id = \"398fd74279aa6748\"):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    raw_document = documentai.RawDocument(content=content, mime_type=\"application/pdf\")\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "    # Make the request\n",
    "    request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
    "    response = client.process_document(request=request)\n",
    "    document = response.document\n",
    "    text = document.text\n",
    "    links = extract_text_links(text)\n",
    "    return text, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9d9fac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_clean_text(url):\n",
    "    \"\"\"\n",
    "    Fetches and cleans text from the given URL.\n",
    "    :param url: The URL to fetch text from.\n",
    "    :return: Cleaned text or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make an HTTP GET request\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract the main text content\n",
    "        # We can focus on specific tags (e.g., <p>, <div>) or use the whole text\n",
    "        text_elements = soup.find_all([\"p\", \"div\"])\n",
    "        text = \" \".join(element.get_text() for element in text_elements)\n",
    "        \n",
    "        # Clean the text\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "        text = text.strip()  # Remove leading/trailing whitespace\n",
    "        \n",
    "        # Handle empty text scenario\n",
    "        if not text:\n",
    "            return f\"Error: No extractable text found at {url}\"\n",
    "        return text\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle HTTP and connection errors\n",
    "        return f\"Error: Unable to fetch content from {url}. Exception: {e}\"\n",
    "    except Exception as e:\n",
    "        # Handle other unexpected errors\n",
    "        return f\"Error: Unexpected error while processing {url}. Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8a0086a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_links(links):\n",
    "    \"\"\"\n",
    "    Processes a list of links, extracting and cleaning text content.\n",
    "    :param links: List of URLs.\n",
    "    :return: Dictionary with URLs as keys and cleaned text (or error messages) as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for url in links:\n",
    "        print(f\"Processing: {url}\")\n",
    "        text = fetch_and_clean_text(url)\n",
    "        results[url] = text\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "01adc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts_with_links = [value['text'] for key,value in all_data.items() if len(value['links'])>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6a5010c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 6.449538166999446 ./chunks/chunk_46_Lecture-12-Columbia.pdf\n",
      "50 7.764267583000219 ./chunks/chunk_51_Lecture-12-Columbia.pdf\n",
      "55 7.30011899999954 ./chunks/chunk_56_Lecture-12-Columbia.pdf\n",
      "60 8.476970249999795 ./chunks/chunk_61_Lecture-12-Columbia.pdf\n",
      "65 8.502008792000197 ./chunks/chunk_66_Lecture-12-Columbia.pdf\n",
      "70 8.57982845800052 ./chunks/chunk_71_Lecture-12-Columbia.pdf\n",
      "75 7.840684208000312 ./chunks/chunk_76_Lecture-12-Columbia.pdf\n",
      "80 7.657551166999838 ./chunks/chunk_81_Lecture-12-Columbia.pdf\n",
      "85 7.173694291999709 ./chunks/chunk_86_Lecture-12-Columbia.pdf\n",
      "90 8.033632041000601 ./chunks/chunk_91_Lecture-12-Columbia.pdf\n",
      "95 8.131241334000151 ./chunks/chunk_96_Lecture-12-Columbia.pdf\n",
      "100 8.513085292000142 ./chunks/chunk_101_Lecture-12-Columbia.pdf\n",
      "105 7.703719500000261 ./chunks/chunk_106_Lecture-12-Columbia.pdf\n",
      "110 9.373608582999623 ./chunks/chunk_111_Lecture-12-Columbia.pdf\n",
      "115 8.92286362499999 ./chunks/chunk_116_Lecture-12-Columbia.pdf\n",
      "120 7.5701725419994546 ./chunks/chunk_121_Lecture-12-Columbia.pdf\n",
      "125 8.885581458999695 ./chunks/chunk_126_Lecture-12-Columbia.pdf\n",
      "130 9.089975707999656 ./chunks/chunk_131_Lecture-12-Columbia.pdf\n",
      "135 8.26253962499959 ./chunks/chunk_2_Lecture-13-Columbia.pdf\n",
      "140 8.236549958999603 ./chunks/chunk_7_Lecture-13-Columbia.pdf\n",
      "145 7.548319667000214 ./chunks/chunk_12_Lecture-13-Columbia.pdf\n",
      "150 7.327647166000133 ./chunks/chunk_17_Lecture-13-Columbia.pdf\n",
      "155 7.944368416000543 ./chunks/chunk_22_Lecture-13-Columbia.pdf\n",
      "160 7.962167457999385 ./chunks/chunk_27_Lecture-13-Columbia.pdf\n",
      "165 7.763841124999999 ./chunks/chunk_32_Lecture-13-Columbia.pdf\n",
      "170 8.528474917000494 ./chunks/chunk_37_Lecture-13-Columbia.pdf\n",
      "175 8.435918166000192 ./chunks/chunk_42_Lecture-13-Columbia.pdf\n",
      "180 8.159709958999883 ./chunks/chunk_47_Lecture-13-Columbia.pdf\n",
      "185 7.086496833000638 ./chunks/chunk_52_Lecture-13-Columbia.pdf\n",
      "190 7.783497625000564 ./chunks/chunk_57_Lecture-13-Columbia.pdf\n",
      "195 7.406822457999624 ./chunks/chunk_62_Lecture-13-Columbia.pdf\n",
      "200 7.861424958000498 ./chunks/chunk_67_Lecture-13-Columbia.pdf\n",
      "205 7.928558624999823 ./chunks/chunk_72_Lecture-13-Columbia.pdf\n",
      "210 7.78217662499992 ./chunks/chunk_77_Lecture-13-Columbia.pdf\n",
      "215 7.780537042000105 ./chunks/chunk_82_Lecture-13-Columbia.pdf\n",
      "220 7.842029082999943 ./chunks/chunk_87_Lecture-13-Columbia.pdf\n",
      "225 8.237499332999505 ./chunks/chunk_92_Lecture-13-Columbia.pdf\n",
      "230 8.330287833000511 ./chunks/chunk_97_Lecture-13-Columbia.pdf\n",
      "235 8.31084012500014 ./chunks/chunk_102_Lecture-13-Columbia.pdf\n",
      "240 8.75368141600029 ./chunks/chunk_107_Lecture-13-Columbia.pdf\n",
      "245 7.9195339589996365 ./chunks/chunk_3_Lecture-5-columbia-Fall2024.pdf\n",
      "250 7.279840749999494 ./chunks/chunk_8_Lecture-5-columbia-Fall2024.pdf\n",
      "255 7.964158249999855 ./chunks/chunk_13_Lecture-5-columbia-Fall2024.pdf\n",
      "260 7.67915075000019 ./chunks/chunk_18_Lecture-5-columbia-Fall2024.pdf\n",
      "265 7.593029124999703 ./chunks/chunk_23_Lecture-5-columbia-Fall2024.pdf\n",
      "270 7.654824750000444 ./chunks/chunk_28_Lecture-5-columbia-Fall2024.pdf\n",
      "275 7.408023332999619 ./chunks/chunk_33_Lecture-5-columbia-Fall2024.pdf\n",
      "280 7.650419833000342 ./chunks/chunk_38_Lecture-5-columbia-Fall2024.pdf\n",
      "285 7.761835332999908 ./chunks/chunk_43_Lecture-5-columbia-Fall2024.pdf\n",
      "290 8.191603917000066 ./chunks/chunk_4_Lecture-7-Columbia.pdf\n",
      "295 9.278614374999961 ./chunks/chunk_9_Lecture-7-Columbia.pdf\n",
      "300 8.957823749999989 ./chunks/chunk_14_Lecture-7-Columbia.pdf\n",
      "305 8.682208458000787 ./chunks/chunk_19_Lecture-7-Columbia.pdf\n",
      "310 8.939435625000442 ./chunks/chunk_24_Lecture-7-Columbia.pdf\n",
      "315 8.399405582999862 ./chunks/chunk_29_Lecture-7-Columbia.pdf\n",
      "320 8.849013416999696 ./chunks/chunk_34_Lecture-7-Columbia.pdf\n",
      "325 8.45350845899975 ./chunks/chunk_39_Lecture-7-Columbia.pdf\n",
      "330 8.78551137499926 ./chunks/chunk_44_Lecture-7-Columbia.pdf\n",
      "335 9.074276291000388 ./chunks/chunk_49_Lecture-7-Columbia.pdf\n",
      "340 8.83377904200006 ./chunks/chunk_54_Lecture-7-Columbia.pdf\n",
      "345 8.709281207999993 ./chunks/chunk_59_Lecture-7-Columbia.pdf\n",
      "350 8.70278800000051 ./chunks/chunk_64_Lecture-7-Columbia.pdf\n",
      "355 7.782674582999789 ./chunks/chunk_69_Lecture-7-Columbia.pdf\n",
      "360 7.945757624999715 ./chunks/chunk_5_Lecture-3-Columbia (1).pdf\n",
      "365 9.017627958000048 ./chunks/chunk_10_Lecture-3-Columbia (1).pdf\n",
      "370 8.849826124999709 ./chunks/chunk_15_Lecture-3-Columbia (1).pdf\n",
      "375 8.68846129099984 ./chunks/chunk_20_Lecture-3-Columbia (1).pdf\n",
      "380 8.775227000000086 ./chunks/chunk_25_Lecture-3-Columbia (1).pdf\n",
      "385 7.722368167000241 ./chunks/chunk_30_Lecture-3-Columbia (1).pdf\n",
      "390 7.860786915999597 ./chunks/chunk_35_Lecture-3-Columbia (1).pdf\n",
      "395 8.501044999999976 ./chunks/chunk_40_Lecture-3-Columbia (1).pdf\n",
      "400 8.740583500000866 ./chunks/chunk_45_Lecture-3-Columbia (1).pdf\n",
      "405 9.020848333000686 ./chunks/chunk_50_Lecture-3-Columbia (1).pdf\n",
      "410 8.964991791000102 ./chunks/chunk_55_Lecture-3-Columbia (1).pdf\n",
      "415 8.706264833000205 ./chunks/chunk_60_Lecture-3-Columbia (1).pdf\n",
      "420 8.27922204099923 ./chunks/chunk_65_Lecture-3-Columbia (1).pdf\n",
      "425 9.838630708999517 ./chunks/chunk_70_Lecture-3-Columbia (1).pdf\n",
      "430 9.201253500000348 ./chunks/chunk_75_Lecture-3-Columbia (1).pdf\n",
      "435 7.668966584000373 ./chunks/chunk_80_Lecture-3-Columbia (1).pdf\n",
      "440 7.781664375000219 ./chunks/chunk_5_Lecture-2-columbia-Fall2024.pdf\n",
      "445 7.8346565419997205 ./chunks/chunk_10_Lecture-2-columbia-Fall2024.pdf\n",
      "450 7.589046707999842 ./chunks/chunk_15_Lecture-2-columbia-Fall2024.pdf\n",
      "455 7.356198208999558 ./chunks/chunk_20_Lecture-2-columbia-Fall2024.pdf\n",
      "460 7.774766458000158 ./chunks/chunk_25_Lecture-2-columbia-Fall2024.pdf\n",
      "465 8.031287833000533 ./chunks/chunk_30_Lecture-2-columbia-Fall2024.pdf\n",
      "470 8.357353708999653 ./chunks/chunk_35_Lecture-2-columbia-Fall2024.pdf\n",
      "475 9.569471917000556 ./chunks/chunk_40_Lecture-2-columbia-Fall2024.pdf\n",
      "480 7.439694833999965 ./chunks/chunk_45_Lecture-2-columbia-Fall2024.pdf\n",
      "485 7.687130541999977 ./chunks/chunk_50_Lecture-2-columbia-Fall2024.pdf\n",
      "490 8.185709874999702 ./chunks/chunk_55_Lecture-2-columbia-Fall2024.pdf\n",
      "495 7.911954541999876 ./chunks/chunk_60_Lecture-2-columbia-Fall2024.pdf\n",
      "500 8.784260291999999 ./chunks/chunk_65_Lecture-2-columbia-Fall2024.pdf\n",
      "505 7.441585542000212 ./chunks/chunk_70_Lecture-2-columbia-Fall2024.pdf\n",
      "510 8.117459291999694 ./chunks/chunk_75_Lecture-2-columbia-Fall2024.pdf\n",
      "515 7.629976332999831 ./chunks/chunk_80_Lecture-2-columbia-Fall2024.pdf\n",
      "520 8.041072209000049 ./chunks/chunk_85_Lecture-2-columbia-Fall2024.pdf\n",
      "525 7.595909167000173 ./chunks/chunk_1_Lecture-9-Columbia.pdf\n",
      "530 7.698650000000271 ./chunks/chunk_6_Lecture-9-Columbia.pdf\n",
      "535 7.467916333000176 ./chunks/chunk_11_Lecture-9-Columbia.pdf\n",
      "540 7.911277707999943 ./chunks/chunk_16_Lecture-9-Columbia.pdf\n",
      "545 7.580332708000242 ./chunks/chunk_21_Lecture-9-Columbia.pdf\n",
      "550 7.750810916999399 ./chunks/chunk_26_Lecture-9-Columbia.pdf\n",
      "555 6.80767829099932 ./chunks/chunk_31_Lecture-9-Columbia.pdf\n",
      "560 7.032660541000041 ./chunks/chunk_36_Lecture-9-Columbia.pdf\n",
      "565 8.225823832999595 ./chunks/chunk_41_Lecture-9-Columbia.pdf\n",
      "570 7.221644125000239 ./chunks/chunk_46_Lecture-9-Columbia.pdf\n",
      "575 7.746955707999405 ./chunks/chunk_51_Lecture-9-Columbia.pdf\n",
      "580 8.89356320800016 ./chunks/chunk_56_Lecture-9-Columbia.pdf\n",
      "585 8.760759707999568 ./chunks/chunk_61_Lecture-9-Columbia.pdf\n",
      "590 7.985754457999974 ./chunks/chunk_66_Lecture-9-Columbia.pdf\n",
      "595 7.885314541999833 ./chunks/chunk_71_Lecture-9-Columbia.pdf\n",
      "600 8.285354124999685 ./chunks/chunk_76_Lecture-9-Columbia.pdf\n",
      "605 7.432501375000356 ./chunks/chunk_81_Lecture-9-Columbia.pdf\n",
      "610 8.597793083000397 ./chunks/chunk_1_Lecture-11-columbia.pdf\n",
      "615 7.085838958999375 ./chunks/chunk_6_Lecture-11-columbia.pdf\n",
      "620 7.750360541999726 ./chunks/chunk_11_Lecture-11-columbia.pdf\n",
      "625 8.704311041999972 ./chunks/chunk_16_Lecture-11-columbia.pdf\n",
      "630 8.437389249999796 ./chunks/chunk_21_Lecture-11-columbia.pdf\n",
      "635 8.01620370799992 ./chunks/chunk_26_Lecture-11-columbia.pdf\n",
      "640 8.339264958000058 ./chunks/chunk_31_Lecture-11-columbia.pdf\n",
      "645 7.960805708999942 ./chunks/chunk_36_Lecture-11-columbia.pdf\n",
      "650 7.983526250000068 ./chunks/chunk_41_Lecture-11-columbia.pdf\n",
      "655 7.528581624999788 ./chunks/chunk_1_Lecture-6-columbia-Fall2024.pdf\n",
      "660 7.995284208000157 ./chunks/chunk_6_Lecture-6-columbia-Fall2024.pdf\n",
      "665 7.4731590829997 ./chunks/chunk_11_Lecture-6-columbia-Fall2024.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670 6.929717415999221 ./chunks/chunk_16_Lecture-6-columbia-Fall2024.pdf\n",
      "675 7.669259667000006 ./chunks/chunk_21_Lecture-6-columbia-Fall2024.pdf\n",
      "680 7.963793916999748 ./chunks/chunk_26_Lecture-6-columbia-Fall2024.pdf\n",
      "685 7.476365084000463 ./chunks/chunk_4_Lecture-10-Columbia.pdf\n",
      "690 8.1687207089999 ./chunks/chunk_9_Lecture-10-Columbia.pdf\n",
      "695 8.90650420899965 ./chunks/chunk_14_Lecture-10-Columbia.pdf\n",
      "700 9.140342749999945 ./chunks/chunk_19_Lecture-10-Columbia.pdf\n",
      "705 7.98733508299938 ./chunks/chunk_24_Lecture-10-Columbia.pdf\n",
      "710 8.115165457999865 ./chunks/chunk_29_Lecture-10-Columbia.pdf\n",
      "715 8.156138875000579 ./chunks/chunk_34_Lecture-10-Columbia.pdf\n",
      "720 9.232960582999112 ./chunks/chunk_39_Lecture-10-Columbia.pdf\n",
      "725 8.515497958000196 ./chunks/chunk_44_Lecture-10-Columbia.pdf\n",
      "730 7.679936417000135 ./chunks/chunk_49_Lecture-10-Columbia.pdf\n",
      "735 7.841302666000047 ./chunks/chunk_54_Lecture-10-Columbia.pdf\n",
      "740 8.02238770799977 ./chunks/chunk_59_Lecture-10-Columbia.pdf\n",
      "745 7.95550374999948 ./chunks/chunk_64_Lecture-10-Columbia.pdf\n",
      "750 8.129068791000464 ./chunks/chunk_69_Lecture-10-Columbia.pdf\n",
      "755 8.778868874999716 ./chunks/chunk_74_Lecture-10-Columbia.pdf\n",
      "760 7.308207125000081 ./chunks/chunk_79_Lecture-10-Columbia.pdf\n",
      "765 7.450056874999973 ./chunks/chunk_84_Lecture-10-Columbia.pdf\n",
      "770 8.137261249999938 ./chunks/chunk_89_Lecture-10-Columbia.pdf\n",
      "775 7.93935566699929 ./chunks/chunk_94_Lecture-10-Columbia.pdf\n",
      "780 7.570794291999846 ./chunks/chunk_99_Lecture-10-Columbia.pdf\n",
      "785 7.668086333000247 ./chunks/chunk_104_Lecture-10-Columbia.pdf\n",
      "790 7.2911123750000115 ./chunks/chunk_109_Lecture-10-Columbia.pdf\n",
      "795 7.571943208999983 ./chunks/chunk_4_Lecture-4-columbia-Fall2024.pdf\n",
      "800 8.164453834000597 ./chunks/chunk_9_Lecture-4-columbia-Fall2024.pdf\n",
      "805 7.55943654100065 ./chunks/chunk_14_Lecture-4-columbia-Fall2024.pdf\n",
      "810 10.460296374999416 ./chunks/chunk_19_Lecture-4-columbia-Fall2024.pdf\n",
      "815 8.346888082999612 ./chunks/chunk_24_Lecture-4-columbia-Fall2024.pdf\n",
      "820 7.89074837499993 ./chunks/chunk_29_Lecture-4-columbia-Fall2024.pdf\n",
      "825 8.620286416000454 ./chunks/chunk_34_Lecture-4-columbia-Fall2024.pdf\n",
      "830 7.123980625000513 ./chunks/chunk_39_Lecture-4-columbia-Fall2024.pdf\n",
      "835 7.343548041999384 ./chunks/chunk_44_Lecture-4-columbia-Fall2024.pdf\n",
      "840 7.287329791999582 ./chunks/chunk_49_Lecture-4-columbia-Fall2024.pdf\n",
      "845 7.550609832999726 ./chunks/chunk_54_Lecture-4-columbia-Fall2024.pdf\n",
      "850 8.078799249999975 ./chunks/chunk_59_Lecture-4-columbia-Fall2024.pdf\n",
      "855 7.282098333000249 ./chunks/chunk_64_Lecture-4-columbia-Fall2024.pdf\n",
      "860 7.505362832999708 ./chunks/chunk_69_Lecture-4-columbia-Fall2024.pdf\n",
      "865 7.529766540999844 ./chunks/chunk_74_Lecture-4-columbia-Fall2024.pdf\n",
      "870 9.03695824999977 ./chunks/chunk_79_Lecture-4-columbia-Fall2024.pdf\n",
      "875 8.04511787499996 ./chunks/chunk_5_Lecture-8-Columbia.pdf\n",
      "880 7.8007027499998 ./chunks/chunk_10_Lecture-8-Columbia.pdf\n",
      "885 8.117723082999873 ./chunks/chunk_15_Lecture-8-Columbia.pdf\n",
      "890 8.800097041999834 ./chunks/chunk_20_Lecture-8-Columbia.pdf\n",
      "895 9.241167666000365 ./chunks/chunk_25_Lecture-8-Columbia.pdf\n",
      "900 7.645130375000008 ./chunks/chunk_30_Lecture-8-Columbia.pdf\n",
      "905 7.796405208000579 ./chunks/chunk_35_Lecture-8-Columbia.pdf\n",
      "910 8.15077137499975 ./chunks/chunk_40_Lecture-8-Columbia.pdf\n",
      "915 8.70919533400047 ./chunks/chunk_45_Lecture-8-Columbia.pdf\n",
      "920 9.176570124999671 ./chunks/chunk_50_Lecture-8-Columbia.pdf\n",
      "925 8.926850458000445 ./chunks/chunk_55_Lecture-8-Columbia.pdf\n",
      "930 8.929297542000313 ./chunks/chunk_60_Lecture-8-Columbia.pdf\n",
      "935 8.783865000000333 ./chunks/chunk_65_Lecture-8-Columbia.pdf\n",
      "940 9.315037708999625 ./chunks/chunk_70_Lecture-8-Columbia.pdf\n",
      "945 11.75281829100004 ./chunks/chunk_75_Lecture-8-Columbia.pdf\n"
     ]
    }
   ],
   "source": [
    "all_processed_chunks = list(all_data.keys())\n",
    "start = timeit.default_timer()\n",
    "for i,chunk in enumerate(all_chunks):\n",
    "    if chunk not in all_processed_chunks:\n",
    "        text, links = get_document_extraction(chunk)\n",
    "        all_data[chunk] = {'text':text,'links':links}\n",
    "        if i%5 ==0:\n",
    "            end = timeit.default_timer()\n",
    "            print(i, end-start, chunk)\n",
    "            start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "13434d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cleaned = {}\n",
    "for key,value in all_data.items():\n",
    "    if len(value['links'])>0:\n",
    "        all_data_cleaned[key] = {'text':value['text'],'links':extract_text_links(value['text'])}\n",
    "    else:\n",
    "        all_data_cleaned[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cc05761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file name of the JSON file\n",
    "file_name = \"data_from_presentations.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(file_name, \"r\") as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1f508f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_chunks = list(all_data_cleaned.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d1411ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = []\n",
    "for extracted_data in list(list(all_data_cleaned.values())):\n",
    "    all_links = all_links + extracted_data['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9e8462d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model\n",
      "Processing: https://arxiv.org/abs/2205.14135\n",
      "Processing: https://ai.stanford.edu/blog/longer-sequencesnext-leap-ai/\n",
      "Processing: https://github.com/vllm-project/vllm\n",
      "Processing: https://vllm.ai\n",
      "Processing: https://arxiv.org/abs/2309.06180\n",
      "Processing: https://discord.gg/jz7wjKhh6g\n",
      "Processing: https://docs.nvidia.com/datacenter/tesla/mig-userguide/index.html\n",
      "Processing: https://huggingface.co/blog/trl-peft\n",
      "Processing: https://arxiv.org/pdf/2202.05924\n",
      "Processing: https://splab.sdu.edu.cn/G\n",
      "Processing: https://research.google/blog/pathways-languagemodel-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/\n",
      "Processing: https://arxiv.org/pdf/2202.05924\n",
      "Processing: https://www.youtube.com/watch?v=EnJ7qX9fkcU\n",
      "Processing: https://jvns.ca/blog/2016/10/10/what-even-is-a-container/\n",
      "Processing: https://kubernetes.io/\n",
      "Processing: https://cloud.google.com/kubernetesengine/\n",
      "Processing: https://www.alibabacloud.com/product/kubernetes\n",
      "Processing: https://aws.amazon.com/eks/\n",
      "Processing: https://azure.microsoft.com/enus/services/kubernetes-service/\n",
      "Processing: https://github.com/IBM/FfDL\n",
      "Processing: https://www.ibm.com/products/watson-studio\n",
      "Processing: https://aws.amazon.com/sagemaker\n",
      "Processing: https://azure.com/ml\n",
      "Processing: https://cloud.google.com/vertex-ai/\n",
      "Processing: https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n",
      "Processing: https://arxiv.org/pdf/2202.05924\n",
      "Processing: https://website-754fwhahs-humanloopml.vercel.app/blog/open_ai_talk\n",
      "Processing: https://arxiv.org/pdf/2202.05924\n",
      "Processing: https://splab.sdu.edu.cn/GPT3.pdf\n",
      "Processing: https://research.google/blog/pathw\n",
      "Processing: https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/\n",
      "Processing: https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/\n",
      "Processing: https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html\n",
      "Processing: https://arxiv.org/pdf/1811.05233.pdf\n",
      "Processing: https://cloud.google.com/tpu/docs/systemarchitecture\n",
      "Processing: https://medium.com/mlreview/a-guide-to-receptive-fieldarithmetic-for-convolutional-neural-networks-e0f514068807\n",
      "Processing: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Processing: https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n",
      "Processing: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
      "Processing: https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n",
      "Processing: https://arxiv.org/pdf/2005.14165\n",
      "Processing: https://gluebenchmark.com/leaderboard\n",
      "Processing: https://super.gluebenchmark.com/leaderboard/\n",
      "Processing: https://crfm.stanford.edu/helm/\n",
      "Processing: https://crfm.stanford.edu/helm/\n",
      "Processing: https://www.anyscale.com/blog/reproducible-performance-metrics-for-llm-inference\n",
      "Processing: https://github.com/ray-project/LLMPerf\n",
      "Processing: https://github.com/ray-project/llmperf-leaderboard\n",
      "Processing: https://github.com/ray-project/llmperf-leaderboard\n",
      "Processing: https://www.kubeflow.org/docs/about/kubeflow/\n",
      "Processing: https://www.kubeflow.org/docs/pipelines/pipelines-quickstart/\n",
      "Processing: https://www.kubeflow.org/docs/pi\n",
      "Processing: https://www.kubeflow.org/docs/pipelines/overview/concepts/comp\n",
      "Processing: https://www.kubeflow.org/docs/pipelines/refe\n",
      "Processing: https://mlcommons.org\n",
      "Processing: https://mlcommons.org/benchmarks/storage/\n",
      "Processing: https://github.com/bigscience-workshop/promptsource/blob/main/promptsource/templates/amazon_polarity/templates.yaml\n",
      "Processing: https://huggingface.co/datasets/samsum,\n",
      "Processing: https://github.com/google-research/FLAN/blob/2c79a31/flan/v2/templates.py\n",
      "Processing: https://huggingface.co/datasets/knkarthick/dialogsum/viewer/knkarthick--dialo\n",
      "Processing: https://arxiv.org/pdf/1806.09055.pdf\n",
      "Processing: http://onnx.ai\n",
      "Processing: http://onnx.ai\n",
      "Processing: http://onnx.ai/supported-tools\n",
      "Processing: https://github.com/onnx/tutorials\n",
      "Processing: https://github.com/onnx/models\n",
      "Processing: http://onnx.ai/supported-tools\n",
      "Processing: https://github.com/microsoft/onnxruntime\n",
      "Processing: https://github.com/huggingface/notebooks/blob/main/examples/onnx\n",
      "Processing: https://github.com/tensorflow/models\n",
      "Processing: https://github.com/pytorch/vision\n",
      "Processing: https://www.restack.io/p/retrieval-augmented-generation-answer-rag-vs-semantic-cat-ai\n",
      "Processing: https://www.geeksforgeeks.org/keywordsearching-algorithms-for-search-engines/\n",
      "Processing: https://en.wikipedia.org/wiki/Okapi_BM25\n",
      "Processing: https://drive.google.com/file/d/1UCb_ED3anGlfUvqm19ZKvVdBTBJb4KM/view?usp=sharing\n",
      "Processing: https://api.open-meteo.com/v1/forecast\"\n"
     ]
    }
   ],
   "source": [
    "extracted_data = process_links(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "de0a2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_extracted_data = {key:value for key, value in extracted_data.items() if len(value)>=1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bad2eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_to_extract_data_from = os.listdir('./HWs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "290d592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hw_chunks = []\n",
    "for file_name in all_files_to_extract_data_from:\n",
    "    file_directory = \"./HWs\"\n",
    "    pdf_path = os.path.join(file_directory, file_name)\n",
    "    chunks = split_pdf(pdf_path,file_name, max_pages = 15)\n",
    "    all_hw_chunks = all_hw_chunks + chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0012bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_hw_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "39e0c74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.847637875000146 ./chunks/chunk_1_HW4-PDF.pdf\n"
     ]
    }
   ],
   "source": [
    "all_processed_chunks = list(all_hw_data.keys())\n",
    "start = timeit.default_timer()\n",
    "for i,chunk in enumerate(all_hw_chunks):\n",
    "    if chunk not in all_processed_chunks:\n",
    "        text, links = get_document_extraction(chunk)\n",
    "        all_hw_data[chunk] = {'text':text,'links':links}\n",
    "        if i%5 ==0:\n",
    "            end = timeit.default_timer()\n",
    "            print(i, end-start, chunk)\n",
    "            start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "de7cc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hw_links = []\n",
    "for extracted_data in list(all_hw_data.values()):\n",
    "    all_hw_links = all_hw_links + extracted_data['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "59823e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://dustinstansbury.github.io/theclevermachine/bias-variance-tradeoff.\n",
      "Processing: https://arxiv.org/pdf/1611.03530.pdf.\n",
      "Processing: https://arxiv.org/abs/1506.01186.\n",
      "Processing: https://arxiv.org/pdf/1611.03530.pdf\n",
      "Processing: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutionalneural-networks.pdf\n",
      "Processing: https://arxiv.org/pdf/1409.1556.pdf\n",
      "Processing: https://arxiv.org/pdf/1409.4842.pdf\n",
      "Processing: https://github.com/qfgaohao/pytorch-ssd\n",
      "Processing: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n",
      "Processing: https://github.com/onnx/tutorials/blob/master/tutorials/OnnxRuntimeServerSSDModel.ipynb\n",
      "Processing: https://storage.googleapis.com/openimages/web/index.html\n",
      "Processing: http://host.robots.ox.ac.uk/pascal/VOC/voc2007/\n",
      "Processing: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
      "Processing: https://cs231n.github.io/transfer-learning/\n",
      "Processing: http://host.robots.ox.ac.uk/pascal/VOC/voc2007/\n"
     ]
    }
   ],
   "source": [
    "extracted_hw_data = process_links(all_hw_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9d6093b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_extracted_hw_data = {key:value for key, value in extracted_hw_data.items() if len(value)>=1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e2e4d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = defaultdict(lambda: \"\")\n",
    "for key,value in cleaned_extracted_data.items():\n",
    "    links_dict[key] = value\n",
    "for key,value in cleaned_extracted_hw_data.items():\n",
    "    links_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "93db6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using a regex-based sentence tokenizer.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'(?<=[.!?]) +')  # Match end of sentence followed by space\n",
    "    return sentence_endings.split(text)\n",
    "\n",
    "def chunk_text_by_sentence(text: str, max_tokens: int, tokenizer) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk text into pieces of max_tokens length, ensuring chunks do not cut sentences.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to chunk.\n",
    "        max_tokens (int): The maximum number of tokens per chunk.\n",
    "        tokenizer: The tokenizer instance for tokenizing the text.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    sentences = split_into_sentences(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    current_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.encode(sentence)\n",
    "        if current_tokens + len(sentence_tokens) <= max_tokens:\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += len(sentence_tokens)\n",
    "        else:\n",
    "            # Complete the current chunk\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            # Start a new chunk\n",
    "            current_chunk = [sentence]\n",
    "            current_tokens = len(sentence_tokens)\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_documents_by_sentence(documents: Dict[str, str], max_tokens: int = 500) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Chunk the text of multiple documents into smaller pieces, ensuring no sentence is cut.\n",
    "    \n",
    "    Args:\n",
    "        documents (Dict[str, str]): A dictionary with document IDs as keys and text as values.\n",
    "        max_tokens (int): The maximum number of tokens per chunk.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]: A dictionary with document IDs as keys and lists of chunked text as values.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Use the tokenizer compatible with OpenAI models\n",
    "    chunked_documents = {}\n",
    "    \n",
    "    for doc_id, text in documents.items():\n",
    "        chunked_documents[doc_id] = chunk_text_by_sentence(text, max_tokens, tokenizer)\n",
    "    \n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "96c12a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_links_dict = chunk_documents_by_sentence(links_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "dd5d7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data_from_embedded_links.json\"\n",
    "with open(file_name, \"w\") as json_file:\n",
    "    json.dump(chunked_links_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e2c6f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q_and_a_docs_final = all_q_and_a_docs + all_q_and_a_docs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8d2b6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"all_q_and_a_docs_final.json\"\n",
    "with open(file_name, \"w\") as json_file:\n",
    "    json.dump(all_q_and_a_docs_final, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bb163285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'Large Language Model',\n",
       "  'output': 'A type of foundation model applied specifically to text with the ability to understand and generate human language, enabling applications such as translation, summarization, and question-answering. Foundation Model: Pre-trained on large amounts of unlabeled and self-supervised data for very general use cases.'},\n",
       " {'input': 'Transformer',\n",
       "  'output': 'A type of neural network architecture designed for handling sequences of data, particularly in natural language processing tasks. Transformers are known for their self-attention mechanism, which allows them to weigh the importance of different parts of an input sequence. They learn context and track relationships in sequential data like words in a sentence.'},\n",
       " {'input': 'Pretraining',\n",
       "  'output': 'The initial phase of training a large language model, during which the model learns general language patterns and structures from a vast corpus of text data.'},\n",
       " {'input': 'Fine tuning',\n",
       "  'output': 'The second phase of training a large language model, during which the model is fine-tuned on a smaller, domain-specific dataset to specialize in a particular task or field.'},\n",
       " {'input': 'Tokenization',\n",
       "  'output': 'The process of breaking down text into individual words or subwords, called tokens, which are then used as input for a language model.'},\n",
       " {'input': 'Vocabulary',\n",
       "  'output': 'The set of unique tokens (words or sub-words) recognized by a large language model, used for both input and output text generation.'},\n",
       " {'input': 'Context Window',\n",
       "  'output': 'The maximum number of tokens a language model can consider from the input text when generating a response or prediction.'},\n",
       " {'input': 'Zero Shot Learning',\n",
       "  'output': 'The ability of a pre-trained language model to perform a task without any additional fine-tuning or task-specific training, relying only on its general understanding of language.'},\n",
       " {'input': 'Few Shot Learning',\n",
       "  'output': 'The ability of a pre-trained language model to perform a task with minimal fine-tuning or exposure to task-specific examples.'},\n",
       " {'input': 'Transfer Learning',\n",
       "  'output': 'The process of leveraging the knowledge acquired by a model during pre-training on one task to improve performance on a different, but related, task.'},\n",
       " {'input': 'Model Size',\n",
       "  'output': 'The number of parameters (weights and biases) in a neural network, often used as a measure of the complexity and capacity of a language model.'},\n",
       " {'input': 'Bias',\n",
       "  'output': \"The presence of unfair or unjustified assumptions in a language model's output, often resulting from biases present in the training data.\"},\n",
       " {'input': 'Overfitting',\n",
       "  'output': 'A situation in which a model becomes too specialized to its training data, leading to poor performance on new or unseen data.'},\n",
       " {'input': 'Generalization',\n",
       "  'output': 'The ability of a model to perform well on new, unseen data, by learning the underlying patterns and structures of the training data without memorizing specific examples.'},\n",
       " {'input': 'Embedding',\n",
       "  'output': 'Expressing words/sentences as vectors, or an array of real values that represent characteristics of the word or sentence.'},\n",
       " {'input': 'Multitask Learning',\n",
       "  'output': 'Collect a dataset of training/test/development data for a range of different tasks, training examples are of the form (dataset, objective) sampled from the distribution of dataset & objectives, in a probabilistic framework, task: estimate a conditional distribution: p(output|input, task).'},\n",
       " {'input': 'Positional Embedding', 'output': 'Capturing word order.'},\n",
       " {'input': 'One-Shot',\n",
       "  'output': 'In addition to the task description, the model sees the a single example of the task.'},\n",
       " {'input': 'RAG (Retrieval Augmented Generation)',\n",
       "  'output': \"Stores knowledge in a database and if it's knowledge that the LLM can't answer, searches this database and processes it into the LLM. - Consists of vector database and embedding technology (to convert text into vectors).\"},\n",
       " {'input': 'Seq2Seq model',\n",
       "  'output': 'A special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc.'},\n",
       " {'input': 'Attention head',\n",
       "  'output': 'A specialized mini-brain within the AI model that helps it selectively focus on certain aspects of the input data. In the context of NLP, attention heads aid in understanding the relationships between words in a sentence or a sequence of text.'},\n",
       " {'input': 'Hallucination',\n",
       "  'output': 'Incorrect information is learned and given by the LLM as a confident answer.'},\n",
       " {'input': 'Recurrent layer',\n",
       "  'output': \"A type of deep neural network where both input data and prior hidden states are fed into the network's layers, giving the network a state and hence memory. RNNs are commonly used for sequence-based or time-based data.\"},\n",
       " {'input': 'Autoregressive',\n",
       "  'output': 'A model that learns from a series of timed steps and takes measurements from previous actions as inputs, in order to predict the value of the next time step.'},\n",
       " {'input': 'Machine learning',\n",
       "  'output': 'A type of artificial intelligence that leverages massive amounts of data so that computers can improve the accuracy of actions and predictions on their own without additional programming.'},\n",
       " {'input': 'Deep Learning',\n",
       "  'output': 'A subset of machine learning, which is essentially a neural network with three or more layers. These neural networks attempt to simulate the behavior of the human brain—albeit far from matching its ability—allowing it to \"learn\" from large amounts of data.'},\n",
       " {'input': 'Decoder-only transformer architecture',\n",
       "  'output': 'Designed to generate/create new text. Produces contextually relevant, coherent text. They receive input and they generate text relevant to that input. During pre-training, its task is to predict the next word in each sequence of text giving it the ability to understand and generate human-like text.**Tokens look at previous tokens.'},\n",
       " {'input': 'Encoder-only transformer architecture',\n",
       "  'output': \"Encoder-only models find their place in scenarios where understanding context is paramount but autoregressive generation isn't necessary (previous text doesn't really matter). By excelling in capturing contextual information, they thrive in tasks such as sentiment analysis, where interpreting the sentiment of a text requires a holistic grasp of its context. Additionally, they excel in tasks like named entity recognition, where identifying entities like names, dates, and locations demands a comprehensive understanding of the input.**Tokens look at each other.\"},\n",
       " {'input': 'Encoder-decoder transformer architecture',\n",
       "  'output': 'Encoder-decoder models are typically used for natural language processing tasks that involve understanding input sequences and generating output sequences, often with different lengths and structures. They are particularly good at tasks where there is a complex mapping between the input and output sequences and where it is crucial to capture the relationships between the elements in both sequences. Some common use cases for encoder-decoder models include text translation and summarization. Good at analyzing text and somewhat good at generating.'},\n",
       " {'input': 'Embedding layer', 'output': 'Creates embeddings from input text.'},\n",
       " {'input': 'Feedforward layer',\n",
       "  'output': \"Multiple connected layers transform the input embeddings to glean higher-level abstractions and understand the user's intent with the text input.\"},\n",
       " {'input': 'Agents',\n",
       "  'output': 'System that uses an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools. They consist of an agent core, a memory module, tools, and a planning module.'},\n",
       " {'input': 'Agent core',\n",
       "  'output': 'Foundational component built around an LLM. Decision-making module that manages behavioral characteristics of the agent. Contains overall objectives, tools for execution, explanation of planning modules, memory of past questions.'},\n",
       " {'input': 'Memory module',\n",
       "  'output': 'Store of internal logs and interactions. Both short-term (sentence by sentence) memory and long-term (conversation history) memory.'},\n",
       " {'input': 'Tools',\n",
       "  'output': 'External resources, services, or third-party APIs that agents can use to execute tasks and enhance capabilities. This includes databases, knowledge bases, external models. Ex. using a RAG pipeline to generate context-aware answers, API to search information online.'},\n",
       " {'input': 'Planning module',\n",
       "  'output': 'Plans out nuanced approaches for complicated questions. -Task and question decomposition: Breaking down one question into multiple subparts-Reflection/critic: Techniques to refine execution plan.'},\n",
       " {'input': 'Structured data',\n",
       "  'output': 'Data that fits neatly into data tables and includes discrete data types such as numbers, short text, and dates.'},\n",
       " {'input': 'Unstructured data',\n",
       "  'output': \"Data that doesn't fit neatly into a data table because its size or nature: for example, audio and video files and large text documents. Also, sentences.\"},\n",
       " {'input': 'Knowledge graph',\n",
       "  'output': 'Well suited for handling complex, multi-part collection since they store data as a network of nodes and the relationship between them. This connected data structure allows RAG apps to navigate from one piece of information to another efficiently, accessing all related information.'},\n",
       " {'input': 'Information extraction pipeline',\n",
       "  'output': 'Transformation of unstructured text into structured information. 1. Run input text through a coreference resolution model: Find all expressions that refer to a specific entity. 2. Entity disambiguation step: Accurately identifying and distinguishing between entities with similar names or references. 3. Identify relationships between entities. When combined with knowledge graphs, you can process each document individually and interconnect the different documents.'},\n",
       " {'input': 'Multi-hop question-answering task',\n",
       "  'output': \"LLM needs information from multiple documents/chunks of text to generate an answer. Chunking + embedding documents doesn't work because: 1. Provided documents might not necessarily contain all information to answer question fully. 2. Missing reference information: Some chunks may not contain the full context and there could be missing references. 3. Hard to identify ideal number of retrieved documents. Solution: Knowledge graphs. They're great with sorting and aggregating unstructured text data.\"},\n",
       " {'input': 'Knowledge graph nodes', 'output': 'Represent entities.'},\n",
       " {'input': 'Knowledge graph edges', 'output': 'Represent relationships.'},\n",
       " {'input': 'Why do we use a knowledge graph for RAG applications?',\n",
       "  'output': '1. Reduced workload during query time, improving latency. 2. Easier traversal and navigation through interconnected documents, enabling multi-hop reasoning. 3. Can easily absorb all types of data.'},\n",
       " {'input': 'Which in-context learning method involves creating an initial prompt that states the task to be completed and includes a single example question with answer followed by a second question to be answered by the LLM?',\n",
       "  'output': 'd. One Shot. One shot inference involves providing an example question with answer followed by a second question to be answered by the LLM. Few shot inference provides multiple example prompts and answers while zero shot provides only one prompt to be answered by the LLM.'},\n",
       " {'input': 'Which configuration parameter for inference can be adjusted to either increase or decrease randomness within the model output layer?',\n",
       "  'output': 'c. Temperature. Temperature is used to affect the randomness of the output of the softmax layer. A lower temperature results in reduced variability while a higher temperature results in increased randomness of the output.'},\n",
       " {'input': 'Which of the following best describes the role of data parallelism in the context of training Large Language Models (LLMs) with GPUs?',\n",
       "  'output': 'd. Data parallelism allows for the use of multiple GPUs to process different parts of the same data simultaneously, speeding up training time. Data parallelism is a strategy that splits the training data across multiple GPUs. Each GPU processes a different subset of the data simultaneously, which can greatly speed up the overall training time.'},\n",
       " {'input': 'Which of the following statements about pretraining scaling laws are correct? Select all that apply.',\n",
       "  'output': \"a, b & c. a. To scale our model, we need to jointly increase dataset size and model size, or they can become a bottleneck for each other. b. There is a relationship between model size (in number of parameters) and the optimal number of tokens to train the model with. c. When measuring compute budget, we can use 'PetaFlops per second-Day' as a metric.\"},\n",
       " {'input': 'Interacting with Large Language Models (LLMs) differs from traditional machine learning models. Working with LLMs involves natural language input, known as a _____, resulting in output from the Large Language Model, known as the ______.',\n",
       "  'output': 'd. prompt, completion'},\n",
       " {'input': 'Large Language Models (LLMs) are capable of performing multiple tasks supporting a variety of use cases. Which of the following tasks supports the use case of converting code comments into executable code?',\n",
       "  'output': 'c. Translation'},\n",
       " {'input': 'What is the self-attention that powers the transformer architecture?',\n",
       "  'output': 'a. A mechanism that allows a model to focus on different parts of the input sequence during computation.'},\n",
       " {'input': 'Which of the following stages are part of the generative AI model lifecycle mentioned in the course? (Select all that apply)',\n",
       "  'output': 'b, c, d & e. b. Selecting a candidate model and potentially pre-training a custom model. c. Manipulating the model to align with specific project needs. d. Defining the problem and identifying relevant datasets. e. Deploying the model into the infrastructure and integrating it with the application.'},\n",
       " {'input': \"'RNNs are better than Transformers for generative AI Tasks.' Is this true or false?\",\n",
       "  'output': 'False'},\n",
       " {'input': 'Which transformer-based model architecture has the objective of guessing a masked token based on the previous sequence of tokens by building bidirectional representations of the input sequence?',\n",
       "  'output': 'c. Autoencoder'},\n",
       " {'input': 'Which transformer-based model architecture is well-suited to the task of text translation?',\n",
       "  'output': 'b. Sequence-to-sequence'},\n",
       " {'input': 'Do we always need to increase the model size to improve its performance?',\n",
       "  'output': 'False'},\n",
       " {'input': 'Scaling laws for pre-training large language models consider several aspects to maximize performance of a model within a set of constraints and available scaling choices. Select all alternatives that should be considered for scaling when performing model pre-training?',\n",
       "  'output': 'a, c & d. a. Compute budget: Compute constraints. c. Model size: Number of parameters. d. Dataset size: Number of tokens.'},\n",
       " {'input': \"'You can combine data parallelism with model parallelism to train LLMs.' Is this true or false?\",\n",
       "  'output': 'True'},\n",
       " {'input': 'Which of the following are true in respect to Catastrophic Forgetting? Select all that apply.',\n",
       "  'output': 'b, c & d. b. Catastrophic forgetting occurs when a machine learning model forgets previously learned information as it learns new information. c. Catastrophic forgetting is a common problem in machine learning, especially in deep learning models. d. One way to mitigate catastrophic forgetting is by using regularization techniques to limit the amount of change that can be made to the weights of the model during training.'},\n",
       " {'input': 'What is the purpose of fine-tuning with prompt datasets?',\n",
       "  'output': 'd. To improve the performance and adaptability of a pre-trained language model for specific tasks.'},\n",
       " {'input': \"'Parameter Efficient Fine-Tuning (PEFT) updates only a small subset of parameters. This helps prevent catastrophic forgetting.' True or False?\",\n",
       "  'output': 'True'},\n",
       " {'input': 'Parameter Efficient Fine-Tuning (PEFT) methods specifically attempt to address some of the challenges of performing full fine-training. Which of the following options describe challenges that PEFT tries to overcome?',\n",
       "  'output': 'a, b & c. a. Computational constraints. b. Catastrophic forgetting. c. Storage requirements.'},\n",
       " {'input': 'Fill in the blanks: __________ involves using many prompt-completion examples as the labeled training dataset to continue training the model by updating its weights. This is different from _________ where you provide prompt-completion examples during inference.',\n",
       "  'output': 'd. Instruction fine-tuning, In-context learning'},\n",
       " {'input': 'Fine-tuning a model on a single task can improve model performance specifically on that task; however, it can also degrade the performance of other tasks as a side effect. This phenomenon is known as:',\n",
       "  'output': 'd. Catastrophic forgetting'},\n",
       " {'input': 'Which evaluation metric below focuses on precision in matching generated output to the reference text and is used for text translation?',\n",
       "  'output': 'b. BLEU'},\n",
       " {'input': 'Which of the following statements about multi-task finetuning is correct? Select all that apply.',\n",
       "  'output': 'a & d. a. FLAN-T5 was trained with multi-task finetuning. d. Multi-task finetuning can help prevent catastrophic forgetting.'},\n",
       " {'input': \"'Smaller LLMs can struggle with one-shot and few-shot inference:' Is this true or false?\",\n",
       "  'output': 'True'},\n",
       " {'input': 'Which of the following are Parameter Efficient Fine-Tuning (PEFT) methods? Select all that apply.',\n",
       "  'output': 'a, b & d. a. Reparameterization. b. Additive. d. Selective.'},\n",
       " {'input': 'Which of the following best describes how LoRA works?',\n",
       "  'output': 'c. LoRA decomposes weights into two smaller rank matrices and trains those instead of the full model weights.'},\n",
       " {'input': 'What is a soft prompt in the context of LLMs (Large Language Models)?',\n",
       "  'output': 'a. A set of trainable tokens that are added to a prompt and whose values are updated during additional training to improve performance on specific tasks.'},\n",
       " {'input': \"'Prompt Tuning is a technique used to adjust all hyperparameters of a language model.' Is this true or false?\",\n",
       "  'output': 'False'},\n",
       " {'input': \"'PEFT methods can reduce the memory needed for fine-tuning dramatically, sometimes to just 12-20% of the memory needed for full fine-tuning.' Is this true or false?\",\n",
       "  'output': 'True'},\n",
       " {'input': 'When using Reinforcement Learning with Human Feedback (RLHF) to align large language models with human preferences, what is the role of human labelers?',\n",
       "  'output': 'b. To score prompt completions, so that this score is used to train the reward model component of the RLHF process.'},\n",
       " {'input': 'How can RLHF align the performance of large language models with human preferences? Select all that apply',\n",
       "  'output': 'b & c. b. RLHF can help reduce model toxicity and misinformation. c. RLHF can enhance the interpretability of generated text.'}]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_q_and_a_docs_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "12cc609d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert ray-project / llm-numbers Public Notifications You must be signed in to change notification settings Fork 141 Star 4.1k Numbers every LLM developer should know 4.1k stars 141 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 9 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights ray-project/llm-numbers mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know.',\n",
       " \"It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle.\",\n",
       " 'It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.',\n",
       " 'Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.',\n",
       " '2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue.',\n",
       " \"Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 Footer © 2024 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.\",\n",
       " 'Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to...',\n",
       " 'Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus Sign in Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation.',\n",
       " 'Cancel Create saved search Sign in Sign up Reseting focus Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation.',\n",
       " 'Cancel Create saved search Sign in Sign up Reseting focus GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less GitHub Copilot Write better code with AI GitHub Copilot Security Find and fix vulnerabilities Security Actions Automate any workflow Actions Codespaces Instant dev environments Codespaces Issues Plan and track work Issues Code Review Manage code changes Code Review Discussions Collaborate outside of code Discussions Code Search Find more, search less Code Search Explore All features Documentation GitHub Skills Blog Explore All features Documentation GitHub Skills Blog By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Topics AI DevOps Security Software Development View all Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections GitHub Sponsors Fund open source developers GitHub Sponsors Fund open source developers GitHub Sponsors The ReadME Project GitHub community articles The ReadME Project GitHub community articles The ReadME Project Repositories Topics Trending Collections Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Enterprise platform AI-powered developer platform Enterprise platform AI-powered developer platform Enterprise platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Advanced Security Enterprise-grade security features Advanced Security GitHub Copilot Enterprise-grade AI features GitHub Copilot Premium Support Enterprise-grade 24/7 support Premium Support Search or jump to...',\n",
       " 'Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Provide feedback Provide feedback Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted We read every piece of feedback, and take your input very seriously. Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Saved searches Use saved searches to filter your results more quickly Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Name Query To see all available qualifiers, see our documentation. Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.',\n",
       " \"Dismiss alert ray-project / llm-numbers Public Notifications You must be signed in to change notification settings Fork 141 Star 4.1k Numbers every LLM developer should know 4.1k stars 141 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 9 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights ray-project/llm-numbers mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens.\",\n",
       " 'Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees).',\n",
       " '6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference.',\n",
       " 'You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. 2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point.',\n",
       " \"~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40.\",\n",
       " \"↩ About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 ray-project / llm-numbers Public Notifications You must be signed in to change notification settings Fork 141 Star 4.1k Numbers every LLM developer should know 4.1k stars 141 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 9 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights ray-project/llm-numbers mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens.\",\n",
       " \"For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings).\",\n",
       " 'Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405.',\n",
       " 'However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. 2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds.',\n",
       " \"This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens.\",\n",
       " '↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 ray-project / llm-numbers Public Notifications You must be signed in to change notification settings Fork 141 Star 4.1k Numbers every LLM developer should know 4.1k stars 141 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 9 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights ray-project / llm-numbers Public Notifications You must be signed in to change notification settings Fork 141 Star 4.1k ray-project / llm-numbers Public ray-project / llm-numbers Public Notifications You must be signed in to change notification settings Fork 141 Star 4.1k Star 4.1k Numbers every LLM developer should know 4.1k stars 141 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Numbers every LLM developer should know 4.1k stars 141 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Numbers every LLM developer should know 4.1k stars 141 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Star Star Notifications You must be signed in to change notification settings Additional navigation options Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights ray-project/llm-numbers mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations.',\n",
       " \"Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle.\",\n",
       " 'It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.',\n",
       " 'Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.',\n",
       " '2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue.',\n",
       " \"Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations.\",\n",
       " \"Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle.\",\n",
       " 'It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.',\n",
       " 'Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.',\n",
       " '2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue.',\n",
       " \"Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations.\",\n",
       " \"Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle.\",\n",
       " 'It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.',\n",
       " 'Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.',\n",
       " '2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue.',\n",
       " \"Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations.\",\n",
       " \"Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle.\",\n",
       " 'It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.',\n",
       " 'Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.',\n",
       " '2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue.',\n",
       " \"Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations.\",\n",
       " \"Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle.\",\n",
       " 'It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.',\n",
       " 'Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.',\n",
       " '2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue.',\n",
       " \"Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage.\",\n",
       " \"Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo.\",\n",
       " 'GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers.',\n",
       " 'The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. 2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter.',\n",
       " 'There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum.',\n",
       " \"If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models.\",\n",
       " \"Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g.\",\n",
       " '“What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more).',\n",
       " 'The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. 2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical.',\n",
       " \"~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th!\",\n",
       " \"We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ mainBranchesTagsGo to fileCode mainBranchesTags main main main BranchesTags Go to fileCode Go to file Go to file Folders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money.\",\n",
       " \"This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x!\",\n",
       " '10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model.',\n",
       " '< 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. 2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially.',\n",
       " \"You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs.\",\n",
       " \"Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ Folders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all files Last commit message Last commit date Latest commit History37 Commits History37 Commits History37 Commits README.md README.md README.md README.md README.md README.md README.md README.md View all files Repository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money.\",\n",
       " \"1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate.\",\n",
       " 'In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7.',\n",
       " 'Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. 2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU.',\n",
       " \">10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08.\",\n",
       " \"↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ Repository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens.\",\n",
       " 'Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees).',\n",
       " '6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference.',\n",
       " 'You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. 2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point.',\n",
       " \"~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40.\",\n",
       " \"↩ Repository files navigationREADME Numbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark.\",\n",
       " '~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model.',\n",
       " '1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters. Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have.',\n",
       " 'Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices. 2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB.',\n",
       " \"No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue. Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Footnotes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40. ↩ Numbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations.\",\n",
       " \"Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage. Notes on the Github version Last updates: 2023-05-17 If you feel there's an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR. We are thinking the next thing we should add here is some stats on tokens per second of different models. Prompts 40-90%1: Amount saved by appending “Be Concise” to your prompt It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money. 1.3:1 -- Average tokens per word LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM's embedding corpus. Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens. Prices2 Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (Anthropic, Cohere) are in the same ballpark. ~50:1 -- Cost Ratio of GPT-4 to GPT-3.5 Turbo3 What this means is that for many practical applications, it’s much better to use GPT-4 for things like generating high quality fine tuning data, or for automated evaluation of other models -- things you might only do once instead of it living in the middle of your inference cycle.\",\n",
       " 'It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example. 5:1 -- Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x4 less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x! 10:1 -- Cost Ratio of OpenAI embedding to Self-Hosted embedding Note: this number is sensitive to load and embedding batch size, so please consider this approximate. In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using Hugging Face’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees). 6:1 -- Cost Ratio of OpenAI fine tuned vs base model queries It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model. 1:1 -- Cost Ratio of Self-Hosted base vs fine-tuned model queries If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.',\n",
       " 'Training and Fine Tuning ~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens The LLaMa paper mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our bread and butter! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model. < 0.001: Cost ratio of fine tuning vs training from scratch This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a 6B parameter model for about $7. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $405. However, fine tuning is one thing and training from scratch is another … GPU Memory If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning. V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.',\n",
       " '2x number of parameters: Typical GPU memory requirements of an LLM for serving For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical. ~1GB: Typical GPU memory requirements of an embedding model Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like sentence transformers. OpenAI also has its own embeddings that they provide commercially. You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU. >10x: Throughput improvement from batching LLM requests Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point. ~1 MB: GPU Memory required for 1 token of output with a 13B parameter model The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue.',\n",
       " \"Cheatsheet Next Steps See our earlier blog series on solving Generative AI infrastructure and using LangChain with Ray. If you are interested in learning more about Ray, see Ray.io and Docs.Ray.io. To connect with the Ray community join #LLM on the Ray Slack or our Discuss forum. If you are interested in our Ray hosted service for ML Training and Serving, see Anyscale.com/Platform and click the 'Try it now' button Ray Summit 2023: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join Ray Summit on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as practical training focused on LLMs. Notes Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08. ↩ Retrieved from http://openai.com/pricing on 2023-05-08. ↩ GPT-4: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). GPT-3.5 Turbo: 0.2c/1k tokens. ↩ This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast. ↩ 1 million words / 0.75 tokens/word / 1000*0.03 = $40.\",\n",
       " '↩ About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Releases No releases published Packages 0 No packages published Contributors 8 About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository About Numbers every LLM developer should know Resources Readme Activity Custom properties Stars 4.1k stars Watchers 60 watching Forks 141 forks Report repository Numbers every LLM developer should know Readme Activity Custom properties 4.1k stars 60 watching 141 forks Report repository Releases No releases published Releases No releases published No releases published Packages 0 No packages published Packages 0 No packages published No packages published Contributors 8 Contributors 8 © 2024 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information © 2024 GitHub, Inc. You can’t perform that action at this time.']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_links_dict['https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e0b04e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-': 'NeurIPS Proceedings Search ImageNet Classification with Deep Convolutional Neural Networks Part of Advances in Neural Information Processing Systems 25 (NIPS 2012) Bibtex Metadata Paper Supplemental Authors Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton Abstract We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\\\\% and 18.9\\\\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective. ImageNet Classification with Deep Convolutional Neural Networks Part of Advances in Neural Information Processing Systems 25 (NIPS 2012) Bibtex Metadata Paper Supplemental Authors Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton Abstract We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\\\\% and 18.9\\\\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective. Part of Advances in Neural Information Processing Systems 25 (NIPS 2012) Bibtex Metadata Paper Supplemental Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\\\\% and 18.9\\\\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective. We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\\\\% and 18.9\\\\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective. Name Change Policy × Requests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings. Use the \"Report an Issue\" link to request a name change. Name Change Policy × Requests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings. Use the \"Report an Issue\" link to request a name change. Name Change Policy × Requests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings. Use the \"Report an Issue\" link to request a name change. Name Change Policy × Requests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings. Use the \"Report an Issue\" link to request a name change. Requests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings. Use the \"Report an Issue\" link to request a name change. Report an Issue | Name Change Policy Do not remove: This comment is monitored to verify that the site is working properly',\n",
       " 'https://github.com/qfgaohao/pytorch-ssd': 'Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert qfgaohao / pytorch-ssd Public Notifications You must be signed in to change notification settings Fork 534 Star 1.4k MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad License MIT license 1.4k stars 534 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 104 Pull requests 7 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights qfgaohao/pytorch-ssd masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% Footer © 2024 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time. Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus Sign in Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less GitHub Copilot Write better code with AI GitHub Copilot Security Find and fix vulnerabilities Security Actions Automate any workflow Actions Codespaces Instant dev environments Codespaces Issues Plan and track work Issues Code Review Manage code changes Code Review Discussions Collaborate outside of code Discussions Code Search Find more, search less Code Search Explore All features Documentation GitHub Skills Blog Explore All features Documentation GitHub Skills Blog By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Topics AI DevOps Security Software Development View all Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections GitHub Sponsors Fund open source developers GitHub Sponsors Fund open source developers GitHub Sponsors The ReadME Project GitHub community articles The ReadME Project GitHub community articles The ReadME Project Repositories Topics Trending Collections Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Enterprise platform AI-powered developer platform Enterprise platform AI-powered developer platform Enterprise platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Advanced Security Enterprise-grade security features Advanced Security GitHub Copilot Enterprise-grade AI features GitHub Copilot Premium Support Enterprise-grade 24/7 support Premium Support Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Provide feedback Provide feedback Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted We read every piece of feedback, and take your input very seriously. Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Saved searches Use saved searches to filter your results more quickly Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Name Query To see all available qualifiers, see our documentation. Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert qfgaohao / pytorch-ssd Public Notifications You must be signed in to change notification settings Fork 534 Star 1.4k MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad License MIT license 1.4k stars 534 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 104 Pull requests 7 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights qfgaohao/pytorch-ssd masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% qfgaohao / pytorch-ssd Public Notifications You must be signed in to change notification settings Fork 534 Star 1.4k MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad License MIT license 1.4k stars 534 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 104 Pull requests 7 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights qfgaohao/pytorch-ssd masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% qfgaohao / pytorch-ssd Public Notifications You must be signed in to change notification settings Fork 534 Star 1.4k MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad License MIT license 1.4k stars 534 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 104 Pull requests 7 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights qfgaohao / pytorch-ssd Public Notifications You must be signed in to change notification settings Fork 534 Star 1.4k qfgaohao / pytorch-ssd Public qfgaohao / pytorch-ssd Public Notifications You must be signed in to change notification settings Fork 534 Star 1.4k Star 1.4k MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad License MIT license 1.4k stars 534 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad License MIT license 1.4k stars 534 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad MIT license 1.4k stars 534 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Star Star Notifications You must be signed in to change notification settings Additional navigation options Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights qfgaohao/pytorch-ssd masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. masterBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. masterBranchesTagsGo to fileCode masterBranchesTags master master master BranchesTags Go to fileCode Go to file Go to file Folders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all filesRepository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. Folders and filesNameNameLast commit messageLast commit dateLatest commit History183 Commitsmodelsmodels visionvision .gitignore.gitignore LICENSELICENSE README.mdREADME.md convert_to_caffe2_models.pyconvert_to_caffe2_models.py draw_eval_results.pydraw_eval_results.py eval_ssd.pyeval_ssd.py extract_tf_weights.pyextract_tf_weights.py gun.jpggun.jpg open_images_downloader.pyopen_images_downloader.py prune_alexnet.pyprune_alexnet.py readme_ssd_example.jpgreadme_ssd_example.jpg run_ssd_example.pyrun_ssd_example.py run_ssd_live_caffe2.pyrun_ssd_live_caffe2.py run_ssd_live_demo.pyrun_ssd_live_demo.py train_ssd.pytrain_ssd.py translate_tf_mobilenetv1.pytranslate_tf_mobilenetv1.py visual_tf_models.pyvisual_tf_models.py View all files Last commit message Last commit date Latest commit History183 Commits History183 Commits History183 Commits models models models models models models models models vision vision vision vision vision vision vision vision .gitignore .gitignore .gitignore .gitignore .gitignore .gitignore .gitignore .gitignore LICENSE LICENSE LICENSE LICENSE LICENSE LICENSE LICENSE LICENSE README.md README.md README.md README.md README.md README.md README.md README.md convert_to_caffe2_models.py convert_to_caffe2_models.py convert_to_caffe2_models.py convert_to_caffe2_models.py convert_to_caffe2_models.py convert_to_caffe2_models.py convert_to_caffe2_models.py convert_to_caffe2_models.py draw_eval_results.py draw_eval_results.py draw_eval_results.py draw_eval_results.py draw_eval_results.py draw_eval_results.py draw_eval_results.py draw_eval_results.py eval_ssd.py eval_ssd.py eval_ssd.py eval_ssd.py eval_ssd.py eval_ssd.py eval_ssd.py eval_ssd.py extract_tf_weights.py extract_tf_weights.py extract_tf_weights.py extract_tf_weights.py extract_tf_weights.py extract_tf_weights.py extract_tf_weights.py extract_tf_weights.py gun.jpg gun.jpg gun.jpg gun.jpg gun.jpg gun.jpg gun.jpg gun.jpg open_images_downloader.py open_images_downloader.py open_images_downloader.py open_images_downloader.py open_images_downloader.py open_images_downloader.py open_images_downloader.py open_images_downloader.py prune_alexnet.py prune_alexnet.py prune_alexnet.py prune_alexnet.py prune_alexnet.py prune_alexnet.py prune_alexnet.py prune_alexnet.py readme_ssd_example.jpg readme_ssd_example.jpg readme_ssd_example.jpg readme_ssd_example.jpg readme_ssd_example.jpg readme_ssd_example.jpg readme_ssd_example.jpg readme_ssd_example.jpg run_ssd_example.py run_ssd_example.py run_ssd_example.py run_ssd_example.py run_ssd_example.py run_ssd_example.py run_ssd_example.py run_ssd_example.py run_ssd_live_caffe2.py run_ssd_live_caffe2.py run_ssd_live_caffe2.py run_ssd_live_caffe2.py run_ssd_live_caffe2.py run_ssd_live_caffe2.py run_ssd_live_caffe2.py run_ssd_live_caffe2.py run_ssd_live_demo.py run_ssd_live_demo.py run_ssd_live_demo.py run_ssd_live_demo.py run_ssd_live_demo.py run_ssd_live_demo.py run_ssd_live_demo.py run_ssd_live_demo.py train_ssd.py train_ssd.py train_ssd.py train_ssd.py train_ssd.py train_ssd.py train_ssd.py train_ssd.py translate_tf_mobilenetv1.py translate_tf_mobilenetv1.py translate_tf_mobilenetv1.py translate_tf_mobilenetv1.py translate_tf_mobilenetv1.py translate_tf_mobilenetv1.py translate_tf_mobilenetv1.py translate_tf_mobilenetv1.py visual_tf_models.py visual_tf_models.py visual_tf_models.py visual_tf_models.py visual_tf_models.py visual_tf_models.py visual_tf_models.py visual_tf_models.py View all files Repository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. Repository files navigationREADMEMIT licenseSingle Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. Repository files navigationREADMEMIT license Single Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Python 3.6+ OpenCV Pytorch 1.0 or Pytorch 0.4+ Caffe2 Pandas Boto3 if you want to train models on the Google OpenImages Dataset. Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO Resnet34 Based Model. BatchNorm Fusion. Single Shot MultiBox Detector Implementation in Pytorch This repo implements SSD (Single Shot MultiBox Detector). The implementation is heavily influenced by the projects ssd.pytorch and Detectron. The design goal is modularity and extensibility. Currently, it has MobileNetV1, MobileNetV2, and VGG based SSD/SSD-Lite implementations. It also has out-of-box support for retraining on Google Open Images dataset. Dependencies Download models Please download the models and put them into the folder \"./models\". The following sections will need them. URL: https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing Run the demo Run the live MobilenetV1 SSD demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt Run the live demo in Caffe2 # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_caffe2.py models/mobilenet-v1-ssd_init_net.pb models/mobilenet-v1-ssd_predict_net.pb models/voc-model-labels.txt You can see a decent speed boost by using Caffe2. Run the live MobileNetV2 SSD Lite demo # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt The above MobileNetV2 SSD-Lite model is not ONNX-Compatible, as it uses Relu6 which is not supported by ONNX. The code supports the ONNX-Compatible version. Once I have trained a good enough MobileNetV2 model with Relu, I will upload the corresponding Pytorch and Caffe2 models. You may notice MobileNetV2 SSD/SSD-Lite is slower than MobileNetV1 SSD/Lite on PC. However, MobileNetV2 is faster on mobile devices. Pretrained Models Mobilenet V1 SSD If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mobilenet-v1-ssd-mp-0_675.pth Average Precision Per-class: aeroplane: 0.6742489426027927 bicycle: 0.7913672875238116 bird: 0.612096015101108 boat: 0.5616407126931772 bottle: 0.3471259064860268 bus: 0.7742298893362103 car: 0.7284171192326804 cat: 0.8360675520354323 chair: 0.5142295855384792 cow: 0.6244090341627014 diningtable: 0.7060035669312754 dog: 0.7849252606216821 horse: 0.8202146617282785 motorbike: 0.793578272243471 person: 0.7042670984734087 pottedplant: 0.40257147509774405 sheep: 0.6071252282334352 sofa: 0.7549120254763918 train: 0.8270992920206008 tvmonitor: 0.6459903029666852 Average Precision Across All Classes:0.6755 MobileNetV2 SSD-Lite If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. Model: mb2-ssd-lite-mp-0_686.pth Average Precision Per-class: aeroplane: 0.6973327307871002 bicycle: 0.7823755921687233 bird: 0.6342429230125619 boat: 0.5478160937380846 bottle: 0.3564069147093762 bus: 0.7882037885117419 car: 0.7444122242934775 cat: 0.8198865557991936 chair: 0.5378973422880109 cow: 0.6186076149254742 diningtable: 0.7369559500950861 dog: 0.7848265495754562 horse: 0.8222948787839229 motorbike: 0.8057808854619948 person: 0.7176976451996411 pottedplant: 0.42802932547480066 sheep: 0.6259124005994047 sofa: 0.7840368059271103 train: 0.8331588002612781 tvmonitor: 0.6555051795079904 Average Precision Across All Classes:0.6860690100560214 The code to re-produce the model: # If you haven\\'t downloaded the models, please download from https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu?usp=sharing. python train_ssd.py --dataset_type voc --datasets ~/data/VOC0712/VOC2007 ~/data/VOC0712/VOC2012 --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb2-ssd-lite --base_net models/mb2-imagenet-71_8.pth --scheduler cosine --lr 0.01 --t_max 200 --validation_epochs 5 --num_epochs 200 VGG SSD Model: vgg16-ssd-mp-0_7726.pth Average Precision Per-class: aeroplane: 0.7957406334737802 bicycle: 0.8305351156180996 bird: 0.7570969203281721 boat: 0.7043869846367731 bottle: 0.5151666571756393 bus: 0.8375121237865507 car: 0.8581508869699901 cat: 0.8696185705648963 chair: 0.6165431194526735 cow: 0.8066422244852381 diningtable: 0.7629391213959706 dog: 0.8444541531856452 horse: 0.8691922094815812 motorbike: 0.8496564646906418 person: 0.793785185549561 pottedplant: 0.5233462463152305 sheep: 0.7786762429478917 sofa: 0.8024887701948746 train: 0.8713861172265407 tvmonitor: 0.7650514925384194 Average Precision Across All Classes:0.7726184620009084 The code to re-produce the model: wget -P models https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net vgg16-ssd --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 200 --scheduler \"multi-step” —-milestones “120,160” Training python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net mb1-ssd --base_net models/mobilenet_v1_with_relu_69_5.pth --batch_size 24 --num_epochs 200 --scheduler cosine --lr 0.01 --t_max 200 The dataset path is the parent directory of the folders: Annotations, ImageSets, JPEGImages, SegmentationClass and SegmentationObject. You can use multiple datasets to train. Evaluation python eval_ssd.py --net mb1-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/mobilenet-v1-ssd-mp-0_675.pth --label_file models/voc-model-labels.txt Convert models to ONNX and Caffe2 models python convert_to_caffe2_models.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt The converted models are models/mobilenet-v1-ssd.onnx, models/mobilenet-v1-ssd_init_net.pb and models/mobilenet-v1-ssd_predict_net.pb. The models in the format of pbtxt are also saved for reference. Retrain on Open Images Dataset Let\\'s we are building a model to detect guns for security purpose. Before you start you can try the demo. python run_ssd_example.py mb1-ssd models/gun_model_2.21.pth models/open-images-model-labels.txt ~/Downloads/big.JPG If you manage to get more annotated data, the accuracy could become much higher. Download data python open_images_downloader.py --root ~/data/open_images --class_names \"Handgun,Shotgun\" --num_workers 20 It will download data into the folder ~/data/open_images. The content of the data directory looks as follows. class-descriptions-boxable.csv test validation sub-test-annotations-bbox.csv test-annotations-bbox.csv validation-annotations-bbox.csv sub-train-annotations-bbox.csv train sub-validation-annotations-bbox.csv train-annotations-bbox.csv The folders train, test, validation contain the images. The files like sub-train-annotations-bbox.csv is the annotation file. Retrain python train_ssd.py --dataset_type open_images --datasets ~/data/open_images --net mb1-ssd --pretrained_ssd models/mobilenet-v1-ssd-mp-0_675.pth --scheduler cosine --lr 0.01 --t_max 100 --validation_epochs 5 --num_epochs 100 --base_net_lr 0.001 --batch_size 5 You can freeze the base net, or all the layers except the prediction heads. --freeze_base_net Freeze base net layers. --freeze_net Freeze all the layers except the prediction head. You can also use different learning rates for the base net, the extra layers and the prediction heads. --lr LR, --learning-rate LR --base_net_lr BASE_NET_LR initial learning rate for base net. --extra_layers_lr EXTRA_LAYERS_LR As subsets of open images data can be very unbalanced, it also provides a handy option to roughly balance the data. --balance_data Balance training data by down-sampling more frequent labels. Test on image python run_ssd_example.py mb1-ssd models/mobilenet-v1-ssd-Epoch-99-Loss-2.2184619531035423.pth models/open-images-model-labels.txt ~/Downloads/gun.JPG ONNX Friendly VGG16 SSD ! The model is not really ONNX-Friendly due the issue mentioned here \"#33 (comment)\" The Scaled L2 Norm Layer has been replaced with BatchNorm to make the net ONNX compatible. Train The pretrained based is borrowed from https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth . python train_ssd.py --datasets ~/data/VOC0712/VOC2007/ ~/data/VOC0712/VOC2012/ --validation_dataset ~/data/VOC0712/test/VOC2007/ --net \"vgg16-ssd\" --base_net models/vgg16_reducedfc.pth --batch_size 24 --num_epochs 150 --scheduler cosine --lr 0.0012 --t_max 150 --validation_epochs 5 Eval python eval_ssd.py --net vgg16-ssd --dataset ~/data/VOC0712/test/VOC2007/ --trained_model models/vgg16-ssd-Epoch-115-Loss-2.819455094383535.pth --label_file models/voc-model-labels.txt TODO About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository Releases No releases published Packages 0 No packages published Contributors 12 Languages Python 100.0% About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository About MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad Topics pytorch ssd object-detection open-images Resources Readme License MIT license Activity Stars 1.4k stars Watchers 39 watching Forks 534 forks Report repository MobileNetV1, MobileNetV2, VGG based SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box support for retraining on Open Images dataset. ONNX and Caffe2 support. Experiment Ideas like CoordConv. medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad pytorch ssd object-detection open-images pytorch ssd object-detection open-images Readme MIT license Activity 1.4k stars 39 watching 534 forks Report repository Releases No releases published Releases No releases published No releases published Packages 0 No packages published Packages 0 No packages published No packages published Contributors 12 Contributors 12 Languages Python 100.0% Languages Python 100.0% © 2024 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information © 2024 GitHub, Inc. You can’t perform that action at this time.',\n",
       " 'https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html': 'Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Become a Member Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Become a Member Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Become a Member Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Become a Member Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Run PyTorch locally or get started quickly with one of the supported cloud platforms Whats new in PyTorch tutorials Familiarize yourself with PyTorch concepts and modules Bite-size, ready-to-deploy PyTorch code examples Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Learn about the tools and frameworks in the PyTorch Ecosystem Join the PyTorch developer community to contribute, learn, and get your questions answered A place to discuss PyTorch code, issues, install, research Find resources and get questions answered Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Build innovative and privacy-aware AI experiences for edge devices End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Explore the documentation for comprehensive guidance on how to use PyTorch Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts Catch up on the latest technical news and happenings Stories from the PyTorch ecosystem Learn about the latest PyTorch tutorials, new, and more Learn how our community solves real, everyday machine learning problems with PyTorch Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Learn more about the PyTorch Foundation Become a Member Table of Contents 2.5.0+cu124 Google Search Classic Search PyTorch Recipes See All Recipes See All Prototype Recipes Introduction to PyTorch Learn the Basics Quickstart Tensors Datasets & DataLoaders Transforms Build the Neural Network Automatic Differentiation with torch.autograd Optimizing Model Parameters Save and Load the Model Introduction to PyTorch - YouTube Series Introduction to PyTorch Introduction to PyTorch Tensors The Fundamentals of Autograd Building Models with PyTorch PyTorch TensorBoard Support Training with PyTorch Model Understanding with Captum Learning PyTorch Deep Learning with PyTorch: A 60 Minute Blitz Learning PyTorch with Examples What is torch.nn really? NLP from Scratch Visualizing Models, Data, and Training with TensorBoard A guide on good usage of non_blocking and pin_memory() in PyTorch Image and Video TorchVision Object Detection Finetuning Tutorial Transfer Learning for Computer Vision Tutorial Adversarial Example Generation DCGAN Tutorial Spatial Transformer Networks Tutorial Optimizing Vision Transformer Model for Deployment Whole Slide Image Classification Using PyTorch and TIAToolbox Audio Audio I/O Audio Resampling Audio Data Augmentation Audio Feature Extractions Audio Feature Augmentation Audio Datasets Speech Recognition with Wav2Vec2 Text-to-speech with Tacotron2 Forced Alignment with Wav2Vec2 Backends Introduction to ONNX Reinforcement Learning Reinforcement Learning (DQN) Tutorial Reinforcement Learning (PPO) with TorchRL Tutorial Train a Mario-playing RL Agent Pendulum: Writing your environment and transforms with TorchRL Deploying PyTorch Models in Production Introduction to ONNX Deploying PyTorch in Python via a REST API with Flask Introduction to TorchScript Loading a TorchScript Model in C++ (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Real Time Inference on Raspberry Pi 4 (30 fps!) Profiling PyTorch Profiling your PyTorch Module Introduction to Holistic Trace Analysis Trace Diff using Holistic Trace Analysis Code Transforms with FX (beta) Building a Convolution/Batch Norm fuser in FX (beta) Building a Simple CPU Performance Profiler with FX Frontend APIs (beta) Channels Last Memory Format in PyTorch Forward-mode Automatic Differentiation (Beta) Jacobians, Hessians, hvp, vhp, and more: composing function transforms Model ensembling Per-sample-gradients Using the PyTorch C++ Frontend Dynamic Parallelism in TorchScript Autograd in C++ Frontend Extending PyTorch PyTorch Custom Operators Python Custom Operators Custom C++ and CUDA Operators Double Backward with Custom Functions Fusing Convolution and Batch Norm using Custom Function Custom C++ and CUDA Extensions Extending TorchScript with Custom C++ Operators Extending TorchScript with Custom C++ Classes Registering a Dispatched Operator in C++ Extending dispatcher for a new backend in C++ Facilitating New Backend Integration by PrivateUse1 Model Optimization Profiling your PyTorch Module PyTorch Profiler With TensorBoard Hyperparameter tuning with Ray Tune Optimizing Vision Transformer Model for Deployment Parametrizations Tutorial Pruning Tutorial (beta) Dynamic Quantization on an LSTM Word Language Model (beta) Dynamic Quantization on BERT (beta) Quantized Transfer Learning for Computer Vision Tutorial (beta) Static Quantization with Eager Mode in PyTorch Grokking PyTorch Intel CPU performance from first principles Grokking PyTorch Intel CPU performance from first principles (Part 2) Getting Started - Accelerate Your Scripts with nvFuser Multi-Objective NAS with Ax Introduction to torch.compile Compiled Autograd: Capturing a larger backward graph for torch.compile Inductor CPU backend debugging and profiling (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA) Knowledge Distillation Tutorial Parallel and Distributed Training Distributed and Parallel Training Tutorials PyTorch Distributed Overview Distributed Data Parallel in PyTorch - Video Tutorials Single-Machine Model Parallel Best Practices Getting Started with Distributed Data Parallel Writing Distributed Applications with PyTorch Getting Started with Fully Sharded Data Parallel(FSDP) Advanced Model Training with Fully Sharded Data Parallel (FSDP) Introduction to Libuv TCPStore Backend Large Scale Transformer model training with Tensor Parallel (TP) Introduction to Distributed Pipeline Parallelism Customize Process Group Backends Using Cpp Extensions Getting Started with Distributed RPC Framework Implementing a Parameter Server Using Distributed RPC Framework Implementing Batch RPC Processing Using Asynchronous Executions Combining Distributed DataParallel with Distributed RPC Framework Distributed Training with Uneven Inputs Using the Join Context Manager Edge with ExecuTorch Exporting to ExecuTorch Tutorial Running an ExecuTorch Model in C++ Tutorial Using the ExecuTorch SDK to Profile a Model Building an ExecuTorch iOS Demo App Building an ExecuTorch Android Demo App Lowering a Model as a Delegate Recommendation Systems Introduction to TorchRec Exploring TorchRec sharding Multimodality TorchMultimodal Tutorial: Finetuning FLAVA 2.5.0+cu124 Google Search Classic Search PyTorch Recipes See All Recipes See All Prototype Recipes Introduction to PyTorch Learn the Basics Quickstart Tensors Datasets & DataLoaders Transforms Build the Neural Network Automatic Differentiation with torch.autograd Optimizing Model Parameters Save and Load the Model Introduction to PyTorch - YouTube Series Introduction to PyTorch Introduction to PyTorch Tensors The Fundamentals of Autograd Building Models with PyTorch PyTorch TensorBoard Support Training with PyTorch Model Understanding with Captum Learning PyTorch Deep Learning with PyTorch: A 60 Minute Blitz Learning PyTorch with Examples What is torch.nn really? NLP from Scratch Visualizing Models, Data, and Training with TensorBoard A guide on good usage of non_blocking and pin_memory() in PyTorch Image and Video TorchVision Object Detection Finetuning Tutorial Transfer Learning for Computer Vision Tutorial Adversarial Example Generation DCGAN Tutorial Spatial Transformer Networks Tutorial Optimizing Vision Transformer Model for Deployment Whole Slide Image Classification Using PyTorch and TIAToolbox Audio Audio I/O Audio Resampling Audio Data Augmentation Audio Feature Extractions Audio Feature Augmentation Audio Datasets Speech Recognition with Wav2Vec2 Text-to-speech with Tacotron2 Forced Alignment with Wav2Vec2 Backends Introduction to ONNX Reinforcement Learning Reinforcement Learning (DQN) Tutorial Reinforcement Learning (PPO) with TorchRL Tutorial Train a Mario-playing RL Agent Pendulum: Writing your environment and transforms with TorchRL Deploying PyTorch Models in Production Introduction to ONNX Deploying PyTorch in Python via a REST API with Flask Introduction to TorchScript Loading a TorchScript Model in C++ (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Real Time Inference on Raspberry Pi 4 (30 fps!) Profiling PyTorch Profiling your PyTorch Module Introduction to Holistic Trace Analysis Trace Diff using Holistic Trace Analysis Code Transforms with FX (beta) Building a Convolution/Batch Norm fuser in FX (beta) Building a Simple CPU Performance Profiler with FX Frontend APIs (beta) Channels Last Memory Format in PyTorch Forward-mode Automatic Differentiation (Beta) Jacobians, Hessians, hvp, vhp, and more: composing function transforms Model ensembling Per-sample-gradients Using the PyTorch C++ Frontend Dynamic Parallelism in TorchScript Autograd in C++ Frontend Extending PyTorch PyTorch Custom Operators Python Custom Operators Custom C++ and CUDA Operators Double Backward with Custom Functions Fusing Convolution and Batch Norm using Custom Function Custom C++ and CUDA Extensions Extending TorchScript with Custom C++ Operators Extending TorchScript with Custom C++ Classes Registering a Dispatched Operator in C++ Extending dispatcher for a new backend in C++ Facilitating New Backend Integration by PrivateUse1 Model Optimization Profiling your PyTorch Module PyTorch Profiler With TensorBoard Hyperparameter tuning with Ray Tune Optimizing Vision Transformer Model for Deployment Parametrizations Tutorial Pruning Tutorial (beta) Dynamic Quantization on an LSTM Word Language Model (beta) Dynamic Quantization on BERT (beta) Quantized Transfer Learning for Computer Vision Tutorial (beta) Static Quantization with Eager Mode in PyTorch Grokking PyTorch Intel CPU performance from first principles Grokking PyTorch Intel CPU performance from first principles (Part 2) Getting Started - Accelerate Your Scripts with nvFuser Multi-Objective NAS with Ax Introduction to torch.compile Compiled Autograd: Capturing a larger backward graph for torch.compile Inductor CPU backend debugging and profiling (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA) Knowledge Distillation Tutorial Parallel and Distributed Training Distributed and Parallel Training Tutorials PyTorch Distributed Overview Distributed Data Parallel in PyTorch - Video Tutorials Single-Machine Model Parallel Best Practices Getting Started with Distributed Data Parallel Writing Distributed Applications with PyTorch Getting Started with Fully Sharded Data Parallel(FSDP) Advanced Model Training with Fully Sharded Data Parallel (FSDP) Introduction to Libuv TCPStore Backend Large Scale Transformer model training with Tensor Parallel (TP) Introduction to Distributed Pipeline Parallelism Customize Process Group Backends Using Cpp Extensions Getting Started with Distributed RPC Framework Implementing a Parameter Server Using Distributed RPC Framework Implementing Batch RPC Processing Using Asynchronous Executions Combining Distributed DataParallel with Distributed RPC Framework Distributed Training with Uneven Inputs Using the Join Context Manager Edge with ExecuTorch Exporting to ExecuTorch Tutorial Running an ExecuTorch Model in C++ Tutorial Using the ExecuTorch SDK to Profile a Model Building an ExecuTorch iOS Demo App Building an ExecuTorch Android Demo App Lowering a Model as a Delegate Recommendation Systems Introduction to TorchRec Exploring TorchRec sharding Multimodality TorchMultimodal Tutorial: Finetuning FLAVA 2.5.0+cu124 Google Search Classic Search 2.5.0+cu124 PyTorch Recipes Introduction to PyTorch Learning PyTorch Image and Video Audio Backends Reinforcement Learning Deploying PyTorch Models in Production Profiling PyTorch Code Transforms with FX Frontend APIs Extending PyTorch Model Optimization Parallel and Distributed Training Edge with ExecuTorch Recommendation Systems Multimodality Tutorials > (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Shortcuts advanced/super_resolution_with_onnxruntime Run in Google Colab Colab Download Notebook Notebook View on GitHub GitHub Note Click here to download the full example code (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime¶ Note As of PyTorch 2.1, there are two versions of ONNX Exporter. torch.onnx.dynamo_export is the newest (still in beta) exporter based on the TorchDynamo technology released with PyTorch 2.0. torch.onnx.export is based on TorchScript backend and has been available since PyTorch 1.2.0. In this tutorial, we describe how to convert a model defined in PyTorch into the ONNX format using the TorchScript torch.onnx.export ONNX exporter. The exported model will be executed with ONNX Runtime. ONNX Runtime is a performance-focused engine for ONNX models, which inferences efficiently across multiple platforms and hardware (Windows, Linux, and Mac and on both CPUs and GPUs). ONNX Runtime has proved to considerably increase performance over multiple models as explained here For this tutorial, you will need to install ONNX and ONNX Runtime. You can get binary builds of ONNX and ONNX Runtime with %%bash pip install onnx onnxruntime ONNX Runtime recommends using the latest stable runtime for PyTorch. # Some standard imports import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx Super-resolution is a way of increasing the resolution of images, videos and is widely used in image processing or video editing. For this tutorial, we will use a small super-resolution model. First, let’s create a SuperResolution model in PyTorch. This model uses the efficient sub-pixel convolution layer described in “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” - Shi et al for increasing the resolution of an image by an upscale factor. The model expects the Y component of the YCbCr of an image as an input, and outputs the upscaled Y component in super resolution. The model comes directly from PyTorch’s examples without modification: # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv2.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv3.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) Ordinarily, you would now train this model; however, for this tutorial, we will instead download some pretrained weights. Note that this model was not trained fully for good accuracy and is used here for demonstration purposes only. It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, to turn the model to inference mode. This is required since operators like dropout or batchnorm behave differently in inference and training mode. # Load pretrained model weights model_url = \\'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\\' batch_size = 64 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() Exporting a model in PyTorch works via tracing or scripting. This tutorial will use as an example a model exported by tracing. To export a model, we call the torch.onnx.export() function. This will execute the model, recording a trace of what operators are used to compute the outputs. Because export runs the model, we need to provide an input tensor x. The values in this can be random as long as it is the right type and size. Note that the input size will be fixed in the exported ONNX graph for all the input’s dimensions, unless specified as a dynamic axes. In this example we export the model with an input of batch_size 1, but then specify the first dimension as dynamic in the dynamic_axes parameter in torch.onnx.export(). The exported model will thus accept inputs of size [batch_size, 1, 224, 224] where batch_size can be variable. To learn more details about PyTorch’s export interface, check out the torch.onnx documentation. # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # whether to execute constant folding for optimization input_names = [\\'input\\'], # the model\\'s input names output_names = [\\'output\\'], # the model\\'s output names dynamic_axes={\\'input\\' : {0 : \\'batch_size\\'}, # variable length axes \\'output\\' : {0 : \\'batch_size\\'}}) We also computed torch_out, the output after of the model, which we will use to verify that the model we exported computes the same values when run in ONNX Runtime. But before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX API. First, onnx.load(\"super_resolution.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs. import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) Now let’s compute the output using ONNX Runtime’s Python APIs. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that ONNX Runtime and PyTorch are computing the same value for the network. In order to run the model with ONNX Runtime, we need to create an inference session for the model with the chosen configuration parameters (here we use the default config). Once the session is created, we evaluate the model using the run() API. The output of this call is a list containing the outputs of the model computed by ONNX Runtime. import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"]) def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") We should see that the output of PyTorch and ONNX Runtime runs match numerically with the given precision (rtol=1e-03 and atol=1e-05). As a side-note, if they do not match then there is an issue in the ONNX exporter, so please contact us in that case. Timing Comparison Between Models¶ Since ONNX models optimize for inference speed, running the same data on an ONNX model instead of a native pytorch model should result in an improvement of up to 2x. Improvement is more pronounced with higher batch sizes. import time x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) start = time.time() torch_out = torch_model(x) end = time.time() print(f\"Inference of Pytorch model used {end - start} seconds\") ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} start = time.time() ort_outs = ort_session.run(None, ort_inputs) end = time.time() print(f\"Inference of ONNX model used {end - start} seconds\") Running the model on an image using ONNX Runtime¶ So far we have exported a model from PyTorch and shown how to load it and run it in ONNX Runtime with a dummy tensor as an input. For this tutorial, we will use a famous cat image used widely which looks like below First, let’s load the image, preprocess it using standard PIL python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks. We first resize the image to fit the size of the model’s input (224x224). Then we split the image into its Y, Cb, and Cr components. These components represent a grayscale image (Y), and the blue-difference (Cb) and red-difference (Cr) chroma components. The Y component being more sensitive to the human eye, we are interested in this component which we will be transforming. After extracting the Y component, we convert it to a tensor which will be the input of our model. from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert(\\'YCbCr\\') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) Now, as a next step, let’s take the tensor representing the grayscale resized cat image and run the super-resolution model in ONNX Runtime as explained previously. ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] At this point, the output of the model is a tensor. Now, we’ll process the output of the model to construct back the final output image from the output tensor, and save the image. The post-processing steps have been adopted from PyTorch implementation of super-resolution model here. img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode=\\'L\\') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") # Save resized original image (without super-resolution) img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img) img.save(\"cat_resized.jpg\") Here is the comparison between the two images: Low-resolution image Image after super-resolution ONNX Runtime being a cross platform engine, you can run it across multiple platforms and on both CPUs and GPUs. ONNX Runtime can also be deployed to the cloud for model inferencing using Azure Machine Learning Services. More information here. More information about ONNX Runtime’s performance here. For more information about ONNX Runtime here. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Gallery generated by Sphinx-Gallery Next Previous Rate this Tutorial © Copyright 2024, PyTorch. Built with Sphinx using a theme provided by Read the Docs. (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Timing Comparison Between Models Running the model on an image using ONNX Runtime Tutorials > (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Shortcuts Tutorials > (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Tutorials > (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Shortcuts advanced/super_resolution_with_onnxruntime Run in Google Colab Colab Download Notebook Notebook View on GitHub GitHub Note Click here to download the full example code (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime¶ Note As of PyTorch 2.1, there are two versions of ONNX Exporter. torch.onnx.dynamo_export is the newest (still in beta) exporter based on the TorchDynamo technology released with PyTorch 2.0. torch.onnx.export is based on TorchScript backend and has been available since PyTorch 1.2.0. In this tutorial, we describe how to convert a model defined in PyTorch into the ONNX format using the TorchScript torch.onnx.export ONNX exporter. The exported model will be executed with ONNX Runtime. ONNX Runtime is a performance-focused engine for ONNX models, which inferences efficiently across multiple platforms and hardware (Windows, Linux, and Mac and on both CPUs and GPUs). ONNX Runtime has proved to considerably increase performance over multiple models as explained here For this tutorial, you will need to install ONNX and ONNX Runtime. You can get binary builds of ONNX and ONNX Runtime with %%bash pip install onnx onnxruntime ONNX Runtime recommends using the latest stable runtime for PyTorch. # Some standard imports import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx Super-resolution is a way of increasing the resolution of images, videos and is widely used in image processing or video editing. For this tutorial, we will use a small super-resolution model. First, let’s create a SuperResolution model in PyTorch. This model uses the efficient sub-pixel convolution layer described in “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” - Shi et al for increasing the resolution of an image by an upscale factor. The model expects the Y component of the YCbCr of an image as an input, and outputs the upscaled Y component in super resolution. The model comes directly from PyTorch’s examples without modification: # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv2.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv3.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) Ordinarily, you would now train this model; however, for this tutorial, we will instead download some pretrained weights. Note that this model was not trained fully for good accuracy and is used here for demonstration purposes only. It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, to turn the model to inference mode. This is required since operators like dropout or batchnorm behave differently in inference and training mode. # Load pretrained model weights model_url = \\'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\\' batch_size = 64 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() Exporting a model in PyTorch works via tracing or scripting. This tutorial will use as an example a model exported by tracing. To export a model, we call the torch.onnx.export() function. This will execute the model, recording a trace of what operators are used to compute the outputs. Because export runs the model, we need to provide an input tensor x. The values in this can be random as long as it is the right type and size. Note that the input size will be fixed in the exported ONNX graph for all the input’s dimensions, unless specified as a dynamic axes. In this example we export the model with an input of batch_size 1, but then specify the first dimension as dynamic in the dynamic_axes parameter in torch.onnx.export(). The exported model will thus accept inputs of size [batch_size, 1, 224, 224] where batch_size can be variable. To learn more details about PyTorch’s export interface, check out the torch.onnx documentation. # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # whether to execute constant folding for optimization input_names = [\\'input\\'], # the model\\'s input names output_names = [\\'output\\'], # the model\\'s output names dynamic_axes={\\'input\\' : {0 : \\'batch_size\\'}, # variable length axes \\'output\\' : {0 : \\'batch_size\\'}}) We also computed torch_out, the output after of the model, which we will use to verify that the model we exported computes the same values when run in ONNX Runtime. But before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX API. First, onnx.load(\"super_resolution.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs. import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) Now let’s compute the output using ONNX Runtime’s Python APIs. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that ONNX Runtime and PyTorch are computing the same value for the network. In order to run the model with ONNX Runtime, we need to create an inference session for the model with the chosen configuration parameters (here we use the default config). Once the session is created, we evaluate the model using the run() API. The output of this call is a list containing the outputs of the model computed by ONNX Runtime. import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"]) def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") We should see that the output of PyTorch and ONNX Runtime runs match numerically with the given precision (rtol=1e-03 and atol=1e-05). As a side-note, if they do not match then there is an issue in the ONNX exporter, so please contact us in that case. Timing Comparison Between Models¶ Since ONNX models optimize for inference speed, running the same data on an ONNX model instead of a native pytorch model should result in an improvement of up to 2x. Improvement is more pronounced with higher batch sizes. import time x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) start = time.time() torch_out = torch_model(x) end = time.time() print(f\"Inference of Pytorch model used {end - start} seconds\") ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} start = time.time() ort_outs = ort_session.run(None, ort_inputs) end = time.time() print(f\"Inference of ONNX model used {end - start} seconds\") Running the model on an image using ONNX Runtime¶ So far we have exported a model from PyTorch and shown how to load it and run it in ONNX Runtime with a dummy tensor as an input. For this tutorial, we will use a famous cat image used widely which looks like below First, let’s load the image, preprocess it using standard PIL python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks. We first resize the image to fit the size of the model’s input (224x224). Then we split the image into its Y, Cb, and Cr components. These components represent a grayscale image (Y), and the blue-difference (Cb) and red-difference (Cr) chroma components. The Y component being more sensitive to the human eye, we are interested in this component which we will be transforming. After extracting the Y component, we convert it to a tensor which will be the input of our model. from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert(\\'YCbCr\\') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) Now, as a next step, let’s take the tensor representing the grayscale resized cat image and run the super-resolution model in ONNX Runtime as explained previously. ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] At this point, the output of the model is a tensor. Now, we’ll process the output of the model to construct back the final output image from the output tensor, and save the image. The post-processing steps have been adopted from PyTorch implementation of super-resolution model here. img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode=\\'L\\') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") # Save resized original image (without super-resolution) img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img) img.save(\"cat_resized.jpg\") Here is the comparison between the two images: Low-resolution image Image after super-resolution ONNX Runtime being a cross platform engine, you can run it across multiple platforms and on both CPUs and GPUs. ONNX Runtime can also be deployed to the cloud for model inferencing using Azure Machine Learning Services. More information here. More information about ONNX Runtime’s performance here. For more information about ONNX Runtime here. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Gallery generated by Sphinx-Gallery Next Previous Rate this Tutorial © Copyright 2024, PyTorch. Built with Sphinx using a theme provided by Read the Docs. advanced/super_resolution_with_onnxruntime Run in Google Colab Colab Download Notebook Notebook View on GitHub GitHub advanced/super_resolution_with_onnxruntime Run in Google Colab Colab Run in Google Colab Colab Download Notebook Notebook Download Notebook Notebook View on GitHub GitHub View on GitHub GitHub Note Click here to download the full example code (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime¶ Note As of PyTorch 2.1, there are two versions of ONNX Exporter. torch.onnx.dynamo_export is the newest (still in beta) exporter based on the TorchDynamo technology released with PyTorch 2.0. torch.onnx.export is based on TorchScript backend and has been available since PyTorch 1.2.0. In this tutorial, we describe how to convert a model defined in PyTorch into the ONNX format using the TorchScript torch.onnx.export ONNX exporter. The exported model will be executed with ONNX Runtime. ONNX Runtime is a performance-focused engine for ONNX models, which inferences efficiently across multiple platforms and hardware (Windows, Linux, and Mac and on both CPUs and GPUs). ONNX Runtime has proved to considerably increase performance over multiple models as explained here For this tutorial, you will need to install ONNX and ONNX Runtime. You can get binary builds of ONNX and ONNX Runtime with %%bash pip install onnx onnxruntime ONNX Runtime recommends using the latest stable runtime for PyTorch. # Some standard imports import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx Super-resolution is a way of increasing the resolution of images, videos and is widely used in image processing or video editing. For this tutorial, we will use a small super-resolution model. First, let’s create a SuperResolution model in PyTorch. This model uses the efficient sub-pixel convolution layer described in “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” - Shi et al for increasing the resolution of an image by an upscale factor. The model expects the Y component of the YCbCr of an image as an input, and outputs the upscaled Y component in super resolution. The model comes directly from PyTorch’s examples without modification: # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv2.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv3.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) Ordinarily, you would now train this model; however, for this tutorial, we will instead download some pretrained weights. Note that this model was not trained fully for good accuracy and is used here for demonstration purposes only. It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, to turn the model to inference mode. This is required since operators like dropout or batchnorm behave differently in inference and training mode. # Load pretrained model weights model_url = \\'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\\' batch_size = 64 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() Exporting a model in PyTorch works via tracing or scripting. This tutorial will use as an example a model exported by tracing. To export a model, we call the torch.onnx.export() function. This will execute the model, recording a trace of what operators are used to compute the outputs. Because export runs the model, we need to provide an input tensor x. The values in this can be random as long as it is the right type and size. Note that the input size will be fixed in the exported ONNX graph for all the input’s dimensions, unless specified as a dynamic axes. In this example we export the model with an input of batch_size 1, but then specify the first dimension as dynamic in the dynamic_axes parameter in torch.onnx.export(). The exported model will thus accept inputs of size [batch_size, 1, 224, 224] where batch_size can be variable. To learn more details about PyTorch’s export interface, check out the torch.onnx documentation. # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # whether to execute constant folding for optimization input_names = [\\'input\\'], # the model\\'s input names output_names = [\\'output\\'], # the model\\'s output names dynamic_axes={\\'input\\' : {0 : \\'batch_size\\'}, # variable length axes \\'output\\' : {0 : \\'batch_size\\'}}) We also computed torch_out, the output after of the model, which we will use to verify that the model we exported computes the same values when run in ONNX Runtime. But before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX API. First, onnx.load(\"super_resolution.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs. import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) Now let’s compute the output using ONNX Runtime’s Python APIs. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that ONNX Runtime and PyTorch are computing the same value for the network. In order to run the model with ONNX Runtime, we need to create an inference session for the model with the chosen configuration parameters (here we use the default config). Once the session is created, we evaluate the model using the run() API. The output of this call is a list containing the outputs of the model computed by ONNX Runtime. import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"]) def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") We should see that the output of PyTorch and ONNX Runtime runs match numerically with the given precision (rtol=1e-03 and atol=1e-05). As a side-note, if they do not match then there is an issue in the ONNX exporter, so please contact us in that case. Timing Comparison Between Models¶ Since ONNX models optimize for inference speed, running the same data on an ONNX model instead of a native pytorch model should result in an improvement of up to 2x. Improvement is more pronounced with higher batch sizes. import time x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) start = time.time() torch_out = torch_model(x) end = time.time() print(f\"Inference of Pytorch model used {end - start} seconds\") ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} start = time.time() ort_outs = ort_session.run(None, ort_inputs) end = time.time() print(f\"Inference of ONNX model used {end - start} seconds\") Running the model on an image using ONNX Runtime¶ So far we have exported a model from PyTorch and shown how to load it and run it in ONNX Runtime with a dummy tensor as an input. For this tutorial, we will use a famous cat image used widely which looks like below First, let’s load the image, preprocess it using standard PIL python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks. We first resize the image to fit the size of the model’s input (224x224). Then we split the image into its Y, Cb, and Cr components. These components represent a grayscale image (Y), and the blue-difference (Cb) and red-difference (Cr) chroma components. The Y component being more sensitive to the human eye, we are interested in this component which we will be transforming. After extracting the Y component, we convert it to a tensor which will be the input of our model. from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert(\\'YCbCr\\') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) Now, as a next step, let’s take the tensor representing the grayscale resized cat image and run the super-resolution model in ONNX Runtime as explained previously. ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] At this point, the output of the model is a tensor. Now, we’ll process the output of the model to construct back the final output image from the output tensor, and save the image. The post-processing steps have been adopted from PyTorch implementation of super-resolution model here. img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode=\\'L\\') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") # Save resized original image (without super-resolution) img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img) img.save(\"cat_resized.jpg\") Here is the comparison between the two images: Low-resolution image Image after super-resolution ONNX Runtime being a cross platform engine, you can run it across multiple platforms and on both CPUs and GPUs. ONNX Runtime can also be deployed to the cloud for model inferencing using Azure Machine Learning Services. More information here. More information about ONNX Runtime’s performance here. For more information about ONNX Runtime here. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Gallery generated by Sphinx-Gallery Next Previous Rate this Tutorial © Copyright 2024, PyTorch. Built with Sphinx using a theme provided by Read the Docs. Note Click here to download the full example code (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime¶ Note As of PyTorch 2.1, there are two versions of ONNX Exporter. torch.onnx.dynamo_export is the newest (still in beta) exporter based on the TorchDynamo technology released with PyTorch 2.0. torch.onnx.export is based on TorchScript backend and has been available since PyTorch 1.2.0. In this tutorial, we describe how to convert a model defined in PyTorch into the ONNX format using the TorchScript torch.onnx.export ONNX exporter. The exported model will be executed with ONNX Runtime. ONNX Runtime is a performance-focused engine for ONNX models, which inferences efficiently across multiple platforms and hardware (Windows, Linux, and Mac and on both CPUs and GPUs). ONNX Runtime has proved to considerably increase performance over multiple models as explained here For this tutorial, you will need to install ONNX and ONNX Runtime. You can get binary builds of ONNX and ONNX Runtime with %%bash pip install onnx onnxruntime ONNX Runtime recommends using the latest stable runtime for PyTorch. # Some standard imports import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx Super-resolution is a way of increasing the resolution of images, videos and is widely used in image processing or video editing. For this tutorial, we will use a small super-resolution model. First, let’s create a SuperResolution model in PyTorch. This model uses the efficient sub-pixel convolution layer described in “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” - Shi et al for increasing the resolution of an image by an upscale factor. The model expects the Y component of the YCbCr of an image as an input, and outputs the upscaled Y component in super resolution. The model comes directly from PyTorch’s examples without modification: # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv2.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv3.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) Ordinarily, you would now train this model; however, for this tutorial, we will instead download some pretrained weights. Note that this model was not trained fully for good accuracy and is used here for demonstration purposes only. It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, to turn the model to inference mode. This is required since operators like dropout or batchnorm behave differently in inference and training mode. # Load pretrained model weights model_url = \\'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\\' batch_size = 64 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() Exporting a model in PyTorch works via tracing or scripting. This tutorial will use as an example a model exported by tracing. To export a model, we call the torch.onnx.export() function. This will execute the model, recording a trace of what operators are used to compute the outputs. Because export runs the model, we need to provide an input tensor x. The values in this can be random as long as it is the right type and size. Note that the input size will be fixed in the exported ONNX graph for all the input’s dimensions, unless specified as a dynamic axes. In this example we export the model with an input of batch_size 1, but then specify the first dimension as dynamic in the dynamic_axes parameter in torch.onnx.export(). The exported model will thus accept inputs of size [batch_size, 1, 224, 224] where batch_size can be variable. To learn more details about PyTorch’s export interface, check out the torch.onnx documentation. # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # whether to execute constant folding for optimization input_names = [\\'input\\'], # the model\\'s input names output_names = [\\'output\\'], # the model\\'s output names dynamic_axes={\\'input\\' : {0 : \\'batch_size\\'}, # variable length axes \\'output\\' : {0 : \\'batch_size\\'}}) We also computed torch_out, the output after of the model, which we will use to verify that the model we exported computes the same values when run in ONNX Runtime. But before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX API. First, onnx.load(\"super_resolution.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs. import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) Now let’s compute the output using ONNX Runtime’s Python APIs. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that ONNX Runtime and PyTorch are computing the same value for the network. In order to run the model with ONNX Runtime, we need to create an inference session for the model with the chosen configuration parameters (here we use the default config). Once the session is created, we evaluate the model using the run() API. The output of this call is a list containing the outputs of the model computed by ONNX Runtime. import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"]) def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") We should see that the output of PyTorch and ONNX Runtime runs match numerically with the given precision (rtol=1e-03 and atol=1e-05). As a side-note, if they do not match then there is an issue in the ONNX exporter, so please contact us in that case. Timing Comparison Between Models¶ Since ONNX models optimize for inference speed, running the same data on an ONNX model instead of a native pytorch model should result in an improvement of up to 2x. Improvement is more pronounced with higher batch sizes. import time x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) start = time.time() torch_out = torch_model(x) end = time.time() print(f\"Inference of Pytorch model used {end - start} seconds\") ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} start = time.time() ort_outs = ort_session.run(None, ort_inputs) end = time.time() print(f\"Inference of ONNX model used {end - start} seconds\") Running the model on an image using ONNX Runtime¶ So far we have exported a model from PyTorch and shown how to load it and run it in ONNX Runtime with a dummy tensor as an input. For this tutorial, we will use a famous cat image used widely which looks like below First, let’s load the image, preprocess it using standard PIL python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks. We first resize the image to fit the size of the model’s input (224x224). Then we split the image into its Y, Cb, and Cr components. These components represent a grayscale image (Y), and the blue-difference (Cb) and red-difference (Cr) chroma components. The Y component being more sensitive to the human eye, we are interested in this component which we will be transforming. After extracting the Y component, we convert it to a tensor which will be the input of our model. from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert(\\'YCbCr\\') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) Now, as a next step, let’s take the tensor representing the grayscale resized cat image and run the super-resolution model in ONNX Runtime as explained previously. ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] At this point, the output of the model is a tensor. Now, we’ll process the output of the model to construct back the final output image from the output tensor, and save the image. The post-processing steps have been adopted from PyTorch implementation of super-resolution model here. img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode=\\'L\\') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") # Save resized original image (without super-resolution) img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img) img.save(\"cat_resized.jpg\") Here is the comparison between the two images: Low-resolution image Image after super-resolution ONNX Runtime being a cross platform engine, you can run it across multiple platforms and on both CPUs and GPUs. ONNX Runtime can also be deployed to the cloud for model inferencing using Azure Machine Learning Services. More information here. More information about ONNX Runtime’s performance here. For more information about ONNX Runtime here. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Gallery generated by Sphinx-Gallery Note Click here to download the full example code Note Click here to download the full example code (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime¶ Note As of PyTorch 2.1, there are two versions of ONNX Exporter. torch.onnx.dynamo_export is the newest (still in beta) exporter based on the TorchDynamo technology released with PyTorch 2.0. torch.onnx.export is based on TorchScript backend and has been available since PyTorch 1.2.0. In this tutorial, we describe how to convert a model defined in PyTorch into the ONNX format using the TorchScript torch.onnx.export ONNX exporter. The exported model will be executed with ONNX Runtime. ONNX Runtime is a performance-focused engine for ONNX models, which inferences efficiently across multiple platforms and hardware (Windows, Linux, and Mac and on both CPUs and GPUs). ONNX Runtime has proved to considerably increase performance over multiple models as explained here For this tutorial, you will need to install ONNX and ONNX Runtime. You can get binary builds of ONNX and ONNX Runtime with %%bash pip install onnx onnxruntime ONNX Runtime recommends using the latest stable runtime for PyTorch. # Some standard imports import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx Super-resolution is a way of increasing the resolution of images, videos and is widely used in image processing or video editing. For this tutorial, we will use a small super-resolution model. First, let’s create a SuperResolution model in PyTorch. This model uses the efficient sub-pixel convolution layer described in “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” - Shi et al for increasing the resolution of an image by an upscale factor. The model expects the Y component of the YCbCr of an image as an input, and outputs the upscaled Y component in super resolution. The model comes directly from PyTorch’s examples without modification: # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv2.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv3.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) Ordinarily, you would now train this model; however, for this tutorial, we will instead download some pretrained weights. Note that this model was not trained fully for good accuracy and is used here for demonstration purposes only. It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, to turn the model to inference mode. This is required since operators like dropout or batchnorm behave differently in inference and training mode. # Load pretrained model weights model_url = \\'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\\' batch_size = 64 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() Exporting a model in PyTorch works via tracing or scripting. This tutorial will use as an example a model exported by tracing. To export a model, we call the torch.onnx.export() function. This will execute the model, recording a trace of what operators are used to compute the outputs. Because export runs the model, we need to provide an input tensor x. The values in this can be random as long as it is the right type and size. Note that the input size will be fixed in the exported ONNX graph for all the input’s dimensions, unless specified as a dynamic axes. In this example we export the model with an input of batch_size 1, but then specify the first dimension as dynamic in the dynamic_axes parameter in torch.onnx.export(). The exported model will thus accept inputs of size [batch_size, 1, 224, 224] where batch_size can be variable. To learn more details about PyTorch’s export interface, check out the torch.onnx documentation. # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # whether to execute constant folding for optimization input_names = [\\'input\\'], # the model\\'s input names output_names = [\\'output\\'], # the model\\'s output names dynamic_axes={\\'input\\' : {0 : \\'batch_size\\'}, # variable length axes \\'output\\' : {0 : \\'batch_size\\'}}) We also computed torch_out, the output after of the model, which we will use to verify that the model we exported computes the same values when run in ONNX Runtime. But before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX API. First, onnx.load(\"super_resolution.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs. import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) Now let’s compute the output using ONNX Runtime’s Python APIs. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that ONNX Runtime and PyTorch are computing the same value for the network. In order to run the model with ONNX Runtime, we need to create an inference session for the model with the chosen configuration parameters (here we use the default config). Once the session is created, we evaluate the model using the run() API. The output of this call is a list containing the outputs of the model computed by ONNX Runtime. import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"]) def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") We should see that the output of PyTorch and ONNX Runtime runs match numerically with the given precision (rtol=1e-03 and atol=1e-05). As a side-note, if they do not match then there is an issue in the ONNX exporter, so please contact us in that case. Timing Comparison Between Models¶ Since ONNX models optimize for inference speed, running the same data on an ONNX model instead of a native pytorch model should result in an improvement of up to 2x. Improvement is more pronounced with higher batch sizes. import time x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) start = time.time() torch_out = torch_model(x) end = time.time() print(f\"Inference of Pytorch model used {end - start} seconds\") ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} start = time.time() ort_outs = ort_session.run(None, ort_inputs) end = time.time() print(f\"Inference of ONNX model used {end - start} seconds\") Running the model on an image using ONNX Runtime¶ So far we have exported a model from PyTorch and shown how to load it and run it in ONNX Runtime with a dummy tensor as an input. For this tutorial, we will use a famous cat image used widely which looks like below First, let’s load the image, preprocess it using standard PIL python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks. We first resize the image to fit the size of the model’s input (224x224). Then we split the image into its Y, Cb, and Cr components. These components represent a grayscale image (Y), and the blue-difference (Cb) and red-difference (Cr) chroma components. The Y component being more sensitive to the human eye, we are interested in this component which we will be transforming. After extracting the Y component, we convert it to a tensor which will be the input of our model. from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert(\\'YCbCr\\') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) Now, as a next step, let’s take the tensor representing the grayscale resized cat image and run the super-resolution model in ONNX Runtime as explained previously. ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] At this point, the output of the model is a tensor. Now, we’ll process the output of the model to construct back the final output image from the output tensor, and save the image. The post-processing steps have been adopted from PyTorch implementation of super-resolution model here. img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode=\\'L\\') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") # Save resized original image (without super-resolution) img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img) img.save(\"cat_resized.jpg\") Here is the comparison between the two images: Low-resolution image Image after super-resolution ONNX Runtime being a cross platform engine, you can run it across multiple platforms and on both CPUs and GPUs. ONNX Runtime can also be deployed to the cloud for model inferencing using Azure Machine Learning Services. More information here. More information about ONNX Runtime’s performance here. For more information about ONNX Runtime here. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Gallery generated by Sphinx-Gallery Note As of PyTorch 2.1, there are two versions of ONNX Exporter. torch.onnx.dynamo_export is the newest (still in beta) exporter based on the TorchDynamo technology released with PyTorch 2.0. torch.onnx.export is based on TorchScript backend and has been available since PyTorch 1.2.0. Note As of PyTorch 2.1, there are two versions of ONNX Exporter. torch.onnx.dynamo_export is the newest (still in beta) exporter based on the TorchDynamo technology released with PyTorch 2.0. torch.onnx.export is based on TorchScript backend and has been available since PyTorch 1.2.0. In this tutorial, we describe how to convert a model defined in PyTorch into the ONNX format using the TorchScript torch.onnx.export ONNX exporter. The exported model will be executed with ONNX Runtime. ONNX Runtime is a performance-focused engine for ONNX models, which inferences efficiently across multiple platforms and hardware (Windows, Linux, and Mac and on both CPUs and GPUs). ONNX Runtime has proved to considerably increase performance over multiple models as explained here For this tutorial, you will need to install ONNX and ONNX Runtime. You can get binary builds of ONNX and ONNX Runtime with %%bash pip install onnx onnxruntime %%bash pip install onnx onnxruntime ONNX Runtime recommends using the latest stable runtime for PyTorch. # Some standard imports import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx # Some standard imports import numpy as np from torch import nn import torch.utils.model_zoo as model_zoo import torch.onnx Super-resolution is a way of increasing the resolution of images, videos and is widely used in image processing or video editing. For this tutorial, we will use a small super-resolution model. First, let’s create a SuperResolution model in PyTorch. This model uses the efficient sub-pixel convolution layer described in “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” - Shi et al for increasing the resolution of an image by an upscale factor. The model expects the Y component of the YCbCr of an image as an input, and outputs the upscaled Y component in super resolution. The model comes directly from PyTorch’s examples without modification: # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv2.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv3.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) # Super Resolution model definition in PyTorch import torch.nn as nn import torch.nn.init as init class SuperResolutionNet(nn.Module): def __init__(self, upscale_factor, inplace=False): super(SuperResolutionNet, self).__init__() self.relu = nn.ReLU(inplace=inplace) self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2)) self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1)) self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1)) self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1)) self.pixel_shuffle = nn.PixelShuffle(upscale_factor) self._initialize_weights() def forward(self, x): x = self.relu(self.conv1(x)) x = self.relu(self.conv2(x)) x = self.relu(self.conv3(x)) x = self.pixel_shuffle(self.conv4(x)) return x def _initialize_weights(self): init.orthogonal_(self.conv1.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv2.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv3.weight, init.calculate_gain(\\'relu\\')) init.orthogonal_(self.conv4.weight) # Create the super-resolution model by using the above model definition. torch_model = SuperResolutionNet(upscale_factor=3) Ordinarily, you would now train this model; however, for this tutorial, we will instead download some pretrained weights. Note that this model was not trained fully for good accuracy and is used here for demonstration purposes only. It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, to turn the model to inference mode. This is required since operators like dropout or batchnorm behave differently in inference and training mode. # Load pretrained model weights model_url = \\'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\\' batch_size = 64 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() # Load pretrained model weights model_url = \\'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\\' batch_size = 64 # just a random number # Initialize model with the pretrained weights map_location = lambda storage, loc: storage if torch.cuda.is_available(): map_location = None torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location)) # set the model to inference mode torch_model.eval() Exporting a model in PyTorch works via tracing or scripting. This tutorial will use as an example a model exported by tracing. To export a model, we call the torch.onnx.export() function. This will execute the model, recording a trace of what operators are used to compute the outputs. Because export runs the model, we need to provide an input tensor x. The values in this can be random as long as it is the right type and size. Note that the input size will be fixed in the exported ONNX graph for all the input’s dimensions, unless specified as a dynamic axes. In this example we export the model with an input of batch_size 1, but then specify the first dimension as dynamic in the dynamic_axes parameter in torch.onnx.export(). The exported model will thus accept inputs of size [batch_size, 1, 224, 224] where batch_size can be variable. To learn more details about PyTorch’s export interface, check out the torch.onnx documentation. # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # whether to execute constant folding for optimization input_names = [\\'input\\'], # the model\\'s input names output_names = [\\'output\\'], # the model\\'s output names dynamic_axes={\\'input\\' : {0 : \\'batch_size\\'}, # variable length axes \\'output\\' : {0 : \\'batch_size\\'}}) # Input to the model x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) torch_out = torch_model(x) # Export the model torch.onnx.export(torch_model, # model being run x, # model input (or a tuple for multiple inputs) \"super_resolution.onnx\", # where to save the model (can be a file or file-like object) export_params=True, # store the trained parameter weights inside the model file opset_version=10, # the ONNX version to export the model to do_constant_folding=True, # whether to execute constant folding for optimization input_names = [\\'input\\'], # the model\\'s input names output_names = [\\'output\\'], # the model\\'s output names dynamic_axes={\\'input\\' : {0 : \\'batch_size\\'}, # variable length axes \\'output\\' : {0 : \\'batch_size\\'}}) We also computed torch_out, the output after of the model, which we will use to verify that the model we exported computes the same values when run in ONNX Runtime. But before verifying the model’s output with ONNX Runtime, we will check the ONNX model with ONNX API. First, onnx.load(\"super_resolution.onnx\") will load the saved model and will output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model. For more information onnx.proto documentation.). Then, onnx.checker.check_model(onnx_model) will verify the model’s structure and confirm that the model has a valid schema. The validity of the ONNX graph is verified by checking the model’s version, the graph’s structure, as well as the nodes and their inputs and outputs. import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) import onnx onnx_model = onnx.load(\"super_resolution.onnx\") onnx.checker.check_model(onnx_model) Now let’s compute the output using ONNX Runtime’s Python APIs. This part can normally be done in a separate process or on another machine, but we will continue in the same process so that we can verify that ONNX Runtime and PyTorch are computing the same value for the network. In order to run the model with ONNX Runtime, we need to create an inference session for the model with the chosen configuration parameters (here we use the default config). Once the session is created, we evaluate the model using the run() API. The output of this call is a list containing the outputs of the model computed by ONNX Runtime. import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"]) def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") import onnxruntime ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"]) def to_numpy(tensor): return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy() # compute ONNX Runtime output prediction ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} ort_outs = ort_session.run(None, ort_inputs) # compare ONNX Runtime and PyTorch results np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05) print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\") We should see that the output of PyTorch and ONNX Runtime runs match numerically with the given precision (rtol=1e-03 and atol=1e-05). As a side-note, if they do not match then there is an issue in the ONNX exporter, so please contact us in that case. Timing Comparison Between Models¶ Since ONNX models optimize for inference speed, running the same data on an ONNX model instead of a native pytorch model should result in an improvement of up to 2x. Improvement is more pronounced with higher batch sizes. import time x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) start = time.time() torch_out = torch_model(x) end = time.time() print(f\"Inference of Pytorch model used {end - start} seconds\") ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} start = time.time() ort_outs = ort_session.run(None, ort_inputs) end = time.time() print(f\"Inference of ONNX model used {end - start} seconds\") Since ONNX models optimize for inference speed, running the same data on an ONNX model instead of a native pytorch model should result in an improvement of up to 2x. Improvement is more pronounced with higher batch sizes. import time x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) start = time.time() torch_out = torch_model(x) end = time.time() print(f\"Inference of Pytorch model used {end - start} seconds\") ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} start = time.time() ort_outs = ort_session.run(None, ort_inputs) end = time.time() print(f\"Inference of ONNX model used {end - start} seconds\") import time x = torch.randn(batch_size, 1, 224, 224, requires_grad=True) start = time.time() torch_out = torch_model(x) end = time.time() print(f\"Inference of Pytorch model used {end - start} seconds\") ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)} start = time.time() ort_outs = ort_session.run(None, ort_inputs) end = time.time() print(f\"Inference of ONNX model used {end - start} seconds\") Running the model on an image using ONNX Runtime¶ So far we have exported a model from PyTorch and shown how to load it and run it in ONNX Runtime with a dummy tensor as an input. For this tutorial, we will use a famous cat image used widely which looks like below First, let’s load the image, preprocess it using standard PIL python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks. We first resize the image to fit the size of the model’s input (224x224). Then we split the image into its Y, Cb, and Cr components. These components represent a grayscale image (Y), and the blue-difference (Cb) and red-difference (Cr) chroma components. The Y component being more sensitive to the human eye, we are interested in this component which we will be transforming. After extracting the Y component, we convert it to a tensor which will be the input of our model. from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert(\\'YCbCr\\') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) Now, as a next step, let’s take the tensor representing the grayscale resized cat image and run the super-resolution model in ONNX Runtime as explained previously. ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] At this point, the output of the model is a tensor. Now, we’ll process the output of the model to construct back the final output image from the output tensor, and save the image. The post-processing steps have been adopted from PyTorch implementation of super-resolution model here. img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode=\\'L\\') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") # Save resized original image (without super-resolution) img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img) img.save(\"cat_resized.jpg\") Here is the comparison between the two images: Low-resolution image Image after super-resolution ONNX Runtime being a cross platform engine, you can run it across multiple platforms and on both CPUs and GPUs. ONNX Runtime can also be deployed to the cloud for model inferencing using Azure Machine Learning Services. More information here. More information about ONNX Runtime’s performance here. For more information about ONNX Runtime here. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Gallery generated by Sphinx-Gallery So far we have exported a model from PyTorch and shown how to load it and run it in ONNX Runtime with a dummy tensor as an input. For this tutorial, we will use a famous cat image used widely which looks like below First, let’s load the image, preprocess it using standard PIL python library. Note that this preprocessing is the standard practice of processing data for training/testing neural networks. We first resize the image to fit the size of the model’s input (224x224). Then we split the image into its Y, Cb, and Cr components. These components represent a grayscale image (Y), and the blue-difference (Cb) and red-difference (Cr) chroma components. The Y component being more sensitive to the human eye, we are interested in this component which we will be transforming. After extracting the Y component, we convert it to a tensor which will be the input of our model. from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert(\\'YCbCr\\') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) from PIL import Image import torchvision.transforms as transforms img = Image.open(\"./_static/img/cat.jpg\") resize = transforms.Resize([224, 224]) img = resize(img) img_ycbcr = img.convert(\\'YCbCr\\') img_y, img_cb, img_cr = img_ycbcr.split() to_tensor = transforms.ToTensor() img_y = to_tensor(img_y) img_y.unsqueeze_(0) Now, as a next step, let’s take the tensor representing the grayscale resized cat image and run the super-resolution model in ONNX Runtime as explained previously. ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)} ort_outs = ort_session.run(None, ort_inputs) img_out_y = ort_outs[0] At this point, the output of the model is a tensor. Now, we’ll process the output of the model to construct back the final output image from the output tensor, and save the image. The post-processing steps have been adopted from PyTorch implementation of super-resolution model here. img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode=\\'L\\') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") # Save resized original image (without super-resolution) img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img) img.save(\"cat_resized.jpg\") img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode=\\'L\\') # get the output image follow post-processing step from PyTorch implementation final_img = Image.merge( \"YCbCr\", [ img_out_y, img_cb.resize(img_out_y.size, Image.BICUBIC), img_cr.resize(img_out_y.size, Image.BICUBIC), ]).convert(\"RGB\") # Save the image, we will compare this with the output image from mobile device final_img.save(\"./_static/img/cat_superres_with_ort.jpg\") # Save resized original image (without super-resolution) img = transforms.Resize([img_out_y.size[0], img_out_y.size[1]])(img) img.save(\"cat_resized.jpg\") Here is the comparison between the two images: Low-resolution image Image after super-resolution ONNX Runtime being a cross platform engine, you can run it across multiple platforms and on both CPUs and GPUs. ONNX Runtime can also be deployed to the cloud for model inferencing using Azure Machine Learning Services. More information here. More information about ONNX Runtime’s performance here. For more information about ONNX Runtime here. Total running time of the script: ( 0 minutes 0.000 seconds) Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Download Python source code: super_resolution_with_onnxruntime.py Download Python source code: super_resolution_with_onnxruntime.py Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Download Jupyter notebook: super_resolution_with_onnxruntime.ipynb Gallery generated by Sphinx-Gallery Next Previous Rate this Tutorial Rate this Tutorial © Copyright 2024, PyTorch. © Copyright 2024, PyTorch. Built with Sphinx using a theme provided by Read the Docs. (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Timing Comparison Between Models Running the model on an image using ONNX Runtime (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Timing Comparison Between Models Running the model on an image using ONNX Runtime (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Timing Comparison Between Models Running the model on an image using ONNX Runtime Docs Access comprehensive developer documentation for PyTorch View Docs Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials Resources Find development resources and get your questions answered View Resources Docs Access comprehensive developer documentation for PyTorch View Docs Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials Resources Find development resources and get your questions answered View Resources Docs Access comprehensive developer documentation for PyTorch View Docs Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials Resources Find development resources and get your questions answered View Resources Docs Access comprehensive developer documentation for PyTorch View Docs Access comprehensive developer documentation for PyTorch Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials Get in-depth tutorials for beginners and advanced developers Resources Find development resources and get your questions answered View Resources Find development resources and get your questions answered PyTorch Get Started Features Ecosystem Blog Contributing Resources Tutorials Docs Discuss Github Issues Brand Guidelines Stay up to date Facebook Twitter YouTube LinkedIn PyTorch Podcasts Spotify Apple Google Amazon Terms | Privacy © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/. PyTorch Get Started Features Ecosystem Blog Contributing Resources Tutorials Docs Discuss Github Issues Brand Guidelines Stay up to date Facebook Twitter YouTube LinkedIn PyTorch Podcasts Spotify Apple Google Amazon PyTorch Get Started Features Ecosystem Blog Contributing Resources Tutorials Docs Discuss Github Issues Brand Guidelines Stay up to date Facebook Twitter YouTube LinkedIn PyTorch Podcasts Spotify Apple Google Amazon Terms | Privacy © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/. © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/. To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy. To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy. To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy. Learn Get Started Tutorials Learn the Basics PyTorch Recipes Introduction to PyTorch - YouTube Series Ecosystem Tools Community Forums Developer Resources Contributor Awards - 2023 Edge About PyTorch Edge ExecuTorch Docs PyTorch PyTorch Domains Blog & News PyTorch Blog Community Blog Videos Community Stories Events About PyTorch Foundation Governing Board Learn Get Started Tutorials Learn the Basics PyTorch Recipes Introduction to PyTorch - YouTube Series Ecosystem Tools Community Forums Developer Resources Contributor Awards - 2023 Edge About PyTorch Edge ExecuTorch Docs PyTorch PyTorch Domains Blog & News PyTorch Blog Community Blog Videos Community Stories Events About PyTorch Foundation Governing Board Learn Get Started Tutorials Learn the Basics PyTorch Recipes Introduction to PyTorch - YouTube Series Ecosystem Tools Community Forums Developer Resources Contributor Awards - 2023 Edge About PyTorch Edge ExecuTorch Docs PyTorch PyTorch Domains Blog & News PyTorch Blog Community Blog Videos Community Stories Events About PyTorch Foundation Governing Board',\n",
       " 'https://github.com/onnx/tutorials/blob/master/tutorials/OnnxRuntimeServerSSDModel.ipynb': 'Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert onnx / tutorials Public Notifications You must be signed in to change notification settings Fork 631 Star 3.4k Code Issues 110 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus Sign in Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less GitHub Copilot Write better code with AI GitHub Copilot Security Find and fix vulnerabilities Security Actions Automate any workflow Actions Codespaces Instant dev environments Codespaces Issues Plan and track work Issues Code Review Manage code changes Code Review Discussions Collaborate outside of code Discussions Code Search Find more, search less Code Search Explore All features Documentation GitHub Skills Blog Explore All features Documentation GitHub Skills Blog By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Topics AI DevOps Security Software Development View all Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections GitHub Sponsors Fund open source developers GitHub Sponsors Fund open source developers GitHub Sponsors The ReadME Project GitHub community articles The ReadME Project GitHub community articles The ReadME Project Repositories Topics Trending Collections Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Enterprise platform AI-powered developer platform Enterprise platform AI-powered developer platform Enterprise platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Advanced Security Enterprise-grade security features Advanced Security GitHub Copilot Enterprise-grade AI features GitHub Copilot Premium Support Enterprise-grade 24/7 support Premium Support Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Search syntax tips Search Clear Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Provide feedback Provide feedback Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted We read every piece of feedback, and take your input very seriously. Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Saved searches Use saved searches to filter your results more quickly Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Name Query To see all available qualifiers, see our documentation. Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert onnx / tutorials Public Notifications You must be signed in to change notification settings Fork 631 Star 3.4k Code Issues 110 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights onnx / tutorials Public Notifications You must be signed in to change notification settings Fork 631 Star 3.4k Code Issues 110 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights onnx / tutorials Public Notifications You must be signed in to change notification settings Fork 631 Star 3.4k Code Issues 110 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights onnx / tutorials Public Notifications You must be signed in to change notification settings Fork 631 Star 3.4k onnx / tutorials Public onnx / tutorials Public Notifications You must be signed in to change notification settings Fork 631 Star 3.4k Star 3.4k Additional navigation options Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights Code Issues Pull requests Actions Projects Security Insights © 2024 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information © 2024 GitHub, Inc. You can’t perform that action at this time.',\n",
       " 'https://storage.googleapis.com/openimages/web/index.html': 'Open Images Dataset V7 and Extensions 15,851,536 boxes on 600 classes 2,785,498 instance segmentations on 350 classes 3,284,280 relationship annotations on 1,466 relationships 675,155 localized narratives 66,391,027 point-level annotations on 5,827 classes 61,404,966 image-level labels on 20,638 classes Extension - 478,000 crowdsourced images with 6,000+ classes Explore Description Download Extended News Extras Challenge Open Images Dataset V7 and Extensions 15,851,536 boxes on 600 classes 2,785,498 instance segmentations on 350 classes 3,284,280 relationship annotations on 1,466 relationships 675,155 localized narratives 66,391,027 point-level annotations on 5,827 classes 61,404,966 image-level labels on 20,638 classes Extension - 478,000 crowdsourced images with 6,000+ classes Explore Description Download Extended News Extras Challenge 15,851,536 boxes on 600 classes 2,785,498 instance segmentations on 350 classes 3,284,280 relationship annotations on 1,466 relationships 675,155 localized narratives 66,391,027 point-level annotations on 5,827 classes 61,404,966 image-level labels on 20,638 classes Extension - 478,000 crowdsourced images with 6,000+ classes Explore Description Download Explore Description Download Extended News Extras Challenge Extended News Extras Challenge',\n",
       " 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/': 'The goal of this challenge is to recognize objects from a number of visual object classes in realistic scenes (i.e. not pre-segmented objects). It is fundamentally a supervised learning learning problem in that a training set of labelled images is provided. The twenty object classes that have been selected are: There will be two main competitions, and two smaller scale \"taster\" competitions. Participants may enter either (or both) of these competitions, and can choose to tackle any (or all) of the twenty object classes. The challenge allows for two approaches to each of the competitions: The intention in the first case is to establish just what level of success can currently be achieved on these problems and by what method; in the second case the intention is to establish which method is most successful given a specified training set. Participants may enter either (or both) of these competitions. The VOC2007 challenge has been organized following the successful VOC2006 and VOC2005 challenges. Compared to VOC2006 we have increased the number of classes from 10 to 20, and added the taster challenges. These tasters have been introduced to sample the interest in segmentation and layout. The training data provided consists of a set of images; each image has an annotation file giving a bounding box and object class label for each object in one of the twenty classes present in the image. Note that multiple objects from multiple classes may be present in the same image. Some example images can be viewed online. Annotation was performed according to a set of guidelines distributed to all annotators. These guidelines can be viewed here. The data will be made available in two stages; in the first stage, a development kit will be released consisting of training and validation data, plus evaluation software (written in MATLAB). One purpose of the validation set is to demonstrate how the evaluation software works ahead of the competition submission. In the second stage, the test set will be made available for the actual competition. As in the VOC2006 challenge, no ground truth for the test data will be released until after the challenge is complete. The data has been split into 50% for training/validation and 50% for testing. The distributions of images and objects by class are approximately equal across the training/validation and test sets. In total there are 9,963 images, containing 24,640 annotated objects. Further statistics can be found here. Example images and the corresponding annotation for the main classification/detection tasks, segmentation and layout tasters can be viewed online: The VOC2007 data includes some images provided by \"flickr\". Use of these images must respect the corresponding terms of use: For the purposes of the challenge, the identity of the images in the database, e.g. source and name of owner, has been obscured. Details of the contributor of each image can be found in the annotation to be included in the final release of the data, after completion of the challenge. Any queries about the use or ownership of the data should be addressed to the organizers. The development kit provided for the VOC challenge 2007 is available. You can: The updated development kit made available 11-Jun-2007 contains two changes: It should be possible to untar the updated development kit over the previous version with no adverse effects. The annotated test data for the VOC challenge 2007 is now available: This is a direct replacement for that provided for the challenge but additionally includes full annotation of each test image, and segmentation ground truth for the segmentation taster images. The annotated test data additionally contains information about the owner of each image as provided by flickr. An updated version of the training/validation data also containing ownership information is available in the development kit. Detailed results of all submitted methods are now online. For summarized results and information about some of the best-performing methods, please see the workshop presentations: If you make use of the VOC2007 data, please cite the following reference in any publications: Participants are expected to submit a single set of results per method employed. Participants who have investigated several algorithms may submit one result per method. Changes in algorithm parameters do not constitute a different method - all parameter tuning must be conducted using the training and validation data alone. Details of the required file formats for submitted results can be found in the development kit documentation. The results files should be collected in a single archive file (tar/zip) and placed on an FTP/HTTP server accessible from outside your institution. Email the URL and any details needed to access the file to Mark Everingham, me@comp.leeds.ac.uk. Please do not send large files (>1MB) directly by email. Participants submitting results for several different methods (noting the definition of different methods above) may either collect results for each method into a single archive, providing separate directories for each method and an appropriate key to the results, or may submit several archive files. In addition to the results files, participants should provide contact details, a list of contributors and a brief description of the method used, see below. This information may be sent by email or included in the results archive file. For participants using the provided development kit, all results are stored in the results/ directory. An archive suitable for submission can be generated using e.g.: Participants not making use of the development kit must follow the specification for contents and naming of results files given in the development kit. Example files in the correct format may be generated by running the example implementations in the development kit. If at all possible, participants are requested to submit results for both the VOC2007 and VOC2006 test sets provided in the test data, to allow comparison of results across the years. The updated development kit provides a switch to select between test sets. Results are placed in two directories, results/VOC2006/ or results/VOC2007/ according to the test set. The main mechanism for dissemination of the results will be the challenge webpage. Further details will be made available at a later date. We gratefully acknowledge the following, who spent many long hours providing annotation for the VOC2007 database: Moray Allan, Patrick Buehler, Terry Herbert, Anitha Kannan, Julia Lasserre, Alain Lehmann, Mukta Prasad, Till Quack, John Quinn, Florian Schroff. We are also grateful to James Philbin, Ondra Chum, and Felix Agakov for additional assistance. The preparation and running of this challenge is supported by the EU-funded PASCAL Network of Excellence on Pattern Analysis, Statistical Modelling and Computational Learning.',\n",
       " 'https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html': 'Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Become a Member Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Become a Member Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Become a Member Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Become a Member Learn Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Get Started Run PyTorch locally or get started quickly with one of the supported cloud platforms Tutorials Whats new in PyTorch tutorials Learn the Basics Familiarize yourself with PyTorch concepts and modules PyTorch Recipes Bite-size, ready-to-deploy PyTorch code examples Intro to PyTorch - YouTube Series Master PyTorch basics with our engaging YouTube tutorial series Run PyTorch locally or get started quickly with one of the supported cloud platforms Whats new in PyTorch tutorials Familiarize yourself with PyTorch concepts and modules Bite-size, ready-to-deploy PyTorch code examples Master PyTorch basics with our engaging YouTube tutorial series Ecosystem Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Tools Learn about the tools and frameworks in the PyTorch Ecosystem Community Join the PyTorch developer community to contribute, learn, and get your questions answered Forums A place to discuss PyTorch code, issues, install, research Developer Resources Find resources and get questions answered Contributor Awards - 2023 Award winners announced at this year\\'s PyTorch Conference Learn about the tools and frameworks in the PyTorch Ecosystem Join the PyTorch developer community to contribute, learn, and get your questions answered A place to discuss PyTorch code, issues, install, research Find resources and get questions answered Award winners announced at this year\\'s PyTorch Conference Edge About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices About PyTorch Edge Build innovative and privacy-aware AI experiences for edge devices ExecuTorch End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Build innovative and privacy-aware AI experiences for edge devices End-to-end solution for enabling on-device inference capabilities across mobile and edge devices Docs PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries PyTorch Explore the documentation for comprehensive guidance on how to use PyTorch PyTorch Domains Read the PyTorch Domains documentation to learn more about domain-specific libraries Explore the documentation for comprehensive guidance on how to use PyTorch Read the PyTorch Domains documentation to learn more about domain-specific libraries Blogs & News PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts PyTorch Blog Catch up on the latest technical news and happenings Community Blog Stories from the PyTorch ecosystem Videos Learn about the latest PyTorch tutorials, new, and more Community Stories Learn how our community solves real, everyday machine learning problems with PyTorch Events Find events, webinars, and podcasts Catch up on the latest technical news and happenings Stories from the PyTorch ecosystem Learn about the latest PyTorch tutorials, new, and more Learn how our community solves real, everyday machine learning problems with PyTorch Find events, webinars, and podcasts About PyTorch Foundation Learn more about the PyTorch Foundation Governing Board PyTorch Foundation Learn more about the PyTorch Foundation Governing Board Learn more about the PyTorch Foundation Become a Member Table of Contents 2.5.0+cu124 Google Search Classic Search PyTorch Recipes See All Recipes See All Prototype Recipes Introduction to PyTorch Learn the Basics Quickstart Tensors Datasets & DataLoaders Transforms Build the Neural Network Automatic Differentiation with torch.autograd Optimizing Model Parameters Save and Load the Model Introduction to PyTorch - YouTube Series Introduction to PyTorch Introduction to PyTorch Tensors The Fundamentals of Autograd Building Models with PyTorch PyTorch TensorBoard Support Training with PyTorch Model Understanding with Captum Learning PyTorch Deep Learning with PyTorch: A 60 Minute Blitz Learning PyTorch with Examples What is torch.nn really? NLP from Scratch Visualizing Models, Data, and Training with TensorBoard A guide on good usage of non_blocking and pin_memory() in PyTorch Image and Video TorchVision Object Detection Finetuning Tutorial Transfer Learning for Computer Vision Tutorial Adversarial Example Generation DCGAN Tutorial Spatial Transformer Networks Tutorial Optimizing Vision Transformer Model for Deployment Whole Slide Image Classification Using PyTorch and TIAToolbox Audio Audio I/O Audio Resampling Audio Data Augmentation Audio Feature Extractions Audio Feature Augmentation Audio Datasets Speech Recognition with Wav2Vec2 Text-to-speech with Tacotron2 Forced Alignment with Wav2Vec2 Backends Introduction to ONNX Reinforcement Learning Reinforcement Learning (DQN) Tutorial Reinforcement Learning (PPO) with TorchRL Tutorial Train a Mario-playing RL Agent Pendulum: Writing your environment and transforms with TorchRL Deploying PyTorch Models in Production Introduction to ONNX Deploying PyTorch in Python via a REST API with Flask Introduction to TorchScript Loading a TorchScript Model in C++ (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Real Time Inference on Raspberry Pi 4 (30 fps!) Profiling PyTorch Profiling your PyTorch Module Introduction to Holistic Trace Analysis Trace Diff using Holistic Trace Analysis Code Transforms with FX (beta) Building a Convolution/Batch Norm fuser in FX (beta) Building a Simple CPU Performance Profiler with FX Frontend APIs (beta) Channels Last Memory Format in PyTorch Forward-mode Automatic Differentiation (Beta) Jacobians, Hessians, hvp, vhp, and more: composing function transforms Model ensembling Per-sample-gradients Using the PyTorch C++ Frontend Dynamic Parallelism in TorchScript Autograd in C++ Frontend Extending PyTorch PyTorch Custom Operators Python Custom Operators Custom C++ and CUDA Operators Double Backward with Custom Functions Fusing Convolution and Batch Norm using Custom Function Custom C++ and CUDA Extensions Extending TorchScript with Custom C++ Operators Extending TorchScript with Custom C++ Classes Registering a Dispatched Operator in C++ Extending dispatcher for a new backend in C++ Facilitating New Backend Integration by PrivateUse1 Model Optimization Profiling your PyTorch Module PyTorch Profiler With TensorBoard Hyperparameter tuning with Ray Tune Optimizing Vision Transformer Model for Deployment Parametrizations Tutorial Pruning Tutorial (beta) Dynamic Quantization on an LSTM Word Language Model (beta) Dynamic Quantization on BERT (beta) Quantized Transfer Learning for Computer Vision Tutorial (beta) Static Quantization with Eager Mode in PyTorch Grokking PyTorch Intel CPU performance from first principles Grokking PyTorch Intel CPU performance from first principles (Part 2) Getting Started - Accelerate Your Scripts with nvFuser Multi-Objective NAS with Ax Introduction to torch.compile Compiled Autograd: Capturing a larger backward graph for torch.compile Inductor CPU backend debugging and profiling (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA) Knowledge Distillation Tutorial Parallel and Distributed Training Distributed and Parallel Training Tutorials PyTorch Distributed Overview Distributed Data Parallel in PyTorch - Video Tutorials Single-Machine Model Parallel Best Practices Getting Started with Distributed Data Parallel Writing Distributed Applications with PyTorch Getting Started with Fully Sharded Data Parallel(FSDP) Advanced Model Training with Fully Sharded Data Parallel (FSDP) Introduction to Libuv TCPStore Backend Large Scale Transformer model training with Tensor Parallel (TP) Introduction to Distributed Pipeline Parallelism Customize Process Group Backends Using Cpp Extensions Getting Started with Distributed RPC Framework Implementing a Parameter Server Using Distributed RPC Framework Implementing Batch RPC Processing Using Asynchronous Executions Combining Distributed DataParallel with Distributed RPC Framework Distributed Training with Uneven Inputs Using the Join Context Manager Edge with ExecuTorch Exporting to ExecuTorch Tutorial Running an ExecuTorch Model in C++ Tutorial Using the ExecuTorch SDK to Profile a Model Building an ExecuTorch iOS Demo App Building an ExecuTorch Android Demo App Lowering a Model as a Delegate Recommendation Systems Introduction to TorchRec Exploring TorchRec sharding Multimodality TorchMultimodal Tutorial: Finetuning FLAVA 2.5.0+cu124 Google Search Classic Search PyTorch Recipes See All Recipes See All Prototype Recipes Introduction to PyTorch Learn the Basics Quickstart Tensors Datasets & DataLoaders Transforms Build the Neural Network Automatic Differentiation with torch.autograd Optimizing Model Parameters Save and Load the Model Introduction to PyTorch - YouTube Series Introduction to PyTorch Introduction to PyTorch Tensors The Fundamentals of Autograd Building Models with PyTorch PyTorch TensorBoard Support Training with PyTorch Model Understanding with Captum Learning PyTorch Deep Learning with PyTorch: A 60 Minute Blitz Learning PyTorch with Examples What is torch.nn really? NLP from Scratch Visualizing Models, Data, and Training with TensorBoard A guide on good usage of non_blocking and pin_memory() in PyTorch Image and Video TorchVision Object Detection Finetuning Tutorial Transfer Learning for Computer Vision Tutorial Adversarial Example Generation DCGAN Tutorial Spatial Transformer Networks Tutorial Optimizing Vision Transformer Model for Deployment Whole Slide Image Classification Using PyTorch and TIAToolbox Audio Audio I/O Audio Resampling Audio Data Augmentation Audio Feature Extractions Audio Feature Augmentation Audio Datasets Speech Recognition with Wav2Vec2 Text-to-speech with Tacotron2 Forced Alignment with Wav2Vec2 Backends Introduction to ONNX Reinforcement Learning Reinforcement Learning (DQN) Tutorial Reinforcement Learning (PPO) with TorchRL Tutorial Train a Mario-playing RL Agent Pendulum: Writing your environment and transforms with TorchRL Deploying PyTorch Models in Production Introduction to ONNX Deploying PyTorch in Python via a REST API with Flask Introduction to TorchScript Loading a TorchScript Model in C++ (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime Real Time Inference on Raspberry Pi 4 (30 fps!) Profiling PyTorch Profiling your PyTorch Module Introduction to Holistic Trace Analysis Trace Diff using Holistic Trace Analysis Code Transforms with FX (beta) Building a Convolution/Batch Norm fuser in FX (beta) Building a Simple CPU Performance Profiler with FX Frontend APIs (beta) Channels Last Memory Format in PyTorch Forward-mode Automatic Differentiation (Beta) Jacobians, Hessians, hvp, vhp, and more: composing function transforms Model ensembling Per-sample-gradients Using the PyTorch C++ Frontend Dynamic Parallelism in TorchScript Autograd in C++ Frontend Extending PyTorch PyTorch Custom Operators Python Custom Operators Custom C++ and CUDA Operators Double Backward with Custom Functions Fusing Convolution and Batch Norm using Custom Function Custom C++ and CUDA Extensions Extending TorchScript with Custom C++ Operators Extending TorchScript with Custom C++ Classes Registering a Dispatched Operator in C++ Extending dispatcher for a new backend in C++ Facilitating New Backend Integration by PrivateUse1 Model Optimization Profiling your PyTorch Module PyTorch Profiler With TensorBoard Hyperparameter tuning with Ray Tune Optimizing Vision Transformer Model for Deployment Parametrizations Tutorial Pruning Tutorial (beta) Dynamic Quantization on an LSTM Word Language Model (beta) Dynamic Quantization on BERT (beta) Quantized Transfer Learning for Computer Vision Tutorial (beta) Static Quantization with Eager Mode in PyTorch Grokking PyTorch Intel CPU performance from first principles Grokking PyTorch Intel CPU performance from first principles (Part 2) Getting Started - Accelerate Your Scripts with nvFuser Multi-Objective NAS with Ax Introduction to torch.compile Compiled Autograd: Capturing a larger backward graph for torch.compile Inductor CPU backend debugging and profiling (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA) Knowledge Distillation Tutorial Parallel and Distributed Training Distributed and Parallel Training Tutorials PyTorch Distributed Overview Distributed Data Parallel in PyTorch - Video Tutorials Single-Machine Model Parallel Best Practices Getting Started with Distributed Data Parallel Writing Distributed Applications with PyTorch Getting Started with Fully Sharded Data Parallel(FSDP) Advanced Model Training with Fully Sharded Data Parallel (FSDP) Introduction to Libuv TCPStore Backend Large Scale Transformer model training with Tensor Parallel (TP) Introduction to Distributed Pipeline Parallelism Customize Process Group Backends Using Cpp Extensions Getting Started with Distributed RPC Framework Implementing a Parameter Server Using Distributed RPC Framework Implementing Batch RPC Processing Using Asynchronous Executions Combining Distributed DataParallel with Distributed RPC Framework Distributed Training with Uneven Inputs Using the Join Context Manager Edge with ExecuTorch Exporting to ExecuTorch Tutorial Running an ExecuTorch Model in C++ Tutorial Using the ExecuTorch SDK to Profile a Model Building an ExecuTorch iOS Demo App Building an ExecuTorch Android Demo App Lowering a Model as a Delegate Recommendation Systems Introduction to TorchRec Exploring TorchRec sharding Multimodality TorchMultimodal Tutorial: Finetuning FLAVA 2.5.0+cu124 Google Search Classic Search 2.5.0+cu124 PyTorch Recipes Introduction to PyTorch Learning PyTorch Image and Video Audio Backends Reinforcement Learning Deploying PyTorch Models in Production Profiling PyTorch Code Transforms with FX Frontend APIs Extending PyTorch Model Optimization Parallel and Distributed Training Edge with ExecuTorch Recommendation Systems Multimodality Tutorials > Transfer Learning for Computer Vision Tutorial Shortcuts beginner/transfer_learning_tutorial Run in Google Colab Colab Download Notebook Notebook View on GitHub GitHub Note Click here to download the full example code Transfer Learning for Computer Vision Tutorial¶ Author: Sasank Chilamkurthy In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes Quoting these notes, In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. These two major transfer learning scenarios look as follows: Finetuning the ConvNet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual. ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. # License: BSD # Author: Sasank Chilamkurthy import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import torch.backends.cudnn as cudnn import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from PIL import Image from tempfile import TemporaryDirectory cudnn.benchmark = True plt.ion() # interactive mode <contextlib.ExitStack object at 0x7faaf691e3b0> Load Data¶ We will use torchvision and torch.utils.data packages for loading the data. The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well. This dataset is a very small subset of imagenet. Note Download the data from here and extract it to the current directory. # Data augmentation and normalization for training # Just normalization for validation data_transforms = { \\'train\\': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \\'val\\': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = \\'data/hymenoptera_data\\' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\\'train\\', \\'val\\']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [\\'train\\', \\'val\\']} dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']} class_names = image_datasets[\\'train\\'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Visualize a few images¶ Let’s visualize a few training images so as to understand the data augmentations. def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) Training the model¶ Now, let’s write a general function to train a model. Here, we will illustrate: Scheduling the learning rate Saving the best model In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() # Create a temporary directory to save training checkpoints with TemporaryDirectory() as tempdir: best_model_params_path = os.path.join(tempdir, \\'best_model_params.pt\\') torch.save(model.state_dict(), best_model_params_path) best_acc = 0.0 for epoch in range(num_epochs): print(f\\'Epoch {epoch}/{num_epochs - 1}\\') print(\\'-\\' * 10) # Each epoch has a training and validation phase for phase in [\\'train\\', \\'val\\']: if phase == \\'train\\': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \\'train\\'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == \\'train\\': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == \\'train\\': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\') # deep copy the model if phase == \\'val\\' and epoch_acc > best_acc: best_acc = epoch_acc torch.save(model.state_dict(), best_model_params_path) print() time_elapsed = time.time() - since print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\') print(f\\'Best val Acc: {best_acc:4f}\\') # load best model weights model.load_state_dict(torch.load(best_model_params_path, weights_only=True)) return model Visualizing the model predictions¶ Generic function to display predictions for a few images def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Finetuning the ConvNet¶ Load a pretrained model and reset final fully connected layer. model_ft = models.resnet18(weights=\\'IMAGENET1K_V1\\') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``. model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 0%| | 0.00/44.7M [00:00<?, ?B/s] 47%|####7 | 21.1M/44.7M [00:00<00:00, 221MB/s] 96%|#########5| 42.8M/44.7M [00:00<00:00, 224MB/s] 100%|##########| 44.7M/44.7M [00:00<00:00, 224MB/s] Train and evaluate¶ It should take around 15-25 min on CPU. On GPU though, it takes less than a minute. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 visualize_model(model_ft) ConvNet as fixed feature extractor¶ Here, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward(). You can read more about this in the documentation here. model_conv = torchvision.models.resnet18(weights=\\'IMAGENET1K_V1\\') for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) Train and evaluate¶ On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 visualize_model(model_conv) plt.ioff() plt.show() Inference on custom images¶ Use the trained model to make predictions on custom images and visualize the predicted class labels along with the images. def visualize_model_predictions(model,img_path): was_training = model.training model.eval() img = Image.open(img_path) img = data_transforms[\\'val\\'](img) img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): outputs = model(img) _, preds = torch.max(outputs, 1) ax = plt.subplot(2,2,1) ax.axis(\\'off\\') ax.set_title(f\\'Predicted: {class_names[preds[0]]}\\') imshow(img.cpu().data[0]) model.train(mode=was_training) visualize_model_predictions( model_conv, img_path=\\'data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg\\' ) plt.ioff() plt.show() Further Learning¶ If you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial. Total running time of the script: ( 1 minutes 38.407 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery Next Previous Rate this Tutorial © Copyright 2024, PyTorch. Built with Sphinx using a theme provided by Read the Docs. Transfer Learning for Computer Vision Tutorial Load Data Visualize a few images Training the model Visualizing the model predictions Finetuning the ConvNet Train and evaluate ConvNet as fixed feature extractor Train and evaluate Inference on custom images Further Learning Tutorials > Transfer Learning for Computer Vision Tutorial Shortcuts Tutorials > Transfer Learning for Computer Vision Tutorial Tutorials > Transfer Learning for Computer Vision Tutorial Shortcuts beginner/transfer_learning_tutorial Run in Google Colab Colab Download Notebook Notebook View on GitHub GitHub Note Click here to download the full example code Transfer Learning for Computer Vision Tutorial¶ Author: Sasank Chilamkurthy In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes Quoting these notes, In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. These two major transfer learning scenarios look as follows: Finetuning the ConvNet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual. ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. # License: BSD # Author: Sasank Chilamkurthy import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import torch.backends.cudnn as cudnn import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from PIL import Image from tempfile import TemporaryDirectory cudnn.benchmark = True plt.ion() # interactive mode <contextlib.ExitStack object at 0x7faaf691e3b0> Load Data¶ We will use torchvision and torch.utils.data packages for loading the data. The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well. This dataset is a very small subset of imagenet. Note Download the data from here and extract it to the current directory. # Data augmentation and normalization for training # Just normalization for validation data_transforms = { \\'train\\': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \\'val\\': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = \\'data/hymenoptera_data\\' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\\'train\\', \\'val\\']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [\\'train\\', \\'val\\']} dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']} class_names = image_datasets[\\'train\\'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Visualize a few images¶ Let’s visualize a few training images so as to understand the data augmentations. def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) Training the model¶ Now, let’s write a general function to train a model. Here, we will illustrate: Scheduling the learning rate Saving the best model In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() # Create a temporary directory to save training checkpoints with TemporaryDirectory() as tempdir: best_model_params_path = os.path.join(tempdir, \\'best_model_params.pt\\') torch.save(model.state_dict(), best_model_params_path) best_acc = 0.0 for epoch in range(num_epochs): print(f\\'Epoch {epoch}/{num_epochs - 1}\\') print(\\'-\\' * 10) # Each epoch has a training and validation phase for phase in [\\'train\\', \\'val\\']: if phase == \\'train\\': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \\'train\\'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == \\'train\\': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == \\'train\\': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\') # deep copy the model if phase == \\'val\\' and epoch_acc > best_acc: best_acc = epoch_acc torch.save(model.state_dict(), best_model_params_path) print() time_elapsed = time.time() - since print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\') print(f\\'Best val Acc: {best_acc:4f}\\') # load best model weights model.load_state_dict(torch.load(best_model_params_path, weights_only=True)) return model Visualizing the model predictions¶ Generic function to display predictions for a few images def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Finetuning the ConvNet¶ Load a pretrained model and reset final fully connected layer. model_ft = models.resnet18(weights=\\'IMAGENET1K_V1\\') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``. model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 0%| | 0.00/44.7M [00:00<?, ?B/s] 47%|####7 | 21.1M/44.7M [00:00<00:00, 221MB/s] 96%|#########5| 42.8M/44.7M [00:00<00:00, 224MB/s] 100%|##########| 44.7M/44.7M [00:00<00:00, 224MB/s] Train and evaluate¶ It should take around 15-25 min on CPU. On GPU though, it takes less than a minute. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 visualize_model(model_ft) ConvNet as fixed feature extractor¶ Here, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward(). You can read more about this in the documentation here. model_conv = torchvision.models.resnet18(weights=\\'IMAGENET1K_V1\\') for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) Train and evaluate¶ On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 visualize_model(model_conv) plt.ioff() plt.show() Inference on custom images¶ Use the trained model to make predictions on custom images and visualize the predicted class labels along with the images. def visualize_model_predictions(model,img_path): was_training = model.training model.eval() img = Image.open(img_path) img = data_transforms[\\'val\\'](img) img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): outputs = model(img) _, preds = torch.max(outputs, 1) ax = plt.subplot(2,2,1) ax.axis(\\'off\\') ax.set_title(f\\'Predicted: {class_names[preds[0]]}\\') imshow(img.cpu().data[0]) model.train(mode=was_training) visualize_model_predictions( model_conv, img_path=\\'data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg\\' ) plt.ioff() plt.show() Further Learning¶ If you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial. Total running time of the script: ( 1 minutes 38.407 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery Next Previous Rate this Tutorial © Copyright 2024, PyTorch. Built with Sphinx using a theme provided by Read the Docs. beginner/transfer_learning_tutorial Run in Google Colab Colab Download Notebook Notebook View on GitHub GitHub beginner/transfer_learning_tutorial Run in Google Colab Colab Run in Google Colab Colab Download Notebook Notebook Download Notebook Notebook View on GitHub GitHub View on GitHub GitHub Note Click here to download the full example code Transfer Learning for Computer Vision Tutorial¶ Author: Sasank Chilamkurthy In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes Quoting these notes, In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. These two major transfer learning scenarios look as follows: Finetuning the ConvNet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual. ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. # License: BSD # Author: Sasank Chilamkurthy import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import torch.backends.cudnn as cudnn import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from PIL import Image from tempfile import TemporaryDirectory cudnn.benchmark = True plt.ion() # interactive mode <contextlib.ExitStack object at 0x7faaf691e3b0> Load Data¶ We will use torchvision and torch.utils.data packages for loading the data. The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well. This dataset is a very small subset of imagenet. Note Download the data from here and extract it to the current directory. # Data augmentation and normalization for training # Just normalization for validation data_transforms = { \\'train\\': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \\'val\\': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = \\'data/hymenoptera_data\\' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\\'train\\', \\'val\\']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [\\'train\\', \\'val\\']} dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']} class_names = image_datasets[\\'train\\'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Visualize a few images¶ Let’s visualize a few training images so as to understand the data augmentations. def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) Training the model¶ Now, let’s write a general function to train a model. Here, we will illustrate: Scheduling the learning rate Saving the best model In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() # Create a temporary directory to save training checkpoints with TemporaryDirectory() as tempdir: best_model_params_path = os.path.join(tempdir, \\'best_model_params.pt\\') torch.save(model.state_dict(), best_model_params_path) best_acc = 0.0 for epoch in range(num_epochs): print(f\\'Epoch {epoch}/{num_epochs - 1}\\') print(\\'-\\' * 10) # Each epoch has a training and validation phase for phase in [\\'train\\', \\'val\\']: if phase == \\'train\\': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \\'train\\'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == \\'train\\': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == \\'train\\': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\') # deep copy the model if phase == \\'val\\' and epoch_acc > best_acc: best_acc = epoch_acc torch.save(model.state_dict(), best_model_params_path) print() time_elapsed = time.time() - since print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\') print(f\\'Best val Acc: {best_acc:4f}\\') # load best model weights model.load_state_dict(torch.load(best_model_params_path, weights_only=True)) return model Visualizing the model predictions¶ Generic function to display predictions for a few images def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Finetuning the ConvNet¶ Load a pretrained model and reset final fully connected layer. model_ft = models.resnet18(weights=\\'IMAGENET1K_V1\\') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``. model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 0%| | 0.00/44.7M [00:00<?, ?B/s] 47%|####7 | 21.1M/44.7M [00:00<00:00, 221MB/s] 96%|#########5| 42.8M/44.7M [00:00<00:00, 224MB/s] 100%|##########| 44.7M/44.7M [00:00<00:00, 224MB/s] Train and evaluate¶ It should take around 15-25 min on CPU. On GPU though, it takes less than a minute. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 visualize_model(model_ft) ConvNet as fixed feature extractor¶ Here, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward(). You can read more about this in the documentation here. model_conv = torchvision.models.resnet18(weights=\\'IMAGENET1K_V1\\') for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) Train and evaluate¶ On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 visualize_model(model_conv) plt.ioff() plt.show() Inference on custom images¶ Use the trained model to make predictions on custom images and visualize the predicted class labels along with the images. def visualize_model_predictions(model,img_path): was_training = model.training model.eval() img = Image.open(img_path) img = data_transforms[\\'val\\'](img) img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): outputs = model(img) _, preds = torch.max(outputs, 1) ax = plt.subplot(2,2,1) ax.axis(\\'off\\') ax.set_title(f\\'Predicted: {class_names[preds[0]]}\\') imshow(img.cpu().data[0]) model.train(mode=was_training) visualize_model_predictions( model_conv, img_path=\\'data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg\\' ) plt.ioff() plt.show() Further Learning¶ If you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial. Total running time of the script: ( 1 minutes 38.407 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery Next Previous Rate this Tutorial © Copyright 2024, PyTorch. Built with Sphinx using a theme provided by Read the Docs. Note Click here to download the full example code Transfer Learning for Computer Vision Tutorial¶ Author: Sasank Chilamkurthy In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes Quoting these notes, In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. These two major transfer learning scenarios look as follows: Finetuning the ConvNet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual. ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. # License: BSD # Author: Sasank Chilamkurthy import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import torch.backends.cudnn as cudnn import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from PIL import Image from tempfile import TemporaryDirectory cudnn.benchmark = True plt.ion() # interactive mode <contextlib.ExitStack object at 0x7faaf691e3b0> Load Data¶ We will use torchvision and torch.utils.data packages for loading the data. The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well. This dataset is a very small subset of imagenet. Note Download the data from here and extract it to the current directory. # Data augmentation and normalization for training # Just normalization for validation data_transforms = { \\'train\\': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \\'val\\': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = \\'data/hymenoptera_data\\' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\\'train\\', \\'val\\']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [\\'train\\', \\'val\\']} dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']} class_names = image_datasets[\\'train\\'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Visualize a few images¶ Let’s visualize a few training images so as to understand the data augmentations. def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) Training the model¶ Now, let’s write a general function to train a model. Here, we will illustrate: Scheduling the learning rate Saving the best model In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() # Create a temporary directory to save training checkpoints with TemporaryDirectory() as tempdir: best_model_params_path = os.path.join(tempdir, \\'best_model_params.pt\\') torch.save(model.state_dict(), best_model_params_path) best_acc = 0.0 for epoch in range(num_epochs): print(f\\'Epoch {epoch}/{num_epochs - 1}\\') print(\\'-\\' * 10) # Each epoch has a training and validation phase for phase in [\\'train\\', \\'val\\']: if phase == \\'train\\': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \\'train\\'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == \\'train\\': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == \\'train\\': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\') # deep copy the model if phase == \\'val\\' and epoch_acc > best_acc: best_acc = epoch_acc torch.save(model.state_dict(), best_model_params_path) print() time_elapsed = time.time() - since print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\') print(f\\'Best val Acc: {best_acc:4f}\\') # load best model weights model.load_state_dict(torch.load(best_model_params_path, weights_only=True)) return model Visualizing the model predictions¶ Generic function to display predictions for a few images def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Finetuning the ConvNet¶ Load a pretrained model and reset final fully connected layer. model_ft = models.resnet18(weights=\\'IMAGENET1K_V1\\') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``. model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 0%| | 0.00/44.7M [00:00<?, ?B/s] 47%|####7 | 21.1M/44.7M [00:00<00:00, 221MB/s] 96%|#########5| 42.8M/44.7M [00:00<00:00, 224MB/s] 100%|##########| 44.7M/44.7M [00:00<00:00, 224MB/s] Train and evaluate¶ It should take around 15-25 min on CPU. On GPU though, it takes less than a minute. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 visualize_model(model_ft) ConvNet as fixed feature extractor¶ Here, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward(). You can read more about this in the documentation here. model_conv = torchvision.models.resnet18(weights=\\'IMAGENET1K_V1\\') for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) Train and evaluate¶ On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 visualize_model(model_conv) plt.ioff() plt.show() Inference on custom images¶ Use the trained model to make predictions on custom images and visualize the predicted class labels along with the images. def visualize_model_predictions(model,img_path): was_training = model.training model.eval() img = Image.open(img_path) img = data_transforms[\\'val\\'](img) img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): outputs = model(img) _, preds = torch.max(outputs, 1) ax = plt.subplot(2,2,1) ax.axis(\\'off\\') ax.set_title(f\\'Predicted: {class_names[preds[0]]}\\') imshow(img.cpu().data[0]) model.train(mode=was_training) visualize_model_predictions( model_conv, img_path=\\'data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg\\' ) plt.ioff() plt.show() Further Learning¶ If you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial. Total running time of the script: ( 1 minutes 38.407 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery Note Click here to download the full example code Note Click here to download the full example code Transfer Learning for Computer Vision Tutorial¶ Author: Sasank Chilamkurthy In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes Quoting these notes, In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. These two major transfer learning scenarios look as follows: Finetuning the ConvNet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual. ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. # License: BSD # Author: Sasank Chilamkurthy import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import torch.backends.cudnn as cudnn import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from PIL import Image from tempfile import TemporaryDirectory cudnn.benchmark = True plt.ion() # interactive mode <contextlib.ExitStack object at 0x7faaf691e3b0> Load Data¶ We will use torchvision and torch.utils.data packages for loading the data. The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well. This dataset is a very small subset of imagenet. Note Download the data from here and extract it to the current directory. # Data augmentation and normalization for training # Just normalization for validation data_transforms = { \\'train\\': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \\'val\\': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = \\'data/hymenoptera_data\\' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\\'train\\', \\'val\\']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [\\'train\\', \\'val\\']} dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']} class_names = image_datasets[\\'train\\'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Visualize a few images¶ Let’s visualize a few training images so as to understand the data augmentations. def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) Training the model¶ Now, let’s write a general function to train a model. Here, we will illustrate: Scheduling the learning rate Saving the best model In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() # Create a temporary directory to save training checkpoints with TemporaryDirectory() as tempdir: best_model_params_path = os.path.join(tempdir, \\'best_model_params.pt\\') torch.save(model.state_dict(), best_model_params_path) best_acc = 0.0 for epoch in range(num_epochs): print(f\\'Epoch {epoch}/{num_epochs - 1}\\') print(\\'-\\' * 10) # Each epoch has a training and validation phase for phase in [\\'train\\', \\'val\\']: if phase == \\'train\\': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \\'train\\'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == \\'train\\': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == \\'train\\': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\') # deep copy the model if phase == \\'val\\' and epoch_acc > best_acc: best_acc = epoch_acc torch.save(model.state_dict(), best_model_params_path) print() time_elapsed = time.time() - since print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\') print(f\\'Best val Acc: {best_acc:4f}\\') # load best model weights model.load_state_dict(torch.load(best_model_params_path, weights_only=True)) return model Visualizing the model predictions¶ Generic function to display predictions for a few images def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Finetuning the ConvNet¶ Load a pretrained model and reset final fully connected layer. model_ft = models.resnet18(weights=\\'IMAGENET1K_V1\\') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``. model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 0%| | 0.00/44.7M [00:00<?, ?B/s] 47%|####7 | 21.1M/44.7M [00:00<00:00, 221MB/s] 96%|#########5| 42.8M/44.7M [00:00<00:00, 224MB/s] 100%|##########| 44.7M/44.7M [00:00<00:00, 224MB/s] Train and evaluate¶ It should take around 15-25 min on CPU. On GPU though, it takes less than a minute. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 visualize_model(model_ft) ConvNet as fixed feature extractor¶ Here, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward(). You can read more about this in the documentation here. model_conv = torchvision.models.resnet18(weights=\\'IMAGENET1K_V1\\') for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) Train and evaluate¶ On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 visualize_model(model_conv) plt.ioff() plt.show() Inference on custom images¶ Use the trained model to make predictions on custom images and visualize the predicted class labels along with the images. def visualize_model_predictions(model,img_path): was_training = model.training model.eval() img = Image.open(img_path) img = data_transforms[\\'val\\'](img) img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): outputs = model(img) _, preds = torch.max(outputs, 1) ax = plt.subplot(2,2,1) ax.axis(\\'off\\') ax.set_title(f\\'Predicted: {class_names[preds[0]]}\\') imshow(img.cpu().data[0]) model.train(mode=was_training) visualize_model_predictions( model_conv, img_path=\\'data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg\\' ) plt.ioff() plt.show() Further Learning¶ If you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial. Total running time of the script: ( 1 minutes 38.407 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery Author: Sasank Chilamkurthy In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes Quoting these notes, In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. These two major transfer learning scenarios look as follows: Finetuning the ConvNet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual. ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. # License: BSD # Author: Sasank Chilamkurthy import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import torch.backends.cudnn as cudnn import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from PIL import Image from tempfile import TemporaryDirectory cudnn.benchmark = True plt.ion() # interactive mode # License: BSD # Author: Sasank Chilamkurthy import torch import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler import torch.backends.cudnn as cudnn import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os from PIL import Image from tempfile import TemporaryDirectory cudnn.benchmark = True plt.ion() # interactive mode <contextlib.ExitStack object at 0x7faaf691e3b0> <contextlib.ExitStack object at 0x7faaf691e3b0> Load Data¶ We will use torchvision and torch.utils.data packages for loading the data. The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well. This dataset is a very small subset of imagenet. Note Download the data from here and extract it to the current directory. # Data augmentation and normalization for training # Just normalization for validation data_transforms = { \\'train\\': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \\'val\\': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = \\'data/hymenoptera_data\\' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\\'train\\', \\'val\\']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [\\'train\\', \\'val\\']} dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']} class_names = image_datasets[\\'train\\'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Visualize a few images¶ Let’s visualize a few training images so as to understand the data augmentations. def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) We will use torchvision and torch.utils.data packages for loading the data. The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well. This dataset is a very small subset of imagenet. Note Download the data from here and extract it to the current directory. Note Download the data from here and extract it to the current directory. # Data augmentation and normalization for training # Just normalization for validation data_transforms = { \\'train\\': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \\'val\\': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = \\'data/hymenoptera_data\\' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\\'train\\', \\'val\\']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [\\'train\\', \\'val\\']} dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']} class_names = image_datasets[\\'train\\'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Data augmentation and normalization for training # Just normalization for validation data_transforms = { \\'train\\': transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \\'val\\': transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } data_dir = \\'data/hymenoptera_data\\' image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\\'train\\', \\'val\\']} dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in [\\'train\\', \\'val\\']} dataset_sizes = {x: len(image_datasets[x]) for x in [\\'train\\', \\'val\\']} class_names = image_datasets[\\'train\\'].classes device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Visualize a few images¶ Let’s visualize a few training images so as to understand the data augmentations. def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) Let’s visualize a few training images so as to understand the data augmentations. def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) def imshow(inp, title=None): \"\"\"Display image for Tensor.\"\"\" inp = inp.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) inp = std * inp + mean inp = np.clip(inp, 0, 1) plt.imshow(inp) if title is not None: plt.title(title) plt.pause(0.001) # pause a bit so that plots are updated # Get a batch of training data inputs, classes = next(iter(dataloaders[\\'train\\'])) # Make a grid from batch out = torchvision.utils.make_grid(inputs) imshow(out, title=[class_names[x] for x in classes]) Training the model¶ Now, let’s write a general function to train a model. Here, we will illustrate: Scheduling the learning rate Saving the best model In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() # Create a temporary directory to save training checkpoints with TemporaryDirectory() as tempdir: best_model_params_path = os.path.join(tempdir, \\'best_model_params.pt\\') torch.save(model.state_dict(), best_model_params_path) best_acc = 0.0 for epoch in range(num_epochs): print(f\\'Epoch {epoch}/{num_epochs - 1}\\') print(\\'-\\' * 10) # Each epoch has a training and validation phase for phase in [\\'train\\', \\'val\\']: if phase == \\'train\\': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \\'train\\'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == \\'train\\': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == \\'train\\': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\') # deep copy the model if phase == \\'val\\' and epoch_acc > best_acc: best_acc = epoch_acc torch.save(model.state_dict(), best_model_params_path) print() time_elapsed = time.time() - since print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\') print(f\\'Best val Acc: {best_acc:4f}\\') # load best model weights model.load_state_dict(torch.load(best_model_params_path, weights_only=True)) return model Visualizing the model predictions¶ Generic function to display predictions for a few images def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Now, let’s write a general function to train a model. Here, we will illustrate: Scheduling the learning rate Saving the best model In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler. def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() # Create a temporary directory to save training checkpoints with TemporaryDirectory() as tempdir: best_model_params_path = os.path.join(tempdir, \\'best_model_params.pt\\') torch.save(model.state_dict(), best_model_params_path) best_acc = 0.0 for epoch in range(num_epochs): print(f\\'Epoch {epoch}/{num_epochs - 1}\\') print(\\'-\\' * 10) # Each epoch has a training and validation phase for phase in [\\'train\\', \\'val\\']: if phase == \\'train\\': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \\'train\\'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == \\'train\\': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == \\'train\\': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\') # deep copy the model if phase == \\'val\\' and epoch_acc > best_acc: best_acc = epoch_acc torch.save(model.state_dict(), best_model_params_path) print() time_elapsed = time.time() - since print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\') print(f\\'Best val Acc: {best_acc:4f}\\') # load best model weights model.load_state_dict(torch.load(best_model_params_path, weights_only=True)) return model def train_model(model, criterion, optimizer, scheduler, num_epochs=25): since = time.time() # Create a temporary directory to save training checkpoints with TemporaryDirectory() as tempdir: best_model_params_path = os.path.join(tempdir, \\'best_model_params.pt\\') torch.save(model.state_dict(), best_model_params_path) best_acc = 0.0 for epoch in range(num_epochs): print(f\\'Epoch {epoch}/{num_epochs - 1}\\') print(\\'-\\' * 10) # Each epoch has a training and validation phase for phase in [\\'train\\', \\'val\\']: if phase == \\'train\\': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == \\'train\\'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == \\'train\\': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) if phase == \\'train\\': scheduler.step() epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(f\\'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\\') # deep copy the model if phase == \\'val\\' and epoch_acc > best_acc: best_acc = epoch_acc torch.save(model.state_dict(), best_model_params_path) print() time_elapsed = time.time() - since print(f\\'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\') print(f\\'Best val Acc: {best_acc:4f}\\') # load best model weights model.load_state_dict(torch.load(best_model_params_path, weights_only=True)) return model Visualizing the model predictions¶ Generic function to display predictions for a few images def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Generic function to display predictions for a few images def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[\\'val\\']): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(\\'off\\') ax.set_title(f\\'predicted: {class_names[preds[j]]}\\') imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) Finetuning the ConvNet¶ Load a pretrained model and reset final fully connected layer. model_ft = models.resnet18(weights=\\'IMAGENET1K_V1\\') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``. model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 0%| | 0.00/44.7M [00:00<?, ?B/s] 47%|####7 | 21.1M/44.7M [00:00<00:00, 221MB/s] 96%|#########5| 42.8M/44.7M [00:00<00:00, 224MB/s] 100%|##########| 44.7M/44.7M [00:00<00:00, 224MB/s] Train and evaluate¶ It should take around 15-25 min on CPU. On GPU though, it takes less than a minute. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 visualize_model(model_ft) Load a pretrained model and reset final fully connected layer. model_ft = models.resnet18(weights=\\'IMAGENET1K_V1\\') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``. model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) model_ft = models.resnet18(weights=\\'IMAGENET1K_V1\\') num_ftrs = model_ft.fc.in_features # Here the size of each output sample is set to 2. # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``. model_ft.fc = nn.Linear(num_ftrs, 2) model_ft = model_ft.to(device) criterion = nn.CrossEntropyLoss() # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 0%| | 0.00/44.7M [00:00<?, ?B/s] 47%|####7 | 21.1M/44.7M [00:00<00:00, 221MB/s] 96%|#########5| 42.8M/44.7M [00:00<00:00, 224MB/s] 100%|##########| 44.7M/44.7M [00:00<00:00, 224MB/s] Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/ci-user/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth 0%| | 0.00/44.7M [00:00<?, ?B/s] 47%|####7 | 21.1M/44.7M [00:00<00:00, 221MB/s] 96%|#########5| 42.8M/44.7M [00:00<00:00, 224MB/s] 100%|##########| 44.7M/44.7M [00:00<00:00, 224MB/s] Train and evaluate¶ It should take around 15-25 min on CPU. On GPU though, it takes less than a minute. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 visualize_model(model_ft) It should take around 15-25 min on CPU. On GPU though, it takes less than a minute. model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 Epoch 0/24 ---------- train Loss: 0.4757 Acc: 0.7623 val Loss: 0.2719 Acc: 0.8758 Epoch 1/24 ---------- train Loss: 0.5315 Acc: 0.7869 val Loss: 0.5678 Acc: 0.7647 Epoch 2/24 ---------- train Loss: 0.4251 Acc: 0.8033 val Loss: 0.3129 Acc: 0.8889 Epoch 3/24 ---------- train Loss: 0.6262 Acc: 0.7869 val Loss: 0.2926 Acc: 0.8954 Epoch 4/24 ---------- train Loss: 0.4113 Acc: 0.8484 val Loss: 0.2341 Acc: 0.9216 Epoch 5/24 ---------- train Loss: 0.4836 Acc: 0.8074 val Loss: 0.2703 Acc: 0.8889 Epoch 6/24 ---------- train Loss: 0.3662 Acc: 0.8238 val Loss: 0.3537 Acc: 0.8824 Epoch 7/24 ---------- train Loss: 0.4137 Acc: 0.8402 val Loss: 0.2295 Acc: 0.9085 Epoch 8/24 ---------- train Loss: 0.1979 Acc: 0.9221 val Loss: 0.2477 Acc: 0.9150 Epoch 9/24 ---------- train Loss: 0.2625 Acc: 0.8811 val Loss: 0.2756 Acc: 0.9085 Epoch 10/24 ---------- train Loss: 0.3540 Acc: 0.8648 val Loss: 0.2108 Acc: 0.9216 Epoch 11/24 ---------- train Loss: 0.3224 Acc: 0.8525 val Loss: 0.3152 Acc: 0.8824 Epoch 12/24 ---------- train Loss: 0.2424 Acc: 0.8852 val Loss: 0.2391 Acc: 0.9085 Epoch 13/24 ---------- train Loss: 0.2911 Acc: 0.8770 val Loss: 0.2330 Acc: 0.9281 Epoch 14/24 ---------- train Loss: 0.2713 Acc: 0.8893 val Loss: 0.2848 Acc: 0.9085 Epoch 15/24 ---------- train Loss: 0.2999 Acc: 0.8648 val Loss: 0.3251 Acc: 0.8562 Epoch 16/24 ---------- train Loss: 0.2112 Acc: 0.9262 val Loss: 0.2511 Acc: 0.9216 Epoch 17/24 ---------- train Loss: 0.2361 Acc: 0.9057 val Loss: 0.2212 Acc: 0.9216 Epoch 18/24 ---------- train Loss: 0.2940 Acc: 0.8934 val Loss: 0.2707 Acc: 0.8954 Epoch 19/24 ---------- train Loss: 0.2108 Acc: 0.9180 val Loss: 0.2285 Acc: 0.9281 Epoch 20/24 ---------- train Loss: 0.2652 Acc: 0.8730 val Loss: 0.2505 Acc: 0.9150 Epoch 21/24 ---------- train Loss: 0.2379 Acc: 0.8893 val Loss: 0.2887 Acc: 0.8954 Epoch 22/24 ---------- train Loss: 0.2940 Acc: 0.8811 val Loss: 0.2246 Acc: 0.9346 Epoch 23/24 ---------- train Loss: 0.2568 Acc: 0.8770 val Loss: 0.2623 Acc: 0.9020 Epoch 24/24 ---------- train Loss: 0.2975 Acc: 0.8730 val Loss: 0.2266 Acc: 0.9216 Training complete in 1m 4s Best val Acc: 0.934641 visualize_model(model_ft) visualize_model(model_ft) ConvNet as fixed feature extractor¶ Here, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward(). You can read more about this in the documentation here. model_conv = torchvision.models.resnet18(weights=\\'IMAGENET1K_V1\\') for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) Train and evaluate¶ On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 visualize_model(model_conv) plt.ioff() plt.show() Here, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward(). You can read more about this in the documentation here. model_conv = torchvision.models.resnet18(weights=\\'IMAGENET1K_V1\\') for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) model_conv = torchvision.models.resnet18(weights=\\'IMAGENET1K_V1\\') for param in model_conv.parameters(): param.requires_grad = False # Parameters of newly constructed modules have requires_grad=True by default num_ftrs = model_conv.fc.in_features model_conv.fc = nn.Linear(num_ftrs, 2) model_conv = model_conv.to(device) criterion = nn.CrossEntropyLoss() # Observe that only parameters of final layer are being optimized as # opposed to before. optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1) Train and evaluate¶ On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 visualize_model(model_conv) plt.ioff() plt.show() On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed. model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25) Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 Epoch 0/24 ---------- train Loss: 0.6996 Acc: 0.6516 val Loss: 0.2014 Acc: 0.9346 Epoch 1/24 ---------- train Loss: 0.4233 Acc: 0.8033 val Loss: 0.2656 Acc: 0.8758 Epoch 2/24 ---------- train Loss: 0.4603 Acc: 0.7869 val Loss: 0.1847 Acc: 0.9477 Epoch 3/24 ---------- train Loss: 0.3096 Acc: 0.8566 val Loss: 0.1747 Acc: 0.9477 Epoch 4/24 ---------- train Loss: 0.4427 Acc: 0.8156 val Loss: 0.1630 Acc: 0.9477 Epoch 5/24 ---------- train Loss: 0.5505 Acc: 0.7828 val Loss: 0.1643 Acc: 0.9477 Epoch 6/24 ---------- train Loss: 0.3004 Acc: 0.8607 val Loss: 0.1744 Acc: 0.9542 Epoch 7/24 ---------- train Loss: 0.4083 Acc: 0.8361 val Loss: 0.1892 Acc: 0.9412 Epoch 8/24 ---------- train Loss: 0.4483 Acc: 0.7910 val Loss: 0.1984 Acc: 0.9477 Epoch 9/24 ---------- train Loss: 0.3335 Acc: 0.8279 val Loss: 0.1942 Acc: 0.9412 Epoch 10/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.2001 Acc: 0.9477 Epoch 11/24 ---------- train Loss: 0.3107 Acc: 0.8689 val Loss: 0.1801 Acc: 0.9412 Epoch 12/24 ---------- train Loss: 0.3032 Acc: 0.8689 val Loss: 0.1669 Acc: 0.9477 Epoch 13/24 ---------- train Loss: 0.3587 Acc: 0.8525 val Loss: 0.1900 Acc: 0.9477 Epoch 14/24 ---------- train Loss: 0.2771 Acc: 0.8893 val Loss: 0.2317 Acc: 0.9216 Epoch 15/24 ---------- train Loss: 0.3064 Acc: 0.8852 val Loss: 0.1909 Acc: 0.9477 Epoch 16/24 ---------- train Loss: 0.4243 Acc: 0.8238 val Loss: 0.2227 Acc: 0.9346 Epoch 17/24 ---------- train Loss: 0.3297 Acc: 0.8238 val Loss: 0.1916 Acc: 0.9412 Epoch 18/24 ---------- train Loss: 0.4235 Acc: 0.8238 val Loss: 0.1766 Acc: 0.9477 Epoch 19/24 ---------- train Loss: 0.2500 Acc: 0.8934 val Loss: 0.2003 Acc: 0.9477 Epoch 20/24 ---------- train Loss: 0.2413 Acc: 0.8934 val Loss: 0.1821 Acc: 0.9477 Epoch 21/24 ---------- train Loss: 0.3762 Acc: 0.8115 val Loss: 0.1842 Acc: 0.9412 Epoch 22/24 ---------- train Loss: 0.3485 Acc: 0.8566 val Loss: 0.2166 Acc: 0.9281 Epoch 23/24 ---------- train Loss: 0.3625 Acc: 0.8361 val Loss: 0.1747 Acc: 0.9412 Epoch 24/24 ---------- train Loss: 0.3840 Acc: 0.8320 val Loss: 0.1768 Acc: 0.9412 Training complete in 0m 32s Best val Acc: 0.954248 visualize_model(model_conv) plt.ioff() plt.show() visualize_model(model_conv) plt.ioff() plt.show() Inference on custom images¶ Use the trained model to make predictions on custom images and visualize the predicted class labels along with the images. def visualize_model_predictions(model,img_path): was_training = model.training model.eval() img = Image.open(img_path) img = data_transforms[\\'val\\'](img) img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): outputs = model(img) _, preds = torch.max(outputs, 1) ax = plt.subplot(2,2,1) ax.axis(\\'off\\') ax.set_title(f\\'Predicted: {class_names[preds[0]]}\\') imshow(img.cpu().data[0]) model.train(mode=was_training) visualize_model_predictions( model_conv, img_path=\\'data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg\\' ) plt.ioff() plt.show() Use the trained model to make predictions on custom images and visualize the predicted class labels along with the images. def visualize_model_predictions(model,img_path): was_training = model.training model.eval() img = Image.open(img_path) img = data_transforms[\\'val\\'](img) img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): outputs = model(img) _, preds = torch.max(outputs, 1) ax = plt.subplot(2,2,1) ax.axis(\\'off\\') ax.set_title(f\\'Predicted: {class_names[preds[0]]}\\') imshow(img.cpu().data[0]) model.train(mode=was_training) def visualize_model_predictions(model,img_path): was_training = model.training model.eval() img = Image.open(img_path) img = data_transforms[\\'val\\'](img) img = img.unsqueeze(0) img = img.to(device) with torch.no_grad(): outputs = model(img) _, preds = torch.max(outputs, 1) ax = plt.subplot(2,2,1) ax.axis(\\'off\\') ax.set_title(f\\'Predicted: {class_names[preds[0]]}\\') imshow(img.cpu().data[0]) model.train(mode=was_training) visualize_model_predictions( model_conv, img_path=\\'data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg\\' ) plt.ioff() plt.show() visualize_model_predictions( model_conv, img_path=\\'data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg\\' ) plt.ioff() plt.show() Further Learning¶ If you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial. Total running time of the script: ( 1 minutes 38.407 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery If you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial. Total running time of the script: ( 1 minutes 38.407 seconds) Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Download Python source code: transfer_learning_tutorial.py Download Python source code: transfer_learning_tutorial.py Download Jupyter notebook: transfer_learning_tutorial.ipynb Download Jupyter notebook: transfer_learning_tutorial.ipynb Gallery generated by Sphinx-Gallery Next Previous Rate this Tutorial Rate this Tutorial © Copyright 2024, PyTorch. © Copyright 2024, PyTorch. Built with Sphinx using a theme provided by Read the Docs. Transfer Learning for Computer Vision Tutorial Load Data Visualize a few images Training the model Visualizing the model predictions Finetuning the ConvNet Train and evaluate ConvNet as fixed feature extractor Train and evaluate Inference on custom images Further Learning Transfer Learning for Computer Vision Tutorial Load Data Visualize a few images Training the model Visualizing the model predictions Finetuning the ConvNet Train and evaluate ConvNet as fixed feature extractor Train and evaluate Inference on custom images Further Learning Transfer Learning for Computer Vision Tutorial Load Data Visualize a few images Training the model Visualizing the model predictions Finetuning the ConvNet Train and evaluate ConvNet as fixed feature extractor Train and evaluate Inference on custom images Further Learning Docs Access comprehensive developer documentation for PyTorch View Docs Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials Resources Find development resources and get your questions answered View Resources Docs Access comprehensive developer documentation for PyTorch View Docs Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials Resources Find development resources and get your questions answered View Resources Docs Access comprehensive developer documentation for PyTorch View Docs Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials Resources Find development resources and get your questions answered View Resources Docs Access comprehensive developer documentation for PyTorch View Docs Access comprehensive developer documentation for PyTorch Tutorials Get in-depth tutorials for beginners and advanced developers View Tutorials Get in-depth tutorials for beginners and advanced developers Resources Find development resources and get your questions answered View Resources Find development resources and get your questions answered PyTorch Get Started Features Ecosystem Blog Contributing Resources Tutorials Docs Discuss Github Issues Brand Guidelines Stay up to date Facebook Twitter YouTube LinkedIn PyTorch Podcasts Spotify Apple Google Amazon Terms | Privacy © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/. PyTorch Get Started Features Ecosystem Blog Contributing Resources Tutorials Docs Discuss Github Issues Brand Guidelines Stay up to date Facebook Twitter YouTube LinkedIn PyTorch Podcasts Spotify Apple Google Amazon PyTorch Get Started Features Ecosystem Blog Contributing Resources Tutorials Docs Discuss Github Issues Brand Guidelines Stay up to date Facebook Twitter YouTube LinkedIn PyTorch Podcasts Spotify Apple Google Amazon Terms | Privacy © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/. © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch open source project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, please see www.lfprojects.org/policies/. To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy. To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy. To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: Cookies Policy. Learn Get Started Tutorials Learn the Basics PyTorch Recipes Introduction to PyTorch - YouTube Series Ecosystem Tools Community Forums Developer Resources Contributor Awards - 2023 Edge About PyTorch Edge ExecuTorch Docs PyTorch PyTorch Domains Blog & News PyTorch Blog Community Blog Videos Community Stories Events About PyTorch Foundation Governing Board Learn Get Started Tutorials Learn the Basics PyTorch Recipes Introduction to PyTorch - YouTube Series Ecosystem Tools Community Forums Developer Resources Contributor Awards - 2023 Edge About PyTorch Edge ExecuTorch Docs PyTorch PyTorch Domains Blog & News PyTorch Blog Community Blog Videos Community Stories Events About PyTorch Foundation Governing Board Learn Get Started Tutorials Learn the Basics PyTorch Recipes Introduction to PyTorch - YouTube Series Ecosystem Tools Community Forums Developer Resources Contributor Awards - 2023 Edge About PyTorch Edge ExecuTorch Docs PyTorch PyTorch Domains Blog & News PyTorch Blog Community Blog Videos Community Stories Events About PyTorch Foundation Governing Board',\n",
       " 'https://cs231n.github.io/transfer-learning/': '(These notes are currently in draft form and under development) Table of Contents: Transfer Learning Additional References Transfer Learning In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows: ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset. Fine-tuning the ConvNet. The second strategy is to not only replace and retrain the classifier on top of the ConvNet on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the backpropagation. It is possible to fine-tune all the layers of the ConvNet, or it’s possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. This is motivated by the observation that the earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the ConvNet becomes progressively more specific to the details of the classes contained in the original dataset. In case of ImageNet for example, which contains many dog breeds, a significant portion of the representational power of the ConvNet may be devoted to features that are specific to differentiating between dog breeds. Pretrained models. Since modern ConvNets take 2-3 weeks to train across multiple GPUs on ImageNet, it is common to see people release their final ConvNet checkpoints for the benefit of others who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo where people share their network weights. When and how to fine-tune? How do you decide what type of transfer learning you should perform on a new dataset? This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different, such as microscope images). Keeping in mind that ConvNet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios: New dataset is small and similar to original dataset. Since the data is small, it is not a good idea to fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes. New dataset is large and similar to the original dataset. Since we have more data, we can have more confidence that we won’t overfit if we were to try to fine-tune through the full network. New dataset is small but very different from the original dataset. Since the data is small, it is likely best to only train a linear classifier. Since the dataset is very different, it might not be best to train the classifier form the top of the network, which contains more dataset-specific features. Instead, it might work better to train the SVM classifier from activations somewhere earlier in the network. New dataset is large and very different from the original dataset. Since the dataset is very large, we may expect that we can afford to train a ConvNet from scratch. However, in practice it is very often still beneficial to initialize with weights from a pretrained model. In this case, we would have enough data and confidence to fine-tune through the entire network. Practical advice. There are a few additional things to keep in mind when performing Transfer Learning: Constraints from pretrained models. Note that if you wish to use a pretrained network, you may be slightly constrained in terms of the architecture you can use for your new dataset. For example, you can’t arbitrarily take out Conv layers from the pretrained network. However, some changes are straight-forward: Due to parameter sharing, you can easily run a pretrained network on images of different spatial size. This is clearly evident in the case of Conv/Pool layers because their forward function is independent of the input volume spatial size (as long as the strides “fit”). In case of FC layers, this still holds true because FC layers can be converted to a Convolutional Layer: For example, in an AlexNet, the final pooling volume before the first FC layer is of size [6x6x512]. Therefore, the FC layer looking at this volume is equivalent to having a Convolutional Layer that has receptive field size 6x6, and is applied with padding of 0. Learning rates. It’s common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don’t wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization). Additional References CNN Features off-the-shelf: an Astounding Baseline for Recognition trains SVMs on features from ImageNet-pretrained ConvNet and reports several state of the art results. DeCAF reported similar findings in 2013. The framework in this paper (DeCAF) was a Python-based precursor to the C++ Caffe library. How transferable are features in deep neural networks? studies the transfer learning performance in detail, including some unintuitive findings about layer co-adaptations. (These notes are currently in draft form and under development) Table of Contents: Transfer Learning Additional References Transfer Learning In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows: ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset. Fine-tuning the ConvNet. The second strategy is to not only replace and retrain the classifier on top of the ConvNet on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the backpropagation. It is possible to fine-tune all the layers of the ConvNet, or it’s possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. This is motivated by the observation that the earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the ConvNet becomes progressively more specific to the details of the classes contained in the original dataset. In case of ImageNet for example, which contains many dog breeds, a significant portion of the representational power of the ConvNet may be devoted to features that are specific to differentiating between dog breeds. Pretrained models. Since modern ConvNets take 2-3 weeks to train across multiple GPUs on ImageNet, it is common to see people release their final ConvNet checkpoints for the benefit of others who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo where people share their network weights. When and how to fine-tune? How do you decide what type of transfer learning you should perform on a new dataset? This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different, such as microscope images). Keeping in mind that ConvNet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios: New dataset is small and similar to original dataset. Since the data is small, it is not a good idea to fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes. New dataset is large and similar to the original dataset. Since we have more data, we can have more confidence that we won’t overfit if we were to try to fine-tune through the full network. New dataset is small but very different from the original dataset. Since the data is small, it is likely best to only train a linear classifier. Since the dataset is very different, it might not be best to train the classifier form the top of the network, which contains more dataset-specific features. Instead, it might work better to train the SVM classifier from activations somewhere earlier in the network. New dataset is large and very different from the original dataset. Since the dataset is very large, we may expect that we can afford to train a ConvNet from scratch. However, in practice it is very often still beneficial to initialize with weights from a pretrained model. In this case, we would have enough data and confidence to fine-tune through the entire network. Practical advice. There are a few additional things to keep in mind when performing Transfer Learning: Constraints from pretrained models. Note that if you wish to use a pretrained network, you may be slightly constrained in terms of the architecture you can use for your new dataset. For example, you can’t arbitrarily take out Conv layers from the pretrained network. However, some changes are straight-forward: Due to parameter sharing, you can easily run a pretrained network on images of different spatial size. This is clearly evident in the case of Conv/Pool layers because their forward function is independent of the input volume spatial size (as long as the strides “fit”). In case of FC layers, this still holds true because FC layers can be converted to a Convolutional Layer: For example, in an AlexNet, the final pooling volume before the first FC layer is of size [6x6x512]. Therefore, the FC layer looking at this volume is equivalent to having a Convolutional Layer that has receptive field size 6x6, and is applied with padding of 0. Learning rates. It’s common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don’t wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization). Additional References CNN Features off-the-shelf: an Astounding Baseline for Recognition trains SVMs on features from ImageNet-pretrained ConvNet and reports several state of the art results. DeCAF reported similar findings in 2013. The framework in this paper (DeCAF) was a Python-based precursor to the C++ Caffe library. How transferable are features in deep neural networks? studies the transfer learning performance in detail, including some unintuitive findings about layer co-adaptations. (These notes are currently in draft form and under development) Table of Contents: Transfer Learning Additional References Transfer Learning In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows: ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset. Fine-tuning the ConvNet. The second strategy is to not only replace and retrain the classifier on top of the ConvNet on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the backpropagation. It is possible to fine-tune all the layers of the ConvNet, or it’s possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. This is motivated by the observation that the earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the ConvNet becomes progressively more specific to the details of the classes contained in the original dataset. In case of ImageNet for example, which contains many dog breeds, a significant portion of the representational power of the ConvNet may be devoted to features that are specific to differentiating between dog breeds. Pretrained models. Since modern ConvNets take 2-3 weeks to train across multiple GPUs on ImageNet, it is common to see people release their final ConvNet checkpoints for the benefit of others who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo where people share their network weights. When and how to fine-tune? How do you decide what type of transfer learning you should perform on a new dataset? This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different, such as microscope images). Keeping in mind that ConvNet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios: New dataset is small and similar to original dataset. Since the data is small, it is not a good idea to fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes. New dataset is large and similar to the original dataset. Since we have more data, we can have more confidence that we won’t overfit if we were to try to fine-tune through the full network. New dataset is small but very different from the original dataset. Since the data is small, it is likely best to only train a linear classifier. Since the dataset is very different, it might not be best to train the classifier form the top of the network, which contains more dataset-specific features. Instead, it might work better to train the SVM classifier from activations somewhere earlier in the network. New dataset is large and very different from the original dataset. Since the dataset is very large, we may expect that we can afford to train a ConvNet from scratch. However, in practice it is very often still beneficial to initialize with weights from a pretrained model. In this case, we would have enough data and confidence to fine-tune through the entire network. Practical advice. There are a few additional things to keep in mind when performing Transfer Learning: Constraints from pretrained models. Note that if you wish to use a pretrained network, you may be slightly constrained in terms of the architecture you can use for your new dataset. For example, you can’t arbitrarily take out Conv layers from the pretrained network. However, some changes are straight-forward: Due to parameter sharing, you can easily run a pretrained network on images of different spatial size. This is clearly evident in the case of Conv/Pool layers because their forward function is independent of the input volume spatial size (as long as the strides “fit”). In case of FC layers, this still holds true because FC layers can be converted to a Convolutional Layer: For example, in an AlexNet, the final pooling volume before the first FC layer is of size [6x6x512]. Therefore, the FC layer looking at this volume is equivalent to having a Convolutional Layer that has receptive field size 6x6, and is applied with padding of 0. Learning rates. It’s common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don’t wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization). Additional References CNN Features off-the-shelf: an Astounding Baseline for Recognition trains SVMs on features from ImageNet-pretrained ConvNet and reports several state of the art results. DeCAF reported similar findings in 2013. The framework in this paper (DeCAF) was a Python-based precursor to the C++ Caffe library. How transferable are features in deep neural networks? studies the transfer learning performance in detail, including some unintuitive findings about layer co-adaptations. (These notes are currently in draft form and under development) Table of Contents: In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows: When and how to fine-tune? How do you decide what type of transfer learning you should perform on a new dataset? This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different, such as microscope images). Keeping in mind that ConvNet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios: Practical advice. There are a few additional things to keep in mind when performing Transfer Learning: cs231n cs231n cs231n cs231n'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_extracted_hw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a9c2c3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(chunked_links_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d3e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
