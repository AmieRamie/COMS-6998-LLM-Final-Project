{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading RAG Data into ChromaDB Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30011 documents.\n",
      "Sample document: {'source': 'https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model', 'text': 'Skip to content Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert ray-project / llm-numbers Public Notifications You must be signed in to change notification settings Fork 141 Star 4.1k Numbers every LLM developer should know 4.1k stars 141 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 9 Pull requests 1 Actions Projects 0 Security Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights ray-project/llm-numbers mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History37 CommitsREADME.mdREADME.md View all filesRepository files navigationREADMENumbers every LLM Developer should know 中文 At Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_json_files(directory):\n",
    "    \"\"\"\n",
    "    Load all JSON files from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the data from all JSON files.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data.append(json.load(f))\n",
    "    return data\n",
    "\n",
    "def extract_text(data):\n",
    "    \"\"\"\n",
    "    Extract text data from the JSON structure, supporting two formats.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries containing JSON data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with URLs or file paths and their corresponding text chunks.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        for key, value in entry.items():\n",
    "            if isinstance(value, list):  # First format with URL keys\n",
    "                for chunk in value:\n",
    "                    documents.append({\"source\": key, \"text\": chunk})\n",
    "            elif isinstance(value, dict):  # Second format with file paths as keys\n",
    "                if \"text\" in value:\n",
    "                    documents.append({\"source\": key, \"text\": value[\"text\"]})\n",
    "    return documents\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory = \"RAG_data\"\n",
    "\n",
    "# Load and preprocess data\n",
    "json_data = load_json_files(directory)\n",
    "documents = extract_text(json_data)\n",
    "\n",
    "# Example output\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(\"Sample document:\", documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be utilizing the text-embedding-3-large embedding model through the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "import timeit\n",
    "from scipy.spatial.distance import cosine\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tiktoken import encoding_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-large\", max_tokens=8192):\n",
    "    tokenizer = encoding_for_model(model)  \n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        print(f\"Truncating text to {max_tokens} tokens: {text[:100]}...\")  # Log truncation\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = tokenizer.decode(tokens)\n",
    "    try:\n",
    "        response = client.embeddings.create(input=text, model=model)\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to encode text: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-3-large\", max_tokens=8192, batch_size=100):\n",
    "    embeddings = []\n",
    "    tokenizer = encoding_for_model(model)  # Get the tokenizer for the model\n",
    "   \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "\n",
    "        # Truncate texts in the batch that exceed the token limit\n",
    "        truncated_batch = []\n",
    "        for text in batch:\n",
    "            tokens = tokenizer.encode(text)\n",
    "            if len(tokens) > max_tokens:\n",
    "                print(f\"Truncating text to {max_tokens} tokens: {text[:100]}...\")  # Log truncation\n",
    "                tokens = tokens[:max_tokens]\n",
    "                text = tokenizer.decode(tokens)\n",
    "            truncated_batch.append(text)\n",
    "\n",
    "        try:\n",
    "            # Generate embeddings for the batch\n",
    "            response = client.embeddings.create(input=truncated_batch, model=model)\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            if i % 5000 == 0:\n",
    "                print(f\"Generated embeddings for batch {i}-{i + batch_size}\")\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for batch {i}-{i + batch_size}: {e}\")\n",
    "            embeddings.extend([None] * len(batch))  # Append None for unexpected errors\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for batch 0-1000\n",
      "Truncating text to 8192 tokens: An In-Depth Guide to the VR Universe Article Top 10 Career Opportunities in Artificial Intelligence ...\n",
      "Generated embeddings for batch 5000-6000\n",
      "Generated embeddings for batch 10000-11000\n",
      "Truncating text to 8192 tokens: R Python library(h2o) # Start the H2O cluster (locally) h2o.init() # Import a sample binary outcome ...\n",
      "Generated embeddings for batch 15000-16000\n",
      "Truncating text to 8192 tokens: All rights reserved Products ignio AIOps Redefining IT Operations with AI and Automation ignio Obser...\n",
      "Generated embeddings for batch 20000-21000\n",
      "Generated embeddings for batch 25000-26000\n",
      "Truncating text to 8192 tokens: Topics Alphabetic H2O Wiki Algorithms Activation Function Confusion Matrix Convolutional Neural Netw...\n",
      "Truncating text to 8192 tokens: By acrastt • Oct 18, 2023 Building Your First Kubeflow Pipeline: A Comprehensive Guide By turhancan9...\n",
      "Truncating text to 8192 tokens: Media center Investors InvestorsCareersPartner Portal login Medical Medical Advanced wound care Adva...\n",
      "Generated embeddings for batch 30000-31000\n"
     ]
    }
   ],
   "source": [
    "texts = [doc[\"text\"] for doc in documents]\n",
    "embeddings = get_embeddings(texts, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into ChromaDB Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to initialize the ChromaDB vector store. The data will persist in the \"chroma\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client = chromadb.PersistentClient(settings=Settings(allow_reset=True))\n",
    "collection = persistent_client.get_or_create_collection(\"llm_tutor_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the documents and generated embeddings into the llm_tutor_collection in our ChromaDB instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "batch_ids = []\n",
    "batch_texts = []\n",
    "batch_metadata = []\n",
    "batch_embeddings = []\n",
    "\n",
    "id = 0\n",
    "for doc, embedding in zip(documents, embeddings):\n",
    "    # Add data to the current batch\n",
    "    batch_ids.append(str(id))\n",
    "    batch_texts.append(doc[\"text\"])\n",
    "    batch_metadata.append({\"source\": doc[\"source\"]})\n",
    "    batch_embeddings.append(embedding)\n",
    "    id += 1\n",
    "\n",
    "    # Check if the batch is ready for uploading\n",
    "    if len(batch_ids) == batch_size:\n",
    "        # Upload the batch to ChromaDB\n",
    "        collection.add(\n",
    "            ids=batch_ids,\n",
    "            documents=batch_texts,\n",
    "            metadatas=batch_metadata,\n",
    "            embeddings=batch_embeddings\n",
    "        )\n",
    "        # Clear the batch lists\n",
    "        batch_ids = []\n",
    "        batch_texts = []\n",
    "        batch_metadata = []\n",
    "        batch_embeddings = []\n",
    "\n",
    "# Upload any remaining data in the last batch\n",
    "if batch_ids:\n",
    "    collection.add(\n",
    "        ids=batch_ids,\n",
    "        documents=batch_texts,\n",
    "        metadatas=batch_metadata,\n",
    "        embeddings=batch_embeddings\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30011"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Generative LLM Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_base = \"gpt-4o-mini\"\n",
    "gpt_4o_finetuned = \"ft:gpt-4o-mini-2024-07-18:f-prime-capital::AbZYSjIT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up RAG methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(query, n_results=5):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents from the ChromaDB vector store.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question or query.\n",
    "        collection (Collection): The ChromaDB collection object.\n",
    "        n_results (int): Number of results to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        str: Concatenated text of the top retrieved documents.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the query using Gemini model\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    # Retrieve top documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "\n",
    "    # Combine text from the retrieved documents\n",
    "    retrieved_text = \" \".join(doc[0] for doc in results[\"documents\"])\n",
    "    return retrieved_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag(model, query):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_text = retrieve_relevant_documents(query)\n",
    "\n",
    "    # Generate response using OpenAI Model\n",
    "    all_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful tutor who answers questions about a class called Introduction to Deep Learning and LLM based Generative AI Systems\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Generate an answer to the following question using the given context:\\n\\n {query}\\n\\n {\"=\"*50}\\n\\nCONTEXT: {retrieved_text}\\n\\n\"}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=all_messages,\n",
    "        max_tokens=1500,\n",
    "    )\n",
    "    model_response_text = response.choices[0].message.content\n",
    "    return model_response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When training a deep learning model for image classification, there are several best practices you should consider to optimize performance and efficiency:\n",
      "\n",
      "1. **Image Size**: Use a consistent image size for your dataset that is appropriate for the model architecture you choose. Common sizes for models trained on ImageNet are 224x224 or 299x299 pixels. Starting with a smaller size (e.g., 64 pixels) can help in quickly testing your pipeline before scaling up.\n",
      "\n",
      "2. **Validation Set Size**: The size of your validation set should be proportionate to your overall dataset. While a common practice is to allocate around 20% for validation, it’s essential to ensure that the validation set is large enough to provide reliable metrics. If results are fluctuating significantly across training runs, it may indicate that the validation set is too small.\n",
      "\n",
      "3. **Data Preprocessing**: Make sure to preprocess your images appropriately. Consider aspects like normalization, resizing, and potentially data augmentation techniques to enhance the robustness of your model.\n",
      "\n",
      "4. **Monitoring GPU Memory**: When working with deep learning models, particularly those with large architectures, you may encounter GPU memory issues (e.g., 'CUDA out of memory' errors). In such cases, start with a smaller batch size and incrementally increase it while monitoring memory usage.\n",
      "\n",
      "5. **Using Pre-trained Models**: Leverage pre-computed classifiers or transfer learning. Initiating training with a pre-trained model can significantly boost your performance and reduce training time. For instance, starting without data augmentation may yield a decent accuracy, and then gradually refining your model can lead to improved results.\n",
      "\n",
      "6. **Continuous Evaluation**: Regularly evaluate your model's accuracy and adjust hyperparameters as necessary. If using different architectural configurations, compare their performances systematically to identify the best approach.\n",
      "\n",
      "Incorporating these best practices will not only enhance the quality of your model but also streamline the training process, making it more efficient and effective.\n"
     ]
    }
   ],
   "source": [
    "question = \"I want to build a deep learning model for image classification. What are some best practices for training deep learning models?\"\n",
    "response = run_rag(gpt_4o_base ,question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating RAG with Embedding Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Test Data/test_data.json', \"r\") as json_file:\n",
    "    test_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_semantic_sim(model):\n",
    "    # Evaluate semantic similarity\n",
    "    similarities = []\n",
    "    start = timeit.default_timer()\n",
    "    # last_pause = timeit.default_timer()\n",
    "    for i,example in enumerate(test_data):\n",
    "        try:\n",
    "            input_text = example['input']\n",
    "            expected_output = example['output']\n",
    "            # Generate model response with RAG\n",
    "            model_response_text = run_rag(model, input_text)\n",
    "            # Generate embeddings for expected and actual responses\n",
    "            expected_embedding = get_embedding(expected_output)\n",
    "            model_response_embedding = get_embedding(model_response_text)\n",
    "\n",
    "            # Calculate similarity\n",
    "            similarity = cosine_similarity(expected_embedding, model_response_embedding)\n",
    "            similarities.append({'question':input_text ,'expected_output':expected_output,'model_output':model_response_text,'similarities':similarity})\n",
    "        except Exception as e:\n",
    "            print(i,e)\n",
    "            # break\n",
    "        if (i%5==0)&(i!=0):\n",
    "            end = timeit.default_timer()\n",
    "            print(f\"{i} - Time Spent: {end-start}, Number of Errors: {i + 1 - len(similarities)}\")\n",
    "            start = timeit.default_timer()\n",
    "            # print(f\"Sleeping for {60-(timeit.default_timer()-last_pause)} seconds\")\n",
    "            # if (60-(timeit.default_timer()-last_pause))>0:\n",
    "            #     time.sleep(60-(timeit.default_timer()-last_pause))\n",
    "            # last_pause = timeit.default_timer()\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 - Time Spent: 26.1099309999845, Number of Errors: 0\n",
      "10 - Time Spent: 21.710568415990565, Number of Errors: 0\n"
     ]
    }
   ],
   "source": [
    "gpt_4o_base_similarities = pd.DataFrame(eval_semantic_sim(gpt_4o_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Similarity for GPT-4o Base: 0.7484650916190843\n",
      "Standard Deviation Similarity for GPT-4o Base: 0.13015104092179497\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Similarity for GPT-4o Base: {gpt_4o_base_similarities['similarities'].mean()}\")\n",
    "print(f\"Standard Deviation Similarity for GPT-4o Base: {gpt_4o_base_similarities['similarities'].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_base_similarities.to_csv('./Eval Results/gpt_4o_base_similarities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 - Time Spent: 11.609997749968898, Number of Errors: 0\n",
      "10 - Time Spent: 11.780776708037592, Number of Errors: 0\n",
      "15 - Time Spent: 13.204438125016168, Number of Errors: 0\n",
      "20 - Time Spent: 9.54497404198628, Number of Errors: 0\n",
      "25 - Time Spent: 10.270393292012159, Number of Errors: 0\n",
      "30 - Time Spent: 9.671339874970727, Number of Errors: 0\n",
      "35 - Time Spent: 10.642300332954619, Number of Errors: 0\n",
      "40 - Time Spent: 9.344267417036463, Number of Errors: 0\n",
      "45 - Time Spent: 22.38648824999109, Number of Errors: 0\n",
      "50 - Time Spent: 11.367734291008674, Number of Errors: 0\n",
      "55 - Time Spent: 9.613058666000143, Number of Errors: 0\n",
      "60 - Time Spent: 34.64262462500483, Number of Errors: 0\n",
      "65 - Time Spent: 15.250248208001722, Number of Errors: 0\n",
      "70 - Time Spent: 10.257519832986873, Number of Errors: 0\n",
      "75 - Time Spent: 11.663794916006736, Number of Errors: 0\n",
      "80 - Time Spent: 9.41814374999376, Number of Errors: 0\n",
      "85 - Time Spent: 12.185798958002124, Number of Errors: 0\n",
      "90 - Time Spent: 12.740425250027329, Number of Errors: 0\n",
      "95 - Time Spent: 12.575084957992658, Number of Errors: 0\n",
      "100 - Time Spent: 11.55015495797852, Number of Errors: 0\n",
      "105 - Time Spent: 14.948899916023947, Number of Errors: 0\n",
      "110 - Time Spent: 10.035550542001147, Number of Errors: 0\n",
      "115 - Time Spent: 9.179149999981746, Number of Errors: 0\n",
      "120 - Time Spent: 14.06527704198379, Number of Errors: 0\n",
      "125 - Time Spent: 10.162685416988097, Number of Errors: 0\n",
      "130 - Time Spent: 10.522971792030148, Number of Errors: 0\n",
      "135 - Time Spent: 10.220513415988535, Number of Errors: 0\n",
      "140 - Time Spent: 13.217193833959755, Number of Errors: 0\n",
      "145 - Time Spent: 9.662053624982946, Number of Errors: 0\n",
      "150 - Time Spent: 10.010060332948342, Number of Errors: 0\n",
      "155 - Time Spent: 10.133697790966835, Number of Errors: 0\n",
      "160 - Time Spent: 9.733397499949206, Number of Errors: 0\n",
      "165 - Time Spent: 8.603737625002395, Number of Errors: 0\n",
      "170 - Time Spent: 9.722611874982249, Number of Errors: 0\n",
      "175 - Time Spent: 9.732358374982141, Number of Errors: 0\n",
      "180 - Time Spent: 14.994964958983473, Number of Errors: 0\n",
      "185 - Time Spent: 10.095481249969453, Number of Errors: 0\n",
      "190 - Time Spent: 9.92759791703429, Number of Errors: 0\n",
      "195 - Time Spent: 10.617668791965116, Number of Errors: 0\n",
      "200 - Time Spent: 11.44374916702509, Number of Errors: 0\n",
      "205 - Time Spent: 8.577421916997992, Number of Errors: 0\n",
      "210 - Time Spent: 8.457254541979637, Number of Errors: 0\n",
      "215 - Time Spent: 9.985156000009738, Number of Errors: 0\n",
      "220 - Time Spent: 13.019725832971744, Number of Errors: 0\n",
      "225 - Time Spent: 9.937248417001683, Number of Errors: 0\n",
      "230 - Time Spent: 11.26937483297661, Number of Errors: 0\n",
      "235 - Time Spent: 9.481424208031967, Number of Errors: 0\n",
      "240 - Time Spent: 9.846363207965624, Number of Errors: 0\n",
      "245 - Time Spent: 13.392195999971591, Number of Errors: 0\n",
      "250 - Time Spent: 10.534282707958482, Number of Errors: 0\n",
      "255 - Time Spent: 9.568594790995121, Number of Errors: 0\n",
      "260 - Time Spent: 11.270714499987662, Number of Errors: 0\n",
      "265 - Time Spent: 11.642699167015962, Number of Errors: 0\n",
      "270 - Time Spent: 10.397796625038609, Number of Errors: 0\n",
      "275 - Time Spent: 9.555108957982156, Number of Errors: 0\n",
      "280 - Time Spent: 13.222407834022306, Number of Errors: 0\n",
      "285 - Time Spent: 10.11032537498977, Number of Errors: 0\n",
      "290 - Time Spent: 10.480153916054405, Number of Errors: 0\n",
      "295 - Time Spent: 9.504992874979507, Number of Errors: 0\n",
      "300 - Time Spent: 9.604856625024695, Number of Errors: 0\n",
      "305 - Time Spent: 10.354227791016456, Number of Errors: 0\n",
      "310 - Time Spent: 8.084621082991362, Number of Errors: 0\n",
      "315 - Time Spent: 10.013794541999232, Number of Errors: 0\n",
      "320 - Time Spent: 9.87691787496442, Number of Errors: 0\n",
      "325 - Time Spent: 10.662769165995996, Number of Errors: 0\n",
      "330 - Time Spent: 9.814284958993085, Number of Errors: 0\n",
      "335 - Time Spent: 8.54344191699056, Number of Errors: 0\n",
      "340 - Time Spent: 13.692863459000364, Number of Errors: 0\n",
      "345 - Time Spent: 9.51617299998179, Number of Errors: 0\n",
      "350 - Time Spent: 13.436540666036308, Number of Errors: 0\n",
      "355 - Time Spent: 9.336336333013605, Number of Errors: 0\n",
      "360 - Time Spent: 8.369742750015575, Number of Errors: 0\n",
      "365 - Time Spent: 9.223781707987655, Number of Errors: 0\n",
      "370 - Time Spent: 9.252853958052583, Number of Errors: 0\n",
      "375 - Time Spent: 9.559485792007763, Number of Errors: 0\n",
      "380 - Time Spent: 9.021055458986666, Number of Errors: 0\n",
      "385 - Time Spent: 24.06511349999346, Number of Errors: 0\n",
      "390 - Time Spent: 12.240242458006833, Number of Errors: 0\n",
      "395 - Time Spent: 7.781908957986161, Number of Errors: 0\n",
      "400 - Time Spent: 10.091886958980467, Number of Errors: 0\n",
      "405 - Time Spent: 11.608382707985584, Number of Errors: 0\n",
      "410 - Time Spent: 10.944646708958317, Number of Errors: 0\n",
      "415 - Time Spent: 10.145898042013869, Number of Errors: 0\n",
      "420 - Time Spent: 12.615936874994077, Number of Errors: 0\n",
      "425 - Time Spent: 10.959328791010194, Number of Errors: 0\n",
      "430 - Time Spent: 11.193079042015597, Number of Errors: 0\n",
      "435 - Time Spent: 12.426373249967583, Number of Errors: 0\n",
      "440 - Time Spent: 9.48335799999768, Number of Errors: 0\n",
      "445 - Time Spent: 10.934301249973942, Number of Errors: 0\n",
      "450 - Time Spent: 13.694297042035032, Number of Errors: 0\n",
      "455 - Time Spent: 10.439776958024595, Number of Errors: 0\n",
      "460 - Time Spent: 10.476273666019551, Number of Errors: 0\n",
      "465 - Time Spent: 12.11827145901043, Number of Errors: 0\n",
      "470 - Time Spent: 8.693169499980286, Number of Errors: 0\n",
      "475 - Time Spent: 10.441740999987815, Number of Errors: 0\n",
      "480 - Time Spent: 10.889816833019722, Number of Errors: 0\n",
      "485 - Time Spent: 10.863340332987718, Number of Errors: 0\n",
      "490 - Time Spent: 9.939594959025271, Number of Errors: 0\n",
      "495 - Time Spent: 9.370921291993, Number of Errors: 0\n",
      "500 - Time Spent: 11.57714266696712, Number of Errors: 0\n",
      "505 - Time Spent: 9.283003374992404, Number of Errors: 0\n",
      "510 - Time Spent: 12.286224542011041, Number of Errors: 0\n",
      "515 - Time Spent: 9.976094749988988, Number of Errors: 0\n",
      "520 - Time Spent: 9.902998542005662, Number of Errors: 0\n",
      "525 - Time Spent: 11.139035874977708, Number of Errors: 0\n",
      "530 - Time Spent: 10.74356070900103, Number of Errors: 0\n",
      "535 - Time Spent: 11.033977957966272, Number of Errors: 0\n",
      "540 - Time Spent: 15.828016249986831, Number of Errors: 0\n",
      "545 - Time Spent: 11.157010332972277, Number of Errors: 0\n",
      "550 - Time Spent: 10.73114083299879, Number of Errors: 0\n",
      "555 - Time Spent: 11.486469625029713, Number of Errors: 0\n",
      "560 - Time Spent: 10.547989874961786, Number of Errors: 0\n",
      "565 - Time Spent: 9.36936691700248, Number of Errors: 0\n",
      "570 - Time Spent: 11.782983957964461, Number of Errors: 0\n",
      "575 - Time Spent: 16.67924458300695, Number of Errors: 0\n",
      "580 - Time Spent: 11.22044179099612, Number of Errors: 0\n",
      "585 - Time Spent: 12.094844875042327, Number of Errors: 0\n",
      "590 - Time Spent: 10.537089999997988, Number of Errors: 0\n",
      "595 - Time Spent: 9.817830667016096, Number of Errors: 0\n",
      "600 - Time Spent: 11.738398167013656, Number of Errors: 0\n",
      "605 - Time Spent: 9.230501707992516, Number of Errors: 0\n",
      "610 - Time Spent: 13.629193166969344, Number of Errors: 0\n",
      "615 - Time Spent: 14.326725291961338, Number of Errors: 0\n",
      "620 - Time Spent: 13.577886957966257, Number of Errors: 0\n",
      "625 - Time Spent: 10.328666459012311, Number of Errors: 0\n",
      "630 - Time Spent: 14.07912229100475, Number of Errors: 0\n",
      "635 - Time Spent: 11.349379958002828, Number of Errors: 0\n",
      "640 - Time Spent: 27.153082665987313, Number of Errors: 0\n",
      "645 - Time Spent: 15.82219408301171, Number of Errors: 0\n",
      "650 - Time Spent: 9.948382708011195, Number of Errors: 0\n",
      "655 - Time Spent: 15.143120167020243, Number of Errors: 0\n",
      "660 - Time Spent: 9.313359749969095, Number of Errors: 0\n",
      "665 - Time Spent: 9.487112749950029, Number of Errors: 0\n",
      "670 - Time Spent: 9.40699379198486, Number of Errors: 0\n",
      "675 - Time Spent: 13.021398667013273, Number of Errors: 0\n",
      "680 - Time Spent: 16.95375054096803, Number of Errors: 0\n",
      "685 - Time Spent: 11.73633724998217, Number of Errors: 0\n",
      "690 - Time Spent: 8.732528583030216, Number of Errors: 0\n",
      "695 - Time Spent: 10.715689292002935, Number of Errors: 0\n",
      "700 - Time Spent: 13.848122874973342, Number of Errors: 0\n",
      "705 - Time Spent: 8.844863417034503, Number of Errors: 0\n",
      "710 - Time Spent: 21.631388625013642, Number of Errors: 0\n",
      "715 - Time Spent: 9.492883542028721, Number of Errors: 0\n",
      "720 - Time Spent: 8.268826624960639, Number of Errors: 0\n",
      "725 - Time Spent: 10.334327665972523, Number of Errors: 0\n",
      "730 - Time Spent: 10.499596500012558, Number of Errors: 0\n",
      "735 - Time Spent: 11.492248666007072, Number of Errors: 0\n",
      "740 - Time Spent: 8.403275166987441, Number of Errors: 0\n",
      "745 - Time Spent: 10.98093791701831, Number of Errors: 0\n",
      "750 - Time Spent: 8.220357917016372, Number of Errors: 0\n",
      "755 - Time Spent: 10.586468458001036, Number of Errors: 0\n",
      "760 - Time Spent: 9.91600958298659, Number of Errors: 0\n",
      "765 - Time Spent: 11.740258959005587, Number of Errors: 0\n",
      "770 - Time Spent: 10.124052457977086, Number of Errors: 0\n",
      "775 - Time Spent: 9.701521624985617, Number of Errors: 0\n",
      "780 - Time Spent: 10.428059083991684, Number of Errors: 0\n",
      "785 - Time Spent: 10.20129233400803, Number of Errors: 0\n",
      "790 - Time Spent: 9.690883959003258, Number of Errors: 0\n",
      "795 - Time Spent: 11.202727374969982, Number of Errors: 0\n",
      "800 - Time Spent: 9.939266165951267, Number of Errors: 0\n",
      "805 - Time Spent: 10.683531666989438, Number of Errors: 0\n",
      "810 - Time Spent: 10.061784416029695, Number of Errors: 0\n",
      "815 - Time Spent: 11.612671457987744, Number of Errors: 0\n",
      "820 - Time Spent: 11.384567708009854, Number of Errors: 0\n",
      "825 - Time Spent: 11.087448000034783, Number of Errors: 0\n",
      "830 - Time Spent: 10.31124687497504, Number of Errors: 0\n",
      "835 - Time Spent: 10.687315959017724, Number of Errors: 0\n",
      "840 - Time Spent: 11.748490415979177, Number of Errors: 0\n",
      "845 - Time Spent: 9.57606420799857, Number of Errors: 0\n",
      "850 - Time Spent: 11.085023999970872, Number of Errors: 0\n",
      "855 - Time Spent: 9.496389709005598, Number of Errors: 0\n",
      "860 - Time Spent: 13.306953000021167, Number of Errors: 0\n",
      "865 - Time Spent: 14.895701083005406, Number of Errors: 0\n",
      "870 - Time Spent: 10.31060541694751, Number of Errors: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt_4o_finetuned_similarities \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43meval_semantic_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_4o_finetuned\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36meval_semantic_sim\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      9\u001b[0m expected_output \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Generate model response with RAG\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model_response_text \u001b[38;5;241m=\u001b[39m \u001b[43mrun_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate embeddings for expected and actual responses\u001b[39;00m\n\u001b[1;32m     13\u001b[0m expected_embedding \u001b[38;5;241m=\u001b[39m get_embedding(expected_output)\n",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m, in \u001b[0;36mrun_rag\u001b[0;34m(model, query)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate response using OpenAI Model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m all_messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful tutor who answers questions about a class called Introduction to Deep Learning and LLM based Generative AI Systems\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate an answer to the following question using the given context:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCONTEXT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretrieved_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      9\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model_response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_response_text\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 993\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    999\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1233\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1232\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1106\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpt_4o_finetuned_similarities = pd.DataFrame(eval_semantic_sim(gpt_4o_finetuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Similarity for GPT-4o Finetuned: {gpt_4o_finetuned_similarities['similarities'].mean()}\")\n",
    "print(f\"Standard Deviation Similarity for GPT-4o Finetuned: {gpt_4o_finetuned_similarities['similarities'].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_base_similarities.to_csv('./Eval Results/gpt_4o_finetuned_similarities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the RAG application responses against the evaluation dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): List of dictionaries with 'input' and 'output' keys.\n",
    "        model_response_function (function): A function that takes an input and generates a response.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    exact_matches = []\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {\"rouge-1\": [], \"rouge-2\": [], \"rouge-l\": []}\n",
    "\n",
    "    for qa in dataset:\n",
    "        time.sleep(10) #Try to avoid rate limiting\n",
    "        # Get input and expected output\n",
    "        input_text = qa[\"input\"]\n",
    "        expected_output = qa[\"output\"]\n",
    "\n",
    "        # Generate model response\n",
    "        generated_output = query(input_text)\n",
    "\n",
    "        # Exact Match\n",
    "        exact_matches.append(int(generated_output.strip() == expected_output.strip()))\n",
    "\n",
    "        # BLEU Score\n",
    "        bleu_score = sentence_bleu(\n",
    "            [expected_output.split()], generated_output.split(), smoothing_function=smoothing_function\n",
    "        )\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # ROUGE Scores\n",
    "        rouge_score = rouge.get_scores(generated_output, expected_output, avg=True)\n",
    "        rouge_scores[\"rouge-1\"].append(rouge_score[\"rouge-1\"][\"f\"])\n",
    "        rouge_scores[\"rouge-2\"].append(rouge_score[\"rouge-2\"][\"f\"])\n",
    "        rouge_scores[\"rouge-l\"].append(rouge_score[\"rouge-l\"][\"f\"])\n",
    "\n",
    "    # Compute averages\n",
    "    metrics = {\n",
    "        \"Exact Match\": sum(exact_matches) / len(exact_matches),\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"ROUGE-1\": sum(rouge_scores[\"rouge-1\"]) / len(rouge_scores[\"rouge-1\"]),\n",
    "        \"ROUGE-2\": sum(rouge_scores[\"rouge-2\"]) / len(rouge_scores[\"rouge-2\"]),\n",
    "        \"ROUGE-L\": sum(rouge_scores[\"rouge-l\"]) / len(rouge_scores[\"rouge-l\"]),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset_path = \"Fine Tuning Data/all_q_and_a_docs_final_v2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(evaluation_dataset_path, \"r\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot get the response text.\nCannot get the Candidate text.\nResponse candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\nContent:\n{}\nCandidate:\n{\n  \"finish_reason\": \"SAFETY\",\n  \"safety_ratings\": [\n    {\n      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.20019531,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.23632812\n    },\n    {\n      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n      \"probability\": \"MEDIUM\",\n      \"blocked\": true,\n      \"probability_score\": 0.75390625,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.43554688\n    },\n    {\n      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.27734375,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.22949219\n    },\n    {\n      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.09033203,\n      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n      \"severity_score\": 0.045410156\n    }\n  ]\n}\nResponse:\n{\n  \"candidates\": [\n    {\n      \"finish_reason\": \"SAFETY\",\n      \"safety_ratings\": [\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.20019531,\n          \"severity\": \"HARM_SEVERITY_LOW\",\n          \"severity_score\": 0.23632812\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"MEDIUM\",\n          \"blocked\": true,\n          \"probability_score\": 0.75390625,\n          \"severity\": \"HARM_SEVERITY_MEDIUM\",\n          \"severity_score\": 0.43554688\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.27734375,\n          \"severity\": \"HARM_SEVERITY_LOW\",\n          \"severity_score\": 0.22949219\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.09033203,\n          \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n          \"severity_score\": 0.045410156\n        }\n      ]\n    }\n  ],\n  \"usage_metadata\": {\n    \"prompt_token_count\": 564,\n    \"candidates_token_count\": 33,\n    \"total_token_count\": 597\n  },\n  \"model_version\": \"gemini-1.5-pro-001\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:2408\u001b[0m, in \u001b[0;36mCandidate.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2410\u001b[0m     \u001b[38;5;66;03m# Enrich the error message with the whole Candidate.\u001b[39;00m\n\u001b[1;32m   2411\u001b[0m     \u001b[38;5;66;03m# The Content object does not have full information.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:2486\u001b[0m, in \u001b[0;36mContent.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts:\n\u001b[0;32m-> 2486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2487\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse candidate content has no parts (and thus no text).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2488\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The candidate is likely blocked by the safety filters.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m _dict_to_pretty_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m   2490\u001b[0m     )\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mValueError\u001b[0m: Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\nContent:\n{}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:2319\u001b[0m, in \u001b[0;36mGenerationResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2321\u001b[0m     \u001b[38;5;66;03m# Enrich the error message with the whole Response.\u001b[39;00m\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;66;03m# The Candidate object does not have full information.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:2412\u001b[0m, in \u001b[0;36mCandidate.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2410\u001b[0m     \u001b[38;5;66;03m# Enrich the error message with the whole Candidate.\u001b[39;00m\n\u001b[1;32m   2411\u001b[0m     \u001b[38;5;66;03m# The Content object does not have full information.\u001b[39;00m\n\u001b[0;32m-> 2412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2413\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get the Candidate text.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2415\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCandidate:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m _dict_to_pretty_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m   2416\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot get the Candidate text.\nResponse candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\nContent:\n{}\nCandidate:\n{\n  \"finish_reason\": \"SAFETY\",\n  \"safety_ratings\": [\n    {\n      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.20019531,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.23632812\n    },\n    {\n      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n      \"probability\": \"MEDIUM\",\n      \"blocked\": true,\n      \"probability_score\": 0.75390625,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.43554688\n    },\n    {\n      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.27734375,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.22949219\n    },\n    {\n      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.09033203,\n      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n      \"severity_score\": 0.045410156\n    }\n  ]\n}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 26\u001b[0m, in \u001b[0;36mevaluate_responses\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     23\u001b[0m expected_output \u001b[38;5;241m=\u001b[39m qa[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Generate model response\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m generated_output \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Exact Match\u001b[39;00m\n\u001b[1;32m     29\u001b[0m exact_matches\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(generated_output\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m expected_output\u001b[38;5;241m.\u001b[39mstrip()))\n",
      "Cell \u001b[0;32mIn[79], line 8\u001b[0m, in \u001b[0;36mquery\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      6\u001b[0m chat_session \u001b[38;5;241m=\u001b[39m gemini_model\u001b[38;5;241m.\u001b[39mstart_chat(response_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an LLM tutor for a Graduate Student taking a course in LLM and Deep Learning System Performance. The student asks you the following question: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_chat_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mHere is context for answering the question:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mretrieved_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mReturn only the answer to the student\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms question.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[70], line 7\u001b[0m, in \u001b[0;36mget_chat_response\u001b[0;34m(chat, prompt)\u001b[0m\n\u001b[1;32m      5\u001b[0m responses \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msend_message(prompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[0;32m----> 7\u001b[0m     text_response\u001b[38;5;241m.\u001b[39mappend(\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_response)\n",
      "File \u001b[0;32m~/Documents/Columbia_Grad/LLM & DL System Performance/Project.nosync/COMS-6998-LLM-Final-Project/.venv/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:2323\u001b[0m, in \u001b[0;36mGenerationResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m   2320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2321\u001b[0m     \u001b[38;5;66;03m# Enrich the error message with the whole Response.\u001b[39;00m\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;66;03m# The Candidate object does not have full information.\u001b[39;00m\n\u001b[0;32m-> 2323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2324\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get the response text.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2326\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m _dict_to_pretty_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m   2327\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot get the response text.\nCannot get the Candidate text.\nResponse candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\nContent:\n{}\nCandidate:\n{\n  \"finish_reason\": \"SAFETY\",\n  \"safety_ratings\": [\n    {\n      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.20019531,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.23632812\n    },\n    {\n      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n      \"probability\": \"MEDIUM\",\n      \"blocked\": true,\n      \"probability_score\": 0.75390625,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.43554688\n    },\n    {\n      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.27734375,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.22949219\n    },\n    {\n      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.09033203,\n      \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n      \"severity_score\": 0.045410156\n    }\n  ]\n}\nResponse:\n{\n  \"candidates\": [\n    {\n      \"finish_reason\": \"SAFETY\",\n      \"safety_ratings\": [\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.20019531,\n          \"severity\": \"HARM_SEVERITY_LOW\",\n          \"severity_score\": 0.23632812\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"MEDIUM\",\n          \"blocked\": true,\n          \"probability_score\": 0.75390625,\n          \"severity\": \"HARM_SEVERITY_MEDIUM\",\n          \"severity_score\": 0.43554688\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.27734375,\n          \"severity\": \"HARM_SEVERITY_LOW\",\n          \"severity_score\": 0.22949219\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.09033203,\n          \"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n          \"severity_score\": 0.045410156\n        }\n      ]\n    }\n  ],\n  \"usage_metadata\": {\n    \"prompt_token_count\": 564,\n    \"candidates_token_count\": 33,\n    \"total_token_count\": 597\n  },\n  \"model_version\": \"gemini-1.5-pro-001\"\n}"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_responses(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
