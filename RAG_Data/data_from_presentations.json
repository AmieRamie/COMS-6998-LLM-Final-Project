{
    "./chunks/chunk_1_Lecture-12-Columbia.pdf": {
        "text": "Lecture 12\nParijat Dube, Chen Wang\n+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\n+\n1\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-12-Columbia.pdf": {
        "text": "Agenda:\nEfficient\nServing of\nLLMs\nBatching Techniques\n\u2022 Static Batching, Dynamic Batching and Continuous Batching\nMemory Optimization Techniques\n\u2022 KV Cache\n\u2022 Flash Attention\nPagedAttention\nLLM Serving Frameworks\n\u2022 VLLM\nServing Systems for Fine-tuning Models\n\u2022 SLORA, VLLM\nGPU Sharing Techniques\n\u2022 Space vs Time Partitioning\n\u2022 Software vs Hardware\n\u2022 LLM inference on MIGS\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-12-Columbia.pdf": {
        "text": "The era of Large Language Models (LLMs)\nChatGPT\nChatting\nBizOps\nLLM\nContent\ncreation\nServed by GPUs\nGitHub\nCopilot\nProgramming\nDev tools\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-12-Columbia.pdf": {
        "text": "Serving LLMs is (surprisingly) slow and\nexpensive\nA single A100 GPU can only serve <1 request per second\n\u2022 Moderate size of model (13 Billion parameters) and input\n\u2022 A ton of GPUs are required for production-scale LLM services\n\u0e08\nr/LocalLLAMA 8\n.8 \ndays ago\nby Financial_Stranger52\nIs local LLM cheaper than ChatGPT API?\nJoin\nChatGPT api only costs 0.002 dollar for 1k token. I found that LLMs like llama output only 10-20 tokens per second,\nwhich is very slow. And such machines costs over 1 dollar per hour. It seems that using api is much cheaper. Based\non these observations, it seems that utilizing the ChatGPT API might be a more affordable option.\nCNBC\nMicrosoft warns of service disruptions\nif it can't get enough A.I. chips for its\ndata centers\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-12-Columbia.pdf": {
        "text": "Inference process of LLMs\nOutput\nthe\nLLM\nfuture\nof\nLLM\nLLM\n\u2191\nInput\nArtificial Intelligence\nIs\nthe\nfuture\nWhy is it slow and inefficient?\n\u2022\nThe sequential dependency between output tokens makes it difficult to fully utilize the parallelism in GPUs.\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-12-Columbia.pdf": {
        "text": "Solution: Batching multiple requests together\nOutput\nthe\na\nLLM\nInput Artificial Intelligence\nis\nAlan\nTuring\nis\nfuture\ncomputer\nof\nscientist\nLLM\nLLM\nthe\na\nfuture\ncomputer\nBetter utilize the parallel hardware\nHowever, we observed that the batch size is significantly limited by the\ninefficient memory management for \"KV Cache\".\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-12-Columbia.pdf": {
        "text": "The basics of LLM inference\nBasic Process\n\u2022\nLLM inference is an iterative process\nthat generates one token at a time\nT, T2 T3 T4 Ts To To Ty\nT\u2081\n\u2022\nGeneration Iteration\nThree Critical Aspects\n\u2022\nPrefill Phase\n\u2022\n\u25cf\n\u2022\nInitial prompt processing takes significant time\nPre-computes attention inputs that remain constant\nduring generation\nUses GPU's parallel compute efficiently\nMemory-IO Bound\n\u2022\n\u2022\nLoading data to GPU cores takes longer than\ncomputation\nThroughput depends on batch size fitting in GPU\nmemory\nMemory efficiency is crucial for performance2\nMemory Scaling\n\u2022\n\u2022\nGPU memory usage scales with model size +\nsequence length\nA 13B parameter model uses ~1MB per token\nOn 40GB A100 GPU: 26GB for model parameters,\n14GB for processing\nhttps://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model\n",
        "links": [
            "https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model"
        ]
    },
    "./chunks/chunk_8_Lecture-12-Columbia.pdf": {
        "text": "Na\u00efve batching/ static batching\nTi T2 T3 Tu Ts To To To\n\u306a\n\u0422\u0443\nSSS S\n\u0422\u0443\nTi Ta Tz Tu Ts To Tr T8\nT3\nSS S SS END\nSAS L S L S L SLEND\nS\u2082 S\u2082 St\nSa So\nSSS S\nSy Sy Sy Sy Sy\nSS S\nCompleting four sequences using static batching. On the first iteration (left), each sequence generates one token (blue)\nfrom the prompt tokens (yellow). After several iterations (right), the completed sequences each have different sizes\nbecause each emits their end-of-sequence-token (red) at different iterations. Even though sequence 3 finished after two\niterations, static batching means that the GPU will be underutilized until the last sequence in the batch finishes generation\n(in this example, sequence 2 after six iterations).\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-12-Columbia.pdf": {
        "text": "Static Batching vs Dynamic Batching\nDynamic batching for generative model inference\nStatic\nbatching\nDynamic\nbatching\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-12-Columbia.pdf": {
        "text": "Continuous Batching\nTi T2 T3 Tu Ts To To To\n\u0422\u0430 \u0442\u044c\nSS S S\nS2 S2 S\nS\u2081 S S S\nSy Sy Sy Sy\nS\n\u0422\u0443\nTi Ta Tz Tu Ts To To To\n\u306a\u306a\nSS S SS END 56 56\nSa Sa SSSSSEND\nS\u2081\u2082 S\u2081\u2082\nS3\nS SEND SS SS\nSy Sy Sy Sy S. SEND\nS5\n57\nCompleting seven sequences using continuous batching. Left shows the batch after a single iteration, right shows the\nbatch after several iterations. Once a sequence emits an end-of-sequence token, we insert a new sequence in its place (i.e.\nsequences S5, S6, and S7). This achieves higher GPU utilization since the GPU does not wait for all sequences to complete\nbefore starting a new one.\nYu, Gyeong-In, et al. \"Orca: A distributed serving system for {Transformer-Based} generative models.\" 16th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022.\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-12-Columbia.pdf": {
        "text": "Continuous Batching\nBatching strategies for LLM inference\nIndividual\nrequests\nDynamic\nbatching\nContinuous\nbatching\n\u2610\n3333\nContinuous batching improves.\nGPU utilization over dynamic\nbatching by eliminating the idle\ntime waiting for the longest\nresponse of each batch to finish.\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-12-Columbia.pdf": {
        "text": "Types of LLM Inference Batching\nStatic Batching\n\u2022 Waits for a fixed number of\nrequests to accumulate\nbefore processing\n\u2022 Processes all requests\nsimultaneously as a single\nbatch\n\u2022 Best suited for offline\nworkloads where latency\nisn't critical\n\u2022 Like a bus that only\ndeparts when completely\nfull\nDynamic Batching\n\u2022 Uses both maximum batch\nsize and time window\nparameters\n\u2022 Processes requests when\neither batch is full or time\nwindow expires\n\u2022 Balances between\nthroughput and latency\n\u2022 Ideal for consistent-length\noutputs like image\ngeneration\nContinuous Batching\n\u2022 Processes requests token-\nby-token at the sequence\nlevel\nDynamically allocates\nGPU resources as tokens\nare generated\n\u2022 Efficiently handles varying\noutput lengths\nOptimizes model weight\nloading across multiple\nrequests\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-12-Columbia.pdf": {
        "text": "Performance Characteristics of Batching\nBatching Type\nStatic\nThroughput\nHighest\nLatency\nUse Case\nHigh\nDynamic\nMedium\nMedium\nContinuous\nHigh\nLow\nOffline processing,\ndaily jobs\nReal-time image\ngeneration\nInteractive LLM\napplications\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-12-Columbia.pdf": {
        "text": "FlashAttention: Fast and\nMemory Efficient Exact\nAttention with IO-Awareness\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R\u00e9\nPaper: https://arxiv.org/abs/2205.14135\n",
        "links": [
            "https://arxiv.org/abs/2205.14135"
        ]
    },
    "./chunks/chunk_15_Lecture-12-Columbia.pdf": {
        "text": "Attention: 10-Aware Tiling Yields Speed, Mem Savings, & Quality\nFlashAttention Speedup\nOuter Loop\nK': dxN\nQ: Nxd\nGPU SRAM: 19 TB/s (20 MB)\nSRAM\nGPU\nHBM\nHBM: 1.5 TB/s (40 GB)\nMain Memory DRAM: 12.8 GB/s\n(CPU DRAM)\nInner Loop\nQKT:NxN\n(>1TB)\nMemory Hierarchy with\nBandwidth & Memory Size\nsm(QKT)V: Nxd\nCopy Block to SRAM\nOuter Loop\nV:NXd\nCopy 1\nCompute Block\non SRAM\n1 Copy\nInner Loop\nOutput to HBM\nInner Loop\nFlashAttention\nOuter Loop\nMemory Reduction ix times less)\nSpeedup (X times foster)\n\u5957\n15\n10\n128 256 512 1024 2048 4096\nSequence Length\nFlashAttention Memory Reduction\n128 256 512 1024 2048 4096\nSequence Length\nModel\nDropout Masking\nMasking Only\nNo Masking. No Drog\nDropout + Masking\nPath-X Path-256\nTransformer\nx\nx\nLinformer [81]\nx\nx\nLinear Attention [48]\nx\nx\nPerformer [11]\nx\nx\nLocal Attention [77]\nx\nx\nReformer [49]\nx\nx\nSMYRF [18]\nx\nx\nFLASHATTENTION\n61.4\nx\nBlock-sparse FLASH ATTENTION\n56.0\n63.1\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-12-Columbia.pdf": {
        "text": "Motivation: Modeling Longer Sequences\nBridging new capabilities\nClosing the reality gap\nOpening new areas\nNLP: Large context required to\nunderstand books, plays,\ninstruction manuals.\nComputer vision: higher\nresolution can lead to better,\nmore robust insight.\nTime series, audio, video,\nmedical imaging data naturally\nmodeled as sequences of\nmillions of steps.\nChallenge: How to scale Transformers to longer sequences?\nBlog post: Can Longer Sequences Help Take the Next Leap in AI? https://ai.stanford.edu/blog/longer-sequences-\nnext-leap-ai/\n",
        "links": [
            "https://ai.stanford.edu/blog/longer-sequencesnext-leap-ai/"
        ]
    },
    "./chunks/chunk_17_Lecture-12-Columbia.pdf": {
        "text": "Background: Attention is Bottlenecked by Memory Reads/Writes\nQ: Nxd K:Nxd A= QKT:NxN\nA = mask(A): NxN\nA = sm(A): NxN\nA = do(A): NxN\nV:Nxd O=AV:Nxd\nX\nt\nAttention Matrix\nMask\nSoftmax\nDropout\nO=Dropout(Softmax(Mask(QKT)))V\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-12-Columbia.pdf": {
        "text": "DATA MOVEMENT IS ALL YOU NEED: A CASE STUDY ON OPTIMIZING\nTRANSFORMERS\n*\n* 1\nAndrei Ivanov\nNikoli Dryden\nTal Ben-Nun\u00b9 Shigang Li\u00b9 Torsten Hoefler \u00b9\nABSTRACT\nTransformers are one of the most important machine learning workloads today. Training one is a very compute-\nintensive task, often taking days or weeks, and significant attention has been given to optimizing transformers.\nDespite this, existing implementations do not efficiently utilize GPUs. We find that data movement is the key\nbottleneck when training. Due to Amdahl's Law and massive improvements in compute performance, training has\nnow become memory-bound. Further, existing frameworks use suboptimal data layouts. Using these insights,\nwe present a recipe for globally optimizing data movement in transformers. We reduce data movement by up to\n22.91% and overall achieve a 1.30\u00d7 performance improvement over state-of-the-art frameworks when training a\nBERT encoder layer and 1.19\u00d7 for the entire BERT. Our approach is applicable more broadly to optimizing deep\nneural networks, and offers insight into how to tackle emerging performance bottlenecks.\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-12-Columbia.pdf": {
        "text": "Background: Attention is Bottlenecked by Memory Reads/Writes\nQ: Nxd K:Nxd\n|A=QK:NxN\nX\nA = mask(A): NxN\nA = sm(A): NxN\nA=do(A): NxN V:Nxd O=AV:Nxd\nt\nAttention Matrix\nMask\nSoftmax\nDropout\nO=Dropout(Softmax(Mask(QKT)))V\nNaive implementation requires\nrepeated R/W from slow GPU HBM\n5\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-12-Columbia.pdf": {
        "text": "A = mask(A): NxN\nA = sm(A): NxN\nA=do(A): NxN\nV:Nxd |O=AV:Nxd\nBackground: Attention is Bottlenecked by Memory Reads/Writes\nQ: Nxd K: Nxd\n|A=QK:NxN\nX\nt\nAttention Matrix\nMask\nSoftmax\nDropout\nAttention on GPT-2\nMatmul\nO=Dropout(Softmax(Mask(QKT)))V\nNaive implementation requires\nrepeated R/W from slow GPU HBM\n15\nDropout\nTime (ms)\n10-\nSoftmax\nFused\n5\nMask Kernel\nMatmul\n0\nPyTorch\nFlashAttention\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-12-Columbia.pdf": {
        "text": "Background: GPU Compute Model & Memory Hierarchy\nStreaming\nMultiprocessors\nStreaming\nMultiprocessors\nGPU SRAM: 19 TB/s (20 MB)\nSRAM\nCompute\nSRAM\nCompute\nSRAM\nCompute\nSRAM\nCompute\nSRAM\nGPU\nHBM\nHBM: 1.5 TB/s (40 GB)\nSlow Data Transfer\nSlow Data Transfer\nMain Memory DRAM: 12.8 GB/s\n(CPU DRAM)\n(>1 TB)\nHBM\nHBM\nGPU #1\nGPU #N\nMemory Hierarchy with\nBandwidth & Memory Size\nBlogpost: Horace He, Making Deep Learning Go Brrrr From First Principles.\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-12-Columbia.pdf": {
        "text": "Background: GPU Compute Model & Memory Hierarchy\nStreaming\nMultiprocessors\nCompute\nSRAM\nCompute\nSRAM\nCompute\nSRAM\nSlow Data Transfer\nHBM\nStreaming\nMultiprocessors\nGPU SRAM: 19 TB/s (20 MB)\nSRAM\nCompute\nSRAM\nGPU\nHBM\nHBM: 1.5 TB/s (40 GB)\nSlow Data Transfer\nMain Memory DRAM: 12.8 GB/s\n(CPU DRAM)\n(>1 TB)\nHBM\nGPU #1\nGPU #N\nMemory Hierarchy with\nBandwidth & Memory Size\nCan we exploit the memory asymmetry to get speed up?\nWith 10-awareness (accounting for R/W to different levels of memory)\nBlogpost: Horace He, Making Deep Learning Go Brrrr From First Principles.\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-12-Columbia.pdf": {
        "text": "How to Reduce HBM Reads/Writes: Compute by Blocks\nChallenges: (1) compute softmax reduction without access to full input.\n(2) backward without the large attention matrix from forward.\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-12-Columbia.pdf": {
        "text": "How to Reduce HBM Reads/Writes: Compute by Blocks\nChallenges: (1) compute softmax reduction without access to full input.\n(2) backward without the large attention matrix from forward.\nTiling: Restructure algorithm to load block by\nblock from HBM to SRAM to compute attention.\nRecomputation: Don't store attn. matrix from\nforward, recompute it in the backward.\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-12-Columbia.pdf": {
        "text": "How to Reduce HBM Reads/Writes: Compute by Blocks\nChallenges: (1) compute softmax reduction without access to full input.\n(2) backward without the large attention matrix from forward.\nTiling: Restructure algorithm to load block by\nblock from HBM to SRAM to compute attention.\nRecomputation: Don't store attn. matrix from\nforward, recompute it in the backward.\nImplementation: fused CUDA kernel for fine-grained control of memory accesses.\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-12-Columbia.pdf": {
        "text": "Tiling\nDecomposing large softmax into smaller ones by scaling.\nsoftmax([A\u2081, A\u2082]) = [a softmax(A\u2081), \u00df softmax(A\u2082)].\nK:dxN\nQ:Nxd\nOuter Loop\nCopy Block to SRAM\nOuter Loop\nsoftmax([A\u2081, A\u2082])\n= a softmax(A\u2081) V\u2081+\u1e9e softmax(A2) V\u2082.\nInner Loop\nV:NXd\nCopy\nCompute Block\non SRAM\n| Copy\nInner Loop\nOutput to HBM\nsm(QKTV: Nxd\nInner Loop\nFlashAttention\nOuter Loop\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-12-Columbia.pdf": {
        "text": "Tiling\nDecomposing large softmax into smaller ones by scaling.\nsoftmax([A\u2081, A\u2082]) = [a softmax(A\u2081), \u00df softmax(A\u2082)].\nK:dxN\nQ:Nxd\nOuter Loop\nCopy Block to SRAM\nOuter Loop\nsoftmax([A1, A2])\nM\n= a softmax(A\u2081) V\u2081 +\u1e9e softmax(A\u2081) V\u2082.\nInner Loop\nCopy\nCompute Block\non SRAM\n| Copy\nInner Loop\n1. Load inputs by blocks from HBM to SRAM.\n2.\nOn chip, compute attention output wrt that block.\n3. Update output in HBM by scaling.\nsm(QKTV: Nxd\nOutput to HBM\nInner Loop\nFlashAttention\nV:NXd\nOuter Loop\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-12-Columbia.pdf": {
        "text": "Recomputation (Backward Pass)\nBy storing softmax normalization factors\nfrom fwd (size N), quickly recompute\nattention in the bwd from inputs in SRAM.\n10\nInner Loop\nK:dxN\nOuter Loop\nCopy Block to SRAM\nOuter Loop\nQ:Nxd\nCopy\nCompute Block\non SRAM\n| Copy\nInner Loop\nOutput to HBM\nsm(QKTV: Nxd\nInner Loop\nFlashAttention\nV:NXd\nOuter Loop\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-12-Columbia.pdf": {
        "text": "Recomputation (Backward Pass)\nBy storing softmax normalization factors\nfrom fwd (size N), quickly recompute\nattention in the bwd from inputs in SRAM.\nAttention\nStandard\nFLASH ATTENTION\nGFLOPS\n66.6\n75.2\nHBM R/W (GB)\n40.3\n4.4\nRuntime (ms)\n41.7\n7.3\nInner Loop\nK: dxN\nOuter Loop\nCopy Block to SRAM\nOuter Loop\nQ:Nxd\nV:NXd\nCopy\nCompute Block\non SRAM\n| Copy\nInner Loop\nOutput to HBM\nsm(QKTV: Nxd\nInner Loop\nFlashAttention\nSpeed up backward pass even with increased FLOPs.\n10\nOuter Loop\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-12-Columbia.pdf": {
        "text": "Extension to Block-Sparse Attention\nS SPARSE V\n11\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-12-Columbia.pdf": {
        "text": "Extension to Block-Sparse Attention\nS SPARSE V\nFwd + Bwd (ms)\nJust skip the zero blocks!\n11\n150\n100\n50\n50\nSparsity Speedup\nDense\nFlashAttention\nBlock-Sparse\nFlashAttention\n20\n60\n% Non-Zero Blocks\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-12-Columbia.pdf": {
        "text": "Attention Module: 2-4x speedup\nBenchmarking runtime against sequence length.\nFlashAttention Speedup, A100\n4+\nm\nSpeedup (X times faster)\n2\nDropout+Masking\nMasking Only\nNo Masking, No Dropout\n0\n128\n256\n512 1024 2048 4096\nSequence Length\n2-4x speedup over naive attention, esp. with dropout/masking\nIn paper: comparison with sparse/approximate attention\n13\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-12-Columbia.pdf": {
        "text": "Faster Transformer Training: MLPerf Record for Training BERT\nMLPerf: (highly optimized) standard benchmark for training speed\nTime to hit an accuracy of 72.0% on MLM from a fixed checkpoint,\naveraged across 10 runs on 8xA100 GPUs\nBERT Implementation\nNvidia MLPerf 1.1 [56]\nTraining time (minutes)\n20.0 \u00b1 1.5\nFLASH ATTENTION (ours)\n17.4 \u00b1 1.4\nFlashAttention outperforms the previous MLPerf record by 15%\nMattson et al. MLSys 2020.\n14\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-12-Columbia.pdf": {
        "text": "Faster Transformer Training: GPT-2\nTraining GPT-2 small and GPT-2 medium from scratch on OpenWebText,\nusing 8xA100 GPUs\nModel implementations\nGPT-2 small - Huggingface [84]\nGPT-2 small Megatron-LM [74]\nGPT-2 small - FLASHATTENTION\nGPT-2 medium - Huggingface [84]\nGPT-2 medium - Megatron-LM [74]\nGPT-2 medium - FLASHATTENTION\nOpenWebText (ppl) Training time (speedup)\n9.5 days (1.0x)\n18.2\n18.2\n4.7 days (2.0x)\n18.2\n2.7 days (3.5x)\n14.2\n21.0 days (1.0x)\n14.3\n11.5 days (1.8x)\n14.3\n6.9 days (3.0x)\nFlashAttention speeds up GPT-2 training by 2.0-3.5x\nLonger sequence length: more speedup\nWolf et al. EMNLP 2020.\nShoeybi et al, arXiv:1909.08053 2019.\n15\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-12-Columbia.pdf": {
        "text": "Attention Module: 10-20x memory reduction\nBenchmarking memory footprint against sequence length\n20\nFlashAttention Memory Reduction\nDropout + Masking\nMemory Reduction (X times less)\n15\n10\nin\n128\n256\n512\n1024 2048 4096\nSequence Length\n10-20x memory saving\nMemory linear in sequence length-scaling up to 64K on one A100\n16\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-12-Columbia.pdf": {
        "text": "Longer Sequences: GPT-2 with Long Context\nTraining GPT-2 small with a longer context\nLonger context: slower, but lower perplexity (better model)\nModel implementations\nGPT-2 small - Megatron-LM\nGPT-2 small - FLASHATTENTION\nGPT-2 small - FLASH\u00c4TTENTION\nGPT-2 small - FLASHATTENTION\nContext length OpenWebText (ppl) Training time (speedup)\nlk\n18.2\n4.7 days (1.0x)\n1k\n18.2\n2.7 days (1.7x)\n2k\n17.6\n3.0 days (1.6x)\n4k\n17.5\n3.6 days (1.3x)\nFlashAttention with context length 4K is faster than\nMegatron-LM at 1K, and achieves lower perplexity\n17\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-12-Columbia.pdf": {
        "text": "Longer Sequences: Path-X, Path-256\nPath-X: Tell whether two dots are connected\ndesigned to push Transformers (no patches)\nRequires sequence length 16K/64K for Path-X/Path-256\nModel\nTransformer\nLinformer [81]\nLinear Attention [48]\nPath-X Path-256\nX\nx\nx\nxx\nPerformer [11]\nx\nx\nLocal Attention [77]\nX\nReformer [49]\nX\nx\nSMYRF [18]\nX\nx\nFLASH ATTENTION\n61.4\nX\n56.0\n63.1\nBlock-sparse FLASHATTENTION\nFlashAttention yields the first\nTransformer that can solve Path-X\nBlock-sparse FlashAttention\nenables Path-256\nTay et al. ICLR 2020.\n18\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-12-Columbia.pdf": {
        "text": "(Attention) KV Cache\nOutput\nInput\nfuture\nLLM\nthe\nIntermediate\n-0.8\nvector repr.\n0.1\n(\"Attention\n0.6\nkey & value\")\nthe\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-12-Columbia.pdf": {
        "text": "(Attention) KV Cache\nOutput\nthe\nLLM\nArtificial Intelligence\nis\nfuture\nLLM\nthe\nIntermediate\n-0.1\n0.3\n0.5\n-0.8\n0.3\n-0.4\n-0.9\nvector repr.\n0.1\n1.2\n-0.7\n1.1\n(\"Attention\n0.6\nkey & value\")\nKV Cache\nInput\nArtificial Intelligence is\nthe\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-12-Columbia.pdf": {
        "text": "KV Cache dynamically grows and shrinks\nOutput\nof\nLUM\nAppended\nArtificial Intelligence is\nthe\nfuture\n-0.1\n0.3\n0.5\n-0.8\n-0.6\n0.3\n-0.4\n-0.9\n0.1\n0.0\n1.2\n-0.7\n1.1\n0.6\n-1.3\nKV Cache\nKV Cache is huge:\n\u2022\nEach token: ~1 MB.\nInput\nfuture\n\u2022\nOne full request: ~several GBs.\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-12-Columbia.pdf": {
        "text": "0\nKV Cache management in previous sytems\nArtificial Intellige is\n3 KV Cache slots for\nrequest A's prompt\n3\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-12-Columbia.pdf": {
        "text": "KV Cache management in previous systems\n0\n3\nArtificial Intellige\nis <resv> <resv>\nnce\nA's max length\n<resv> <resv>\n.\n3 KV Cache slots for\nrequest A's prompt\nPre-allocated slots for A's output\nPre-allocates contiguous space of memory to the request's maximum\nlength\n.\nUseful convention in traditional deep learning workloads where the input/output\nshapes are static (e.g., faster pointer arithmetic, efficient memory access)\nResults in memory fragmentation\nInternal fragmentation due to the unknown output length.\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-12-Columbia.pdf": {
        "text": "KV Cache management in previous systems\n.\n0\n3\nArtificial Intellige\nis\n<resv> <resv>\nnce\nA's max length\n<resv> <resv>\nAlan\nTuring\n3 KV Cache slots for\nrequest A's prompt\nPre-allocated slots for A's output\n(Internal frag.)\nExternal frag.\nRequest B\nPre-allocates contiguous space of memory to the request's maximum\nlength\n\u2022 Useful convention in traditional deep learning workloads where the input/output\nshapes are static (e.g., faster pointer arithmetic, efficient memory access)\nResults in memory fragmentation\n.\nInternal fragmentation due to the unknown output length.\n\u2022 External fragmentation due to non-uniform pre-request max lengths.\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-12-Columbia.pdf": {
        "text": "Significant memory waste in KV Cache space\nOnly 20-40% of KV Cache space is utilized to store actual token\nstates\nKV Cache Internal frag.\nExternal frag. & Others\n100\n8.9\n80\n60\n60\n60\nKV Cache space usage (%)\n40\n40\n36.6\n41.6\n70.6\n25.2\n31.5\n20\n20\n38.2\n26.8\n20.4\n0\nOrca\n(Max)\nOrca\n(Pow2)\nOrca\n(Oracle)\nOurs\nYu, Gyeong-In, et al. \"Orca: A distributed serving system for {Transformer-Based} generative models.\" 16th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 22). 2022.\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-12-Columbia.pdf": {
        "text": "\u2022\nPagedAttention\nApplication-level memory paging and virtualization for attention KV cache\nMemory management in OS\nPagedAttention\nPage 0\nKV Block 0\nProcess\nPage 1\nProcess\nKV Block 1\nRequest\nRequest\nA\nPage 2\nB\nA\nKV Block 2\nB\nPage 3\nKV Block 3\nPage 4\nKV Block 4\nPhysical Memory\nKV Cache\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-12-Columbia.pdf": {
        "text": "Paging KV Cache Space into KV Blocks\nKV Cache\nSpace\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-12-Columbia.pdf": {
        "text": "Paging KV Cache Space into KV Blocks KV blocks\n.\nBlock 0\nKV Cache is a fixed size contiguous\nchunk of memory that can store KV\nBlock 1\ntoken states from left to right\nBlock 2\nBlock 3\nBlock 4\nArtificial Intelligence is the\nBlock 5\nBlock 6\nBlock 7\nBlock size = 4\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-12-Columbia.pdf": {
        "text": "Virtualizing KV Cache\nRequest\nA\nPhysical KV blocks\nblock 0\nblock 1 computer scientist\nPrompt: \"Alan Turing is a computer scientist\"\nblock 2\nblock 3\nLogical KV blocks\nblock 0 Alan\nTuring\nis\na\nblock 4\nblock 1 computer scientist\nblock 2\nblock 5\nblock 6\nblock 3\nblock 7\nAlan Turing is a\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-12-Columbia.pdf": {
        "text": "Virtualizing KV Cache\nRequest\nA\nPrompt: \"Alan Turing is a computer scientist\"\nLogical KV blocks.\nblock 0\nAlan\nTuring\nblock 1 computer scientist\nblock 2\nblock 3\nPhysical KV blocks\nblock 0\nblock 1 computer scientist\nblock 2\nblock 3\nis\nBlock table\nPhysical\na\n# Filled\nblock 4\nblock number\n7\n4\nblock 5\n1\n2\n-\nblock 6\n-\nblock 7\nAlan\nTuring\nis\nS\na\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-12-Columbia.pdf": {
        "text": "Attention mechanism with virtualized KV Cache\n1. Fetch non-continuous KV blocks using the block table\n2. Apply attention operation on the fly\nKey and value vectors\nBlock 1 computer scientist and\nmathe-\nmatician\nBlock 2 renowned for\nQuery\nfor\n10-15% slowdown in GPU kernel\nlatency due to memory indirection\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-12-Columbia.pdf": {
        "text": "Memory management with PagedAttention\n0. Before generation.\nSeq Prompt: \"Alan Turing is a computer scientist\"\nA\nCompletion:\n6639\nBlock 0\nLogical KV cache blocks.\nBlock 1\nBlock 2\nBlock 3\nPhysical KV cache blocks\nBlock 0\nBlock 1\nBlock 2\nBlock table\nPhysical\n# Filled\nBlock 3\nblock no.\nslots\n| | |\nBlock 4\nBlock 5\nBlock 6\nBlock 7\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-12-Columbia.pdf": {
        "text": "Memory efficiency of PagedAttention\nAlan\nTuring\nis\na\n.\nMinimal internal fragmentation\ncomputer\nscientist\nand\nOnly happens at the last block of a sequence\nmathemati\ncian\n.\n# wasted tokens / seq < block size\nrenowned\n\u2022\nSequence: O(100) - O(1000) tokens\n\u25cf\nBlock size: O(10) tokens.\nInternal fragmentation\n\u2022 No external fragmentation\nKV Cache space usage (%)\nKV Cache Internal frag.\nExternal frag. & Others\n100\n8.9\n80\n36.6\n41.6\n60\n70.6\n25.2\n96.3\n2.5-5x improvement\n31.5\n40\n20\n20\n38.2\n26.8\n20.4\n0\nOrca\nOrca\n(Max)\n(Pow2)\nOrca\n(Oracle)\nOurs\n",
        "links": []
    },
    "./chunks/chunk_53_Lecture-12-Columbia.pdf": {
        "text": "Configuring the block size\nLow spatial locality\nThroughput\n1 2 4\n8 16 32 64 128 256\nBlock sizes\n\u2022 Block size 16 works generally well in practice\n.\nLarge internal\nfragmentation\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-12-Columbia.pdf": {
        "text": "Paging enables sharing\nPrompt\nThe future of artificial\nintelligence is\nShared among different outputs\nLLM\nMultiple outputs\n",
        "links": []
    },
    "./chunks/chunk_55_Lecture-12-Columbia.pdf": {
        "text": "Sharing KV blocks\nSample\nA\nSample\nB\n",
        "links": []
    },
    "./chunks/chunk_56_Lecture-12-Columbia.pdf": {
        "text": "Sharing KV blocks\nPhysical KV blocks\nSample\nA\nkey\ntrends\nThe future\nof\ncloud\ncomput-\ning\nis\nlikely\nto\nbe\ncharac-\nterized\nby\nseveral\nSample\nB\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-12-Columbia.pdf": {
        "text": "Sharing KV blocks\nSample\nA\nLogical KV blocks\nPhysical KV blocks\nkey\ntrends\nThe\nfuture\nof\ncloud\nSample\nB\nLogical KV blocks.\nThe\nfuture\nof\ncloud\nThe\nfuture\nof\ncloud\ncomput-\ncomput-\nis\nlikely\nto\nis\nlikely\nto\ning\ncharac-\ncomput-\ning\ning\nis\nlikely to\nbe\nby several\nbe\nterized\ncharac-\nterized\nby several\nkey\ntrends\nkey\ntrends\nbe\ncharac-\nterized\nby several\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-12-Columbia.pdf": {
        "text": "Sharing KV blocks\n0. Shared prompt: Map logical blocks to the same physical blocks.\nPhysical KV cache blocks\nSeq\nA\nLogical KV cache blocks\nThe\nfuture\nof artificial\nThe\nfuture\nof artificial\nintelli-\ngence\nis\nintelli-\ngence\nis\nSeq\nB\nLogical KV cache blocks\nThe\nfuture\nof\nartificial\nintelli-\nAll prompt blocks (except the last)\nare shared across samples\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-12-Columbia.pdf": {
        "text": "More complex sharing: beam search\nShared btw. two beams\nBeam 0\ncreated\nfamous\ncomputer\nShared btw. three beams\nscientist\nwho\nBeam 1\nled\nAlan Turing isa\n\u25cf\nPrompt\nBeam 2\nBritish\nmathemat\nician\nand\ncomputer\nscientist\nSimilar to process tree (fork & kill)\nEfficiently supported by paged attention and copy-on-write mechanism\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-12-Columbia.pdf": {
        "text": "Memory saving via sharing\nMemory Saving (%)\n50\n2652322\n70\n60\n40\n30\n30.5\n25.7\n20\n16.2\n10\nMemory Saving (%)\nHN WA UON\n30\n20\n70-\n60\n66.3\n61.0\n50\n40\n44.3\n10\n0\n0\n2\n4\n6\n2\n#Parallel Decoding Samples\n4\nBeam Search Width\n6\nPercentage = (#blocks saved by sharing) / (#total blocks without sharing)\nOPT-13B on 1x A100-40G with Share GPT dataset\nKwon, Woosuk, et al. \"Efficient memory management for large language model serving with\npagedattention.\" Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-12-Columbia.pdf": {
        "text": "Out of KV Block Memory\nPhysical KV blocks\nThe\nfuture\nof\ncloud\nRequest\nA\nmathe-\ncomputer scientist and\nmatician\ncomputing is\nlikely\nto\nrenowned for\nRequest\nB\nAlan\nTuring\nis\na\nCannot allocate a new\nphysical block for\nRequest B\n",
        "links": []
    },
    "./chunks/chunk_62_Lecture-12-Columbia.pdf": {
        "text": "Request Preemption & Recovery\nGoal: Free some requests' KV cache to let others run first.\nOption 1: Swapping\nCPU\nSwap to\nCPU\nOption 2: Recomputation\nDelete\nGPU\nSwap back\nto GPU\nGPU\nRecompute\n",
        "links": []
    },
    "./chunks/chunk_63_Lecture-12-Columbia.pdf": {
        "text": "Notes on Preemption & Recovery\n.\nSwap/recompute the whole request\nsince all previous tokens are\nrequired every step.\n\u2022 Swapping: smaller block size \u2192\nhigher overhead due to small data\ntransfers.\n.\n\u2022 Recomputation: surprisingly fast\nsince all token's KV cache can be\ncomputed in parallel.\nTime (ms)\n140\n120\n100\n80\n869\n60\n40\n20\n0\nRecompute\nSwap in\nSwap out\nSwap in + out\n1 2 4 8 16 32 64 128 256\nBlock size\nFigure: Swap/Recomputation\nlatency of 256 tokens.\nvLLM strategy: Use recomputation when possible with FCFS policy.\n",
        "links": []
    },
    "./chunks/chunk_64_Lecture-12-Columbia.pdf": {
        "text": "Comparison with Operating System Virtual Memory\nAnalogies\nOS pages KV blocks\n\u2022 Reduce memory\nfragmentation\nShared pages across processes\n>>> Shared KV blocks across\nsamples\nReduce memory waste via\nsharing\nDifferences\nSingle-level block table\n\u2022 Block table is tiny compared to\nthe actual data.\nPreemption & Recovery\n.\n\u2022 Request-level preemption\nRecomputation-based\nrecovery\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-12-Columbia.pdf": {
        "text": "LLM Distributed System Architecture & Implementation\nvLLM Engine\nScheduler\nBlock Manager\nBlock tables\nCPU Block\nAllocator\nGPU Block\nAllocator\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-12-Columbia.pdf": {
        "text": "vLLM Distributed System Architecture & Implementation\nvLLM Engine\nScheduler\nWorker 1\nCache\nModel\nEngine\nShard 1\nVLLM\n(13K LoC in Python, 3K LoC in C++/CUDA)\nWorker 2\nBlock Manager\nCache\nEngine\nModel\nShard 2\nMegatron-LM\nO PyTorch\nCustom Ops\nBlock tables\nWorker N\nCPU Block\nAllocator\nGPU Block\nAllocator\nCache\nEngine\n\u101d\u1004\u103a\nog RAY\nModel\nShard 3\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-12-Columbia.pdf": {
        "text": "Evaluation \u2015 Settings\n-\nMetric: Serving throughput\nInput/Output Length Distribution\n\u2022 Alpaca dataset (instruction-following)\nShareGPT dataset (conversation)\nBaselines\nNVIDIA FasterTransformer (FT)\nOrca\nOracle: No over-reserve and know exact output lengths.\nPow2: Over-reserve the space for outputs by at most 2x.\nMax: Over-reserve to the maximum possible output length.\n",
        "links": []
    },
    "./chunks/chunk_68_Lecture-12-Columbia.pdf": {
        "text": "Throughput (req/s)\nThroughput\u2015 Greedy Decoding\n2.00\n1.9\n1.75\n1.50\n2.4x speedup\n1.25\n1.1\n1.00\n0.8\n0.75\n# Batched requests\n332222\n35\n30\n30.42\n25\n20\n15\n13.62\n9.81\n10\n7.00\n5\n0.5\n0.50\nOrca\nOrca\nOrca\nVLLM\n0.2\n(Max) (Pow2) (Oracle)\n0.25\n0.00\nFT\nOrca Orca Orca VLLM\n(Max) (Pow2) (Oracle)\nAverage number of batched requests.\nOPT-13B on 1xA100 40G with Share GPT trace.\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-12-Columbia.pdf": {
        "text": "Throughput (req/s)\nThroughput-Beam Search\n30\n25\n28.0\n17.5\n17.0\n10\n10\n15.0\n8\n21.0\n16.0\n12.5\n6\n10.0\n9.0\n7.5\n7.0\n4\n10\n3.0\n5.0\n2\n5\n4.0\n2.5\n1.2\n0.2\n0\n0.0\n0\nOrca\nOrca\n(Max) (Pow2) (Oracle)\nOrca\nVLLM\nOrca Orca\nOrca\n(Max) (Pow2) (Oracle)\nVLLM\nOrca Orca Orca\n(Max) (Pow2) (Oracle)\nVLLM\nNo beam search\n1.8x speedup\nBeam width = 2\nBeam width = 4\n2.4x speedup\n3.2x speedup\n9.5\n7.0\n7\n6\n5\n5.0\n4\n3\n3.0\n2.0\n2\n1\n0.2\nOrca\nOrca Orca\n(Max) (Pow2) (Oracle)\nVLLM\nBeam width = 6\n3.5x speedup\nOPT-13B on 1xA100 40G with Alpaca trace.\nSpeedup: vLLM v.s. Orca(Pow2)\n",
        "links": []
    },
    "./chunks/chunk_70_Lecture-12-Columbia.pdf": {
        "text": "GitHub Stars\nvLLM Open-Source Adoption\ngithub.com/vllm-project/vllm\n$ pip install vllm\n8.8K Stars\n95+ Contributors\nvilm-project/vilm\n8.0K\n6.0K\n\u0447\u043e\u043a\n2.0k\nV Star History\nJune\nJuly\nAugust\nDate\nSeptember\nOctober\nstar-history.com\nOpen-Source Projects\nIm-sys/FastChat\nAi2 allenai/open-instruct\nCompanies\n\u042f anyscale\nc3.ai\nBentoML\ndatabricks\nGoogle Cloud\nLepton Al\nMeta\nMicrosoft\nModal\nNVIDIA replicate\nscale\nTwelve Labs\nUber\n",
        "links": []
    },
    "./chunks/chunk_71_Lecture-12-Columbia.pdf": {
        "text": "VLLM\n\u2022 Improve memory efficiency by 2.5x-5x by reducing\nfragmentation and enabling sharing with PagedAttention\n\u2022 Outperform SOTA by up to 4x in terms of serving throughput\nA vibrant open-source project that gets widely adopted\nhttps://github.com/vllm-project/vllm\nBLOG https://vllm.ai\n:\nhttps://arxiv.org/abs/2309.06180\nhttps://discord.gg/jz7wjKhh6g\n",
        "links": [
            "https://github.com/vllm-project/vllm",
            "https://vllm.ai",
            "https://arxiv.org/abs/2309.06180",
            "https://discord.gg/jz7wjKhh6g"
        ]
    },
    "./chunks/chunk_72_Lecture-12-Columbia.pdf": {
        "text": "Text Generation Inference\nFast optimized inference for LLMs\nR Web Server\ngRPC\nModel Shard\n/generate\n/generate\ngRPC\nModel Shard\nBuffer\nBatcher\n/generate\ngRPC\nModel Shard\nNVIDIA GPUS, AMD GPUs,\nInferentia2 or Gaudi2\nNCCL\n(or rccl, etc.)\n",
        "links": []
    },
    "./chunks/chunk_73_Lecture-12-Columbia.pdf": {
        "text": "microsoft/\nDeepSpeed\nDeepSpeed is a deep learning optimization library\nthat makes distributed training and inference easy,\nefficient, and effective.\n\u042f315\nContributors\n6k\n193\n32k\n4k\nUsed by\nDiscussions\nStars\nForks\nC\n",
        "links": []
    },
    "./chunks/chunk_74_Lecture-12-Columbia.pdf": {
        "text": "NVIDIA\nTensorRT\nWhat is TensorRT?\n\u2022 High-performance C++ library for GPU-accelerated inference\n\u2022 Optimizes trained neural networks for faster execution on\n.\nNVIDIA GPUs\n\u2022 Supports models from major frameworks like TensorFlow,\nPyTorch, and ONNX\nKey Features\n\u2022 Graph optimizations and layer fusion\n\u2022 Mixed precision capabilities (FP32, FP16, INT8, INT4)\n.\n\u2022 Automatic performance tuning\n\u2022Tensor and layer parallelism\n\u2022 Support for various NVIDIA GPU architectures\nPerformance Benefits\n\u2022 Up to 36X faster inference compared to CPU-only platforms\n\u2022 Significant latency reduction for real-time applications\n\u2022 Improved throughput for data center deployments\n",
        "links": []
    },
    "./chunks/chunk_75_Lecture-12-Columbia.pdf": {
        "text": "SLORA: Scalable Serving of\nThousands of LoRA Adapters\n",
        "links": []
    },
    "./chunks/chunk_76_Lecture-12-Columbia.pdf": {
        "text": "Motivation: Customized Serving\nExpert Musician\nMusic Theory geek and\ngenius\nLife Coach\nwell-being & spirituality\nSop\n6.2m\nwan\nPair Programmer\nYour programming Al\nassistant\n696.2k\nlondon\n3.5m\ncharacter.ai\n",
        "links": []
    },
    "./chunks/chunk_77_Lecture-12-Columbia.pdf": {
        "text": "Motivation: Customized Serving\n\u27a4 Personalized writing style.\n\u27a4 Personal habits and preferences\n\u27a4 Unique set of data\n",
        "links": []
    },
    "./chunks/chunk_78_Lecture-12-Columbia.pdf": {
        "text": "Naive: Full Fine-tuning for Each User\nModel 1\nModel 2\nModel 3\nModel n\n",
        "links": []
    },
    "./chunks/chunk_79_Lecture-12-Columbia.pdf": {
        "text": "Naive: Full Fine-tuning for Each User\nNot every user is active all the time\nModel 1\nModel 2\nModel 3\nExpensive\nModel n\n",
        "links": []
    },
    "./chunks/chunk_80_Lecture-12-Columbia.pdf": {
        "text": "Naive: Full Fine-tuning for Each User\nNot every user is active all the time\nHow do we let users with different\nneeds share the resources?\nModel 1\nModel 2\nModel 3\nExpensive\nModel n\n",
        "links": []
    },
    "./chunks/chunk_81_Lecture-12-Columbia.pdf": {
        "text": "Desired: Share One Foundational Model\nBase Model\n",
        "links": []
    },
    "./chunks/chunk_82_Lecture-12-Columbia.pdf": {
        "text": "Background: Low-Rank Adaptation (LORA)\nBase Model\n+ Adapter 1\nCustomize for User 1\n",
        "links": []
    },
    "./chunks/chunk_83_Lecture-12-Columbia.pdf": {
        "text": "Background: Low-Rank Adaptation (LORA)\nBase Model\n+\nAdapter 1\nCustomize for User 1\nModel 1\nModel 2\nModel 3\nModel 4\n\u2191\nBase Model\nAdapter 1\nAdapter 3\nAdapter 2\nAdapter 4\n",
        "links": []
    },
    "./chunks/chunk_84_Lecture-12-Columbia.pdf": {
        "text": "Background: Low-Rank Adaptation (LORA)\nB (r*h)\nPretrained\nWeights\nW (h*h)\nh\nA (h*r)\nOnly update low-rank additive\nmatrices (a small number of\nparams)\n",
        "links": []
    },
    "./chunks/chunk_85_Lecture-12-Columbia.pdf": {
        "text": "Background: Low-Rank Adaptation (LORA)\nPretrained\nWeights\nB (r*h)\nr\nW (h*h)\nh\nA (h*r)\n\u2022\nOnly update low-rank additive\nmatrices (a small number of\nparams)\nPerformance on par with full-\nweight fine-tuning.\n",
        "links": []
    },
    "./chunks/chunk_86_Lecture-12-Columbia.pdf": {
        "text": "Background: Low-Rank Adaptation (LORA)\nh = xW' = x(W + AB)\n(1)\n= xW +xAB.\n(2)\nB (r*h)\nPretrained\nWeights\nr\nW (h*h)\nh\nA (h*r)\n\u2022\nOnly update low-rank additive\nmatrices (a small number of\nparams)\nPerformance on par with full-\nweight fine-tuning.\n",
        "links": []
    },
    "./chunks/chunk_87_Lecture-12-Columbia.pdf": {
        "text": "Use Low-Rank Adaption (LORA)\nLORA\nAdapter 1\nLORA\nAdapter 2\nLORA\nAdapter 3\nLORA\nAdapter n\nAdapter:\nMuch smaller than\nthe base model\nBase Model\n8\n",
        "links": []
    },
    "./chunks/chunk_88_Lecture-12-Columbia.pdf": {
        "text": "Use Low-Rank Adaption (LORA)\nLORA\nAdapter 1\nLORA\nAdapter 2\nLORA\nAdapter 3\nLORA\nAdapter n\nAdapter:\nMuch smaller than\nthe base model\nBase Model\n8\n",
        "links": []
    },
    "./chunks/chunk_89_Lecture-12-Columbia.pdf": {
        "text": "Use Low-Rank Adaption (LORA)\nAdapter:\nLORA\nAdapter 1\nLORA\nAdapter 2\nLORA\nAdapter 3\nMuch smaller than\nthe base model\nBase Model\nLORA\nAdapter n\nHow to store them?\n8\n",
        "links": []
    },
    "./chunks/chunk_90_Lecture-12-Columbia.pdf": {
        "text": "Use Low-Rank Adaption (LORA)\nLORA\nAdapter 1\nLORA\nAdapter 2\nLORA\nAdapter 3\nLORA\nAdapter n\nBase Model\nthe\nHow to batch\ncomputation?\n",
        "links": []
    },
    "./chunks/chunk_91_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nCPU/GPU Offloading\nUnified Memory\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\n\u27a4 Adapters cannot fit on one GPU\n\u27a4 Or they occupy significant memory\n10\n",
        "links": []
    },
    "./chunks/chunk_92_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nCPU/GPU Offloading\nUnified Memory\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\n\u27a4 Adapters cannot fit on one GPU\n\u27a4 Or they occupy significant memory\nShrink the memory for the KV Cache\nLimit the batch size\n10\n",
        "links": []
    },
    "./chunks/chunk_93_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nMain Memory\nCPU/GPU Offloading\nAdapter 1\nAdapter 2\nfor dynamic tensors\nAdapter 3\nAdapter 4\nUnified Memory\nAdapter 5\nAdapter 6\nGPU Memory\nUnified memory pool\nKV cache\nBase\nModel\nWeights\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\nFetch active\nAdapter 2\nOther\nadapters for\nthe current batch\nAdapter 5\nTemporary\nTensors\n11\n",
        "links": []
    },
    "./chunks/chunk_94_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nMain Memory\nCPU/GPU Offloading\nAdapter 1\nAdapter 2\nfor dynamic tensors\nAdapter 3\nAdapter 4\nUnified Memory\nAdapter 5\nAdapter 6\nGPU Memory\nUnified memory pool\nKV cache\nBase\nModel\nWeights\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\nFetch active\nadapters for\nthe current batch\nAdapter 2\nAdapter 5\nOther\nTemporary\nTensors\nActive Adapters\n11\n",
        "links": []
    },
    "./chunks/chunk_95_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nMain Memory\nCPU/GPU Offloading\nAdapter 1\nAdapter 2\nfor dynamic tensors\nAdapter 3\nAdapter 4\nUnified Memory\nAdapter S\nAdapter 6\nGPU Memory\nUnified memory pool\nKV cache\nBase\nModel\nWeights\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\nFetch active\nAdapter 2\nadapters for\nthe current batch\nAdapter 5\nOther\nTemporary\nTensors\n12\n",
        "links": []
    },
    "./chunks/chunk_96_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nMain Memory\nCPU/GPU Offloading\nAdapter 1\nAdapter 2\nfor dynamic tensors\nAdapter 3\nAdapter 4\nUnified Memory\nAdapter S\nAdapter 6\nGPU Memory\nUnified memory pool\nKV cache\nBase\nModel\nWeights\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\nFetch active\nadapters for\nthe current batch\nAdapter 2\nAdapter 5\nOther\nTemporary\nTensors\nMemory use for KV cache and adapters are dynamic\n12\n",
        "links": []
    },
    "./chunks/chunk_97_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nMain Memory\nCPU/GPU Offloading\nAdapter 1\nAdapter 2\nfor dynamic tensors\nAdapter 3\nAdapter 4\nUnified Memory\nAdapter 5\nAdapter 6\nGPU Memory\nUnified memory pool\nKV cache\nBase\nModel\nWeights\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\nFetch active\nadapters for\nthe current batch\nAdapter 2\nAdapter 5\nOther\nTemporary\nTensors\nMemory use for KV cache and adapters are dynamic\n\u27a4 Separate memory pool cause more fragmentation\n12\n",
        "links": []
    },
    "./chunks/chunk_98_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nMain Memory\nCPU/GPU Offloading\nAdapter 1\nAdapter 2\nfor dynamic tensors\nAdapter 3\nAdapter 4\nUnified Memory\nAdapter 5\nAdapter 6\nGPU Memory\nUnified memory pool\nKV cache\nBase\nModel\nWeights\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\nFetch active\nadapters for\nthe current batch\nAdapter 2\nAdapter 5\nOther\nTemporary\nTensors\nMemory use for KV cache and adapters are dynamic\n\u27a4 Separate memory pool cause more fragmentation\nUnified memory for dynamic tensors: Memory fragmentation -> 0\n12\n",
        "links": []
    },
    "./chunks/chunk_99_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nCPU/GPU Offloading\nUnified Memory\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\nH\nKV caches\nAdapter weights\n\u2610 Empty\nDynamic memory size for KV caches and adapters\n",
        "links": []
    },
    "./chunks/chunk_100_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nCPU/GPU Offloading\nUnified Memory\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\nH\nKV caches\nAdapter weights\n\u2610 Empty\nDynamic memory size for KV caches and adapters\nPaged KV cache (PagedAttention (11) -> Unify paged KV cache and paged adapter weights\n[1] VLLM: Woosuk Kwon, Zhuohan Li\", Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, lon Stoica. SOSP'23\n",
        "links": []
    },
    "./chunks/chunk_101_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nRequest 1\nAdapter 1\nRequest 2\nAdapter 2\nCPU/GPU Offloading\nRequest 3.\nAdapter 3\nUnified Memory\nHeterogeneous Batching\nDifferent adapters\nDifferent ranks\n+++\nBase Model\nBase Model\nBase Model\n14\n",
        "links": []
    },
    "./chunks/chunk_102_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nRequest 1\nAdapter 1\nRequest 2\nAdapter 2\nCPU/GPU Offloading\nRequest 3\nAdapter 3\nUnified Memory\nHeterogeneous Batching\nDifferent adapters\nRequest 1\nDifferent ranks\nAdapter 1\nRequest 2\nAdapter 2\n+\nRequest 3\nAdapter 3\n+++\nRequest 1\nBase Model\nBase Model\nBase Model\nRequest 2\nBase Model\nRequest 3\n14\n",
        "links": []
    },
    "./chunks/chunk_103_Lecture-12-Columbia.pdf": {
        "text": "Batching different adapters use customized kernel\n(extend from punica)\nBatched base computation\nx1\nx2\nX\nW\nx3\nBatched LORA computation\nX\nx1\nX\nX\n1\nx3\nX\nN\nA\n3\nB2\nX\nB3\nadd\n",
        "links": []
    },
    "./chunks/chunk_104_Lecture-12-Columbia.pdf": {
        "text": "Batching different adapters use customized kernel\n(extend from punica)\nBatched base computation\nx1\nx2\nX\nW\nx3\nBatched LORA computation\nX\nx1\nX\nX\n1\nx3\nX\nN\nA\n3\nX\nB2\nX\nB3\nadd\n",
        "links": []
    },
    "./chunks/chunk_105_Lecture-12-Columbia.pdf": {
        "text": "Batching different adapters use customized kernel\n(extend from punica)\nBatched base computation\nx1\nx2\nX\nW\nx3\nBatched LORA computation\nX\nx1\nX\nX\n1\nN\nA\nB2\nx3\nX\nX\nB3\n3\nadd\nExample: xA for decode\nx: [B, H], A_i: [H, R_i]\n",
        "links": []
    },
    "./chunks/chunk_106_Lecture-12-Columbia.pdf": {
        "text": "Batching different adapters use customized kernel\n(extend from punica)\nBatched base computation\nx1\nx2\nX\nw\nx3\nBatched LORA computation\nx1\nx\nx\nx3\nX\nA3\nX\nB2\nx\nB3\nExample: xA for decode\nx: [B, H], A_i: [H, R_i]\nX\nadd\nIndex of A\n",
        "links": []
    },
    "./chunks/chunk_107_Lecture-12-Columbia.pdf": {
        "text": "Batching different adapters use customized kernel\n(extend from punica)\nBatched base computation\nx1\nx2\nX\nW\nx3\nBatched LORA computation\nx1\nx\nx\nx3\nx\nA3\nX\nB2\nx\nB3\nExample: xA for decode\nx: [B, H], A_i: [H, R_i]\nX\nadd\n@A\nIndex of A\nParallel each H*H vector-vector multiplication\n",
        "links": []
    },
    "./chunks/chunk_108_Lecture-12-Columbia.pdf": {
        "text": "Batching different adapters use customized kernel\n(extend from punica)\nBatched base computation\nx1\nx2\nX\nW\nx3\nBatched LORA computation\nx1\nx\nx\nx3\nX\nAM\n3\nx\nB2\nX\nB3\nExample: xA for decode\nx: [B, H], A_i: [H, R_i]\nX\nadd\n@ A\nIndex of A\nGrid [max (R_i), B]\n",
        "links": []
    },
    "./chunks/chunk_109_Lecture-12-Columbia.pdf": {
        "text": "S-LORA: Serving thousands of adapters on a single GPU\nPerformance of Serving Thousands of LORA Adapters.\n(single A100 GPU, Llama-7B base model)\n\u27a4Serve 100x more users\n1000x users \"for free\"\n10\nThroughput (req/s)\n5\nx\n100\nx\n1000\nNumber of adapters\nHuggingFace PEFT VLLM-packed \u25a0S-LORA\n2000\n\u2713 8\n20\n",
        "links": []
    },
    "./chunks/chunk_110_Lecture-12-Columbia.pdf": {
        "text": "Evaluation\n1.5\n1.0\n0.5\n0.0\n60.0\n40.0\n20.0\n0.0\nS1 (Llama-7b)\nA10G (24GB)\n100\n200\n100\nSLORA\nSLORA-bmm\nSLORA-no-unify-mem\n52 (Llama-7b)\nS4 (Llama-13b)\nA10G (24GB)\nA100 (40GB)\n52 (Llama-7b)\nA100 (80GB)\nS4 (Llama-13b)\nA100 (80GB)\n8.0\n1.5\n4.0\n1.5\n6.0\n3.0\n1.0\n1.0\n4.0\n2.0\n0.5\n0.5\n2.0\n1.0\n0.0\n0.0\n100\n200\n0.0\n200\n400\n0.0\n100\n200\n100\n200\n200.0\n100.0\n75.0\n50.0\n50.0\n25.0\n0.0\n200\n100\n200\n0.00\nNumber of Adapters\n150.0\n150.0\n100.0\n100.0\n50.0\n100\nNumber of Adapters\n200\n0.00\nNumber of Adapters\nReal world trace give similar results\n100\nNumber of Adapters\n200.0\n100.0\n200\n0.00\n200\n400\nNumber of Adapters\n",
        "links": []
    },
    "./chunks/chunk_111_Lecture-12-Columbia.pdf": {
        "text": "Tensor Parallelism Base: 2(N-1)/N * Bh >> Adapter: 3(N-1)/N * Br + 2(N-1)/N * Br\ninput X\n(B, h)\n1\n(B, 1/N)\nColumn Partition\nRow Partition\nPartial Sum\nReplication\n(h, d/N)\nW1\n(d/N, 1) I\nW2\nBase model\n(B. d/N)\n(B. h)\nmatmul 1\nmatmul 2\n(B, d/N)\nadd 1\n(h, r/N)\nB1\n(d/N, )\n82\nA1\nA2\n(r, d/N)\n(t, h/N)\n(B. 1)\nLORA adapters\n(B, r)\n\uba38\nmatmul 3\n(B,)\nmatmul 4\nmatmul all-gather\n(B, d/N)\nmatmul all-reduce\n(B. 11/N)\nadd 2\nall-reduce\n(8, h)\n(8, h)\n",
        "links": []
    },
    "./chunks/chunk_112_Lecture-12-Columbia.pdf": {
        "text": "Tensor Parallelism Base: 2(N-1)/N Bh >> Adapter: 3(N-1)/N *Br + 2(N-1)/NBr.\ninput X\n(B, b)\n(B, /N)\nColumn Partition\nRow Partition\nPartial Sum\nReplication\n(h, d/N)\nW1\n(d/N, h) I\nW2\nBase model\n(B. d/N))\n(B. 1)\nmatmul 1\nmatmul 2\n(B. d/N)\nadd 1\nadd_2\nall-reduce\n(8, h)\n(8, b]\n(h, /N)\nB1\n(d/N. r)\n82\nA1\nA2\n(r. d/N)\n(ch/N)\n(B. 1)\nLORA adapters\nFuse all-gather\n(B,r)\nand all-reduce\nmatmul 3\n(8,1)\nmatmul 4\nmatmul all-gather\n(B, d/N)\nmatmul all-reduce\n(B, h/N)\nSmall tensors\nwith rank r << d\n",
        "links": []
    },
    "./chunks/chunk_113_Lecture-12-Columbia.pdf": {
        "text": "Tensor Parallelism\nThroughput (req/s)\nSLORA\nSLORA (w/o LORA communication)\n4\n3.5\n3.0\n2.5\n3\n2.0\n1.5\n1.0\n0.0\nLlama-30B Llama-30B\n2xA100(40GB) 2xA100(40GB)\nn=10\nn=10\nLlama-30B Llama-30B\n2xA100(40GB) 4xA100(40GB)\nn=100\nn=100\nSLORA (base only)\nLlama-70B Llama-70B\n2xA100(80GB) 4xA100(80GB)\nn=10\nn=10\n",
        "links": []
    },
    "./chunks/chunk_114_Lecture-12-Columbia.pdf": {
        "text": "Conclusion\n\u2022 SLORA: a system that can serve 1000x adapters with much higher\nthroughput.\n\u2022 Unified memory pool, heterogeneous batching, S-LORA tensor\nparallelism.\n\u26ab Batching different adapters improve the throughput by up to 4x, and\nincrease the number of served adapters by several orders of magnitude.\n",
        "links": []
    },
    "./chunks/chunk_115_Lecture-12-Columbia.pdf": {
        "text": "vilm-project/vllm\n#4068 [Feature]: Allow\nLORA adapters to be\nspecified as in-memor...\n5 comments\njacobthebanana opened on April 14, 2024\nC\n",
        "links": []
    },
    "./chunks/chunk_116_Lecture-12-Columbia.pdf": {
        "text": "vLLM vs\nsLoRA in\nserving LoRA\nadapters\nAdapter Management\n\u2022 VLLM: Merges LoRA weights into base model\n\u2022 S-LORA: Stores adapters in main memory, fetches as needed\nMemory Efficiency\n\u2022 VLLM: Limited to <5 adapters in GPU due to GPU memory constraints, but can\nswap adapters between GPU and CPU memory\n\u2022 S-LORA: Can serve thousands of adapters on a single GPU\nPerformance\n\u2022 VLLM: Lower throughput, limited adapter capacity\n\u2022 S-LORA: Up to 4x higher throughput, stable with increasing adapters\nParallelism\n\u2022 VLLM: Separate processes for each adapter\n\u2022 S-LORA: Novel tensor parallelism strategy (S-LORA TP)\nOptimization\n\u2022 vLLM: No specific LoRA optimizations\n\u2022 S-LORA: Custom CUDA kernels for batched LORA computation\n",
        "links": []
    },
    "./chunks/chunk_117_Lecture-12-Columbia.pdf": {
        "text": "Why Share GPUs?\nIncrease Utilization\nDecrease Costs\nMore Efficient Use of GPUs \u2192 Fewer GPUs to Get Work Done\u2192\u2192 Lower Costs\n",
        "links": []
    },
    "./chunks/chunk_118_Lecture-12-Columbia.pdf": {
        "text": "Workload 1\nSpace\nGPU\nWorkload 2\nSpace vs.\nTime\nPartitioning\nAdvantages:\n\u2022 Fully resident at all times.\nNo context switch overhead\nPredictable performance\nDisadvantages:\nSubset of GPU resources\n\u2022 Limited number of clients\n7\n",
        "links": []
    },
    "./chunks/chunk_119_Lecture-12-Columbia.pdf": {
        "text": "Space vs Time Partitioning\nWorkload 1\nSpace\nGPU\nWorkload 2\nTime\nWorkload 1\nGPU\nAdvantages:\n\u2022 Fully resident at all times\nNo context switch overhead\nPredictable performance\nDisadvantages:\nSubset of GPU resources\nLimited number of clients\nAdvantages:\n\u2022 Full set of GPU resources\nUnlimited number of clients\nDisadvantages:\nOnly resident a fraction of the time\n\u2022 Additional context switch overhead\nUnpredictable performance\n",
        "links": []
    },
    "./chunks/chunk_120_Lecture-12-Columbia.pdf": {
        "text": "Space\nSpace vs Time Partitioning\nWorkload 1\nWorkload 2\nGPU\n,\u05d9\nTime\nWorkload 1 \u2192 2 \u2192 1 ...\nGPU\nAdvantages:\n\u2022\nFully resident at all times\nNo context switch overhead\nPredictable performance\nDisadvantages:\nSubset of GPU resources\nLimited number of clients\nAdvantages:\nFull set of GPU resources\nUnlimited number of clients\nDisadvantages:\n\u2022\nOnly resident a fraction of the time\nAdditional context switch overhead\nUnpredictable performance\n",
        "links": []
    },
    "./chunks/chunk_121_Lecture-12-Columbia.pdf": {
        "text": "Space vs Time Partitioning\nSpace\nWorkload 1\nAdvantages:\nGPU\nWorkload 2\nFully resident at all times\nNo context switch overhead\nPredictable performance\nDisadvantages:\n\u2022\nSubset of GPU resources\nLimited number of clients\n\u0f0b\nL\nTime\nWorkload 1 \u21922\u2192 1 ...\nGPU\nAdvantages:\n\u2022 Full set of GPU resources\nUnlimited number of clients\nDisadvantages:\nOnly resident a fraction of the time\nAdditional context switch overhead\nUnpredictable performance\n",
        "links": []
    },
    "./chunks/chunk_122_Lecture-12-Columbia.pdf": {
        "text": "Space vs Time Partitioning\nWorkload 1\nSpace\nGPU\nWorkload 2\n\u3001\nTime\nWorkload 1 \u21922\u2192 1 ..\nGPU\nAdvantages:\nFully resident at all times\nNo context switch overhead\nPredictable performance\nDisadvantages:\nSubset of GPU resources\nLimited number of clients\nAdvantages:\nFull set of GPU resources\nUnlimited number of clients\nDisadvantages:\nOnly resident a fraction of the time\nAdditional context switch overhead\nUnpredictable performance\nInherent Tradeoff in Choosing One Strategy over the Other\n",
        "links": []
    },
    "./chunks/chunk_123_Lecture-12-Columbia.pdf": {
        "text": "Hardware vs.\nSoftware-\nBased Space\nPartitioning\nBU\nMulti-Instance GPU (MIG)\nUSERO\nGPU Instance 0\nUSER1\nGPU Instance 1\nUSER2\nGPU Instance 2\nUSER3\nGPU\nGPU Instance 3\nUSER4\nGPU Instance 4\nUSERS\nGPU Instance 5\nSys\nPipe\nUSER6\nGPU Instance 6\nWE\nControl\nXbar\nPhysical partitioning\nFull isolation between workloads\nSMS\nData\nXbar\nL2\nDRAM\nDRAM\n",
        "links": []
    },
    "./chunks/chunk_124_Lecture-12-Columbia.pdf": {
        "text": "GPU\nBLO\nHardware vs. Software-Based Space Partitioning\nMulti-Instance GPU (MIG)\nUSERO\nGPU Instance 0\nUSER1\nGPU Instance 1\nUSER2\nGPU Instance 2\nUSER3\nGPU Instance 3\nUSER4\nGPU Instance 4\nUSERS\nGPU Instance 5\nSys\nPipe\nControl\nXbar\nControl\nXbar\nSMS\nXhar\nData\nXhar\nData\nelen\nXhar\nXbar\nL2\nL2\nL2\nL2\n717\nL2\nDRAM\nDRAM\nDRAM\nDRAM\nDRAM\nDRAM\nDRAM\nMulti-Process Service (MPS)\nA B C\nCUDA MULTI-PROCESS SERVICE CONTROL\nGPU MULTI-PROCESS SERVICE\nUSER6\nGPU Instance 6\nPhysical partitioning\nFull isolation between workloads\nGPU\nLogical partitioning\nLimited isolation between workloads\n",
        "links": []
    },
    "./chunks/chunk_125_Lecture-12-Columbia.pdf": {
        "text": "Hardware vs. Software-Based Space Partitioning\nMulti-Instance GPU (MIG)\nAdvantages:\nFull Fault Isolation\nGuaranteed Memory Bandwidth QoS\nSuitable for Multi-Tenant Environments\nDisadvantages:\nFixed Size Partitions (multiple dimensions)\nMulti-Process Service (MPS)\nAdvantages:\nFlexible Partition Size (multiple dimensions)\nDisadvantages:\nLimited Fault Isolation\n\u2022 No Memory Bandwidth QoS\nNot Suitable for Multi-Tenant Environments\nCPU ance\nOPU Stance 1\nUTERY\nPU3\nCPU Stanco 4\nGPU tance-\nS\nA\nB C\nGPU MULTI-PROCESS SERVICE\nGPU\n",
        "links": []
    },
    "./chunks/chunk_126_Lecture-12-Columbia.pdf": {
        "text": "Hardware vs. Software-Based Space Partitioning\nMulti-Instance GPU (MIG)\nAdvantages:\nFull Fault Isolation\nGuaranteed Memory Bandwidth QoS\nSuitable for Multi-Tenant Environments\nDisadvantages:\nFixed Size Partitions (multiple dimensions)\nMulti-Process Service (MPS)\nAdvantages:\nFlexible Partition Size (multiple dimensions)\nDisadvantages:\nLimited Fault Isolation\nNo Memory Bandwidth QoS\nNot Suitable for Multi-Tenant Environments\nB C\nCPU ance\nCPU1\nUTERY\nOPU V3-\nLARKS\nCPU-\nCPU\nCPU Inc\nLDERS\nUTARN\nCPU c\nCUBA MULTIPROCESS SERVICE CONTROL\nGPU MULTI-PROCESS SERVICE\nGPU\n",
        "links": []
    },
    "./chunks/chunk_127_Lecture-12-Columbia.pdf": {
        "text": "\u2022\n\u2022\nMotivation\nGPUs are\nidling\nfor\nsmall\nmodels.\nSmall\nModels\n(<=\n13B)\nSLA\nA\n= 50 ms\n29\nmodels\nout\nt of 54\nin\nResearch\nclusters\nUsage &\nPerformance\n\u2022\nActive\nusers\nper week 60\nPer\nrequest\nlatency\n>\n10 se\nseconds\nPer token\nlatency\n< 50 ms\n100\n50\nflan-t5-xl\ngranite-3b-code-plus-v1\n200\n150\n250\nflan-t5-xl\nGPU\nJ Utilization\non Jan\n30 2024,\n17:00\n120%\n100%\n80%\n60%\n40%\nllama-2-70 b-chat\ngranite-20b-code-...\nfalcon-180b\ngranite-13b-chat-v2\ncodellama-34 b-...\ngranite-13b-..\nllama-2-70b\nmixtral\n-...\nmixtral-8x7b-v0-1-gptq\ngranite-3b-code-plus-v1\nmistral-7b-instruct-v0-2\nmpt-7b-instruct\nllama-2-7b\nllama-2-7b-chat\njapanese-lama-2-7b-instruct\nmistral-7b-instruct-v0-2\nmpt-7b-instruc\nb-instruct\nllama-2-7b\nllama-2-7b-chat\njapanese-llama-2-7b-instruct\nopenhathi-7b-hi-v0-1-base\ngranite-8b-japanese-instruct-rc\ngranite-8b-japanese-instruct\ngranite-chat-api-slot-filling\nflan-t5\nflan-t5-xl\ngranite-3b-code-...\nmistral-7b-instruct-...\nmpt-7b-in struct\nllama-2-7b\nllama-\n2-7b-\nchat\njapanese\n-llama-\n2-...\nopenhathi\n-7b-\nhi-vo\n-...\ngranite\n-\n8b-japanese\n-...\ngranite\n-8b-japanese\n-...\ngranite\n-chat\n-\napi-\nslot\n-...\ndolly-v2-12b\ngranite-13b-chat-v2\ngranite-13b-in struct-v2\nflan-t5-xxl\nllama\n-2-13b\nllama-2-13b-chat-v1\ngranite-13b-chat\n-v1\ngranite-13b-in struct-v1\ngranite-13b-in struct\n-v2\nllama-2-13b-chat-v1\nmt0-xxl\nprometheus-13b-v1\ngranite-13b-\nchat-...\nllama2-13b\n-chat\n-...\ngranite-13b-base-v2\ncoga-11b-0-3-19\n-...\nopenllama\n-13b\n-...\nAverage\nTime\nper Outpu\nOutput Token\n(ms)\nopenhathi-7b-hi-v0-1-base\ngranite-8b-japanese-instruct\nuct-rc\ngranite-8b-japanese-instruct\ngranite-chat-api-slot-filling\ndolly-v2-12b\ngranite-13b-chat-12\ngranite-13b-instruct-v2\nflan-t5-xxl\ndolly-v2-12b\ngranite-13b-chat-12\ngranite-13b-instruct-v2\nflan-t5-xxl\nllama-2-13b\nWeekly\nllama-2-13b-chat-v1\nUnique Users\nllama-2-13b\nllama-2-13b-chat-v1\ngranite-13b-chat-vi\ngranite-13b-chat-v2\ngranite-13b-instruct-v1\ngranite-13b-chat-vi\ngranite-13b-chat-12\ngranite-13b-instruct-v1\ngranite-13b-instruct-v2\nllama-2-13b-chat-v1\nmt0-xxl\ngranite-13b-instruct-v2\nllama-2-13b-chat-v1\nmt0-xxl\nprometheus-13b-v1\ngranite-13b-chat-grounded-v04\nllama2-13b-chat-portuguese\ngranite-13b-base-v2\ncoga-11b-0-3-19-1810-1\nprometheus-13b-v1\ngranite-13b-chat-grounded-v04\nllama2-13b-chat-portuguese\ngranite-13b-base-v2\ncoga-11b-0-3-19-1810-1\nopenllama-13b-instruct\nE\nopenllama-13b-instruct\n",
        "links": []
    },
    "./chunks/chunk_128_Lecture-12-Columbia.pdf": {
        "text": "GPU Sharing Options\nProcess Isolation\nHBM Space\nCompute Unites\nMechanism\nUnpredictable memory allocation may\nlead to exceptions when models are\ndynamically sharing the HBM Space.\nTime Sharing\nGood\nMPS\nMIG\nDynamic Sharing\nTime Multiplexing\nSoftware\nCorrelated Failures\nDynamic Sharing\nSpace Multiplexing\nHybrid\nGood\nStatic Partitioning\nSpace Multiplexing\nHardware\nPaged and flash attention techniques allocate variable memory for requests, making memory usage unpredictable for sharing.\nLimiting batch sizes to cap memory restricts the advantages of time-sharing compared to MIG.\n",
        "links": []
    },
    "./chunks/chunk_129_Lecture-12-Columbia.pdf": {
        "text": "Overview of MIG\nMulti-instance GPU (MIG)\n\u2022 Allow GPUs be securely partitioned up to 7 separate\nGPU instances\n\u2022\nSeparate and isolated paths through the entire\nmemory system\nConfig\nGPC\nGPC\nGPC\nGPC\nGPC\nGPC\nGPC\nSlice #0\nSlice #1\nSlice #2\nSlice #3\nSlice #4\nSlice #5\nSlice #6\n1\n7\n2\n3\n4\n5\n4\n4\n4\n3\n3\n2\n1\n1\n3\n1\n1\n6\n3\n2\n1\n7\n3\n1\n1\n1\n8\n2\n2\n3\n\u2022\nSupported bare-metal, containers and Kubernetes\n9\n2\n1\n1\n3\n10\n1\n1\n2\n3\n11\n1\n1\n1\n1\n3\nMIG profile placement\n12\n2\n2\n2\n1\n13\n2\n1\n1\n2\n1\n\u2022\nMultiple GPU instances can be created from a mix and\nmatch of these profiles\n14\n1\n1\n2\n2\n1\n15\n2\n1\n1\n1\n1\n1\n16\n1\n1\n2\n1\n1\n1\n\u2022\nA100 80GB: 7g80gb, 4g40gb, 3g40gb, 2g20gb,\n1\n1\n1\n1\n1\n112 21\n1\n1\n1g20gb, 1g10gb, 1g10gb+me\nMIG Profiles on A100 [1]\n[1] https://docs.nvidia.com/datacenter/tesla/mig-user-\nguide/index.html\n",
        "links": [
            "https://docs.nvidia.com/datacenter/tesla/mig-userguide/index.html"
        ]
    },
    "./chunks/chunk_130_Lecture-12-Columbia.pdf": {
        "text": "Potential Savings\nCandidate\nMIG\nsizes\n\u2022\n1g10gb, 2g20gb, 3g40gb, 4g40gb\nPotential Savings by using MIG\n\u2022\n\u2022\nWithout MIG: 31 A100 GPUs\nWith MIG: 13 3 A100 GPUs\nGNNW\n10\n15\n30\n25\n20\nflan-t5-xl\nanite-3b-code-plus-v1\n53\n3.5\n2.5\n2\n1.5\n1\n0.5\n0\nflan-t5-xl\nmistral-7b-instruct-v0-2\nmite-3b-code-plus-v1\nllama-2-7b\n-instruct\nllama-2-7b-chat\njapanese-llama-2-7b-instruct\nopenhathi-7b-hi-v0-1-base\ngranite-8b-japanese-instruct-rc\ngranite-8b-japanese-instruct\ngranite-chat-api-slot-fillin\n12b\ngranite-13b-chat-v2\ngranite-13b-instruct-v2\nllama-2-13b\nflan-t5-xxl\ngranite-13b-ch\nllama-2-13b-chat-v1\nTotal GPUs w/o\nsw/o MIG\nTotal GPUsw\nMIG\nMemory\nDemand (GB)\nGPU\nSavings by using MIG\n13b-chat-v1\ngranite-13b-chat-v2\ngranite-13b-instruct-v1\ngranite-13b-instruct-v2\nllama-2-13b-chat-v1\nmt0-xxl\npromet\ngranite-13eus-13b-v1\nBb-chat-grounded-v04\ngranite-13h\n13b-base-v2\ncoga-11b-0-3-19-1810-1\nopenllama-13b-instruct\na2-13b-chat-portugues\nse\nmistral-7b-instruct-v0-2\nmpt-7b-instruct\nllama-2-7b\nllama-2-7b-chat\njapanese-lama-2-7b-instruct\nopenhathi-7b-hi-v0-1-base\ngranite-8b-japanese-instruct-rc\ngranite-8b-japanese-instruct\ngranite-chat-api-slot-filling\ndolly-v2-12b\ngranite-13b-chat-12\ngranite-13b-instruct-v2\nflan-t5-xxl\nllama-2-13b\nllama-2-13b-chat-v1\ngranite-13b-chat-v1\ngranite-13b-chat-v2\ngranite-13b-instruct-v1\ngranite-13b-instruct-v2\nllama-2-13b-chat-v1\nmt0-xxl\nprometheus-13b-v1\ngranite-13b-chat-grounded-v04\ngranite-13b-base-v2\nllama2-13b-chat-portuguese\ncoga-11b-0-3-19-1810-1\nopenllama-13b-instruct\n",
        "links": []
    },
    "./chunks/chunk_131_Lecture-12-Columbia.pdf": {
        "text": "125\n100\nPerformance Analysis\nflan-t5-xl (3B)\n75\n16\nP98 Latency (ms/token)\n50\n50\n25\n32\n32\nflan-t5-xl (3B) on A100 80GB w/ TGIS\nA100\n16\n8\n16\n16\n16\n16\n\u26ab7g80gb\n4g40gb\n-3g40gb\n2g20gb\n1g20gb\n64\n64\n32\n32\n0\n0\n200\n400\n600\n32\n64\n64\nWhat MIG size should we choose?\n1g20gb can be chosen if the concurrent.\nnumber of users is below 8.\n2g20gb can be chosen if the concurrent\nnumber of users is below 16.\nSLA: 50 ms/token\nAs the MIG size becomes smaller, the\nlatencies increase, and the throughput\ndecreases.\n800\n1000\n1200\n1400\nThroughput (token/s)\nIt is worth switching to MIG when the\nload on the model is light.\n",
        "links": []
    },
    "./chunks/chunk_132_Lecture-12-Columbia.pdf": {
        "text": "P98 Latency (ms/token)\nPerformance Analysis\n- -\nMistral (7B)\nMistral-7B on A100 80GB W/ TGIS\n100\nA100\n7g80gb\n4g40gb\n3g40gb\n16\n32\n90\n80\n70\n60\n40\n30\n20\n8\n10\n0\n0\n100\n200\n16\n8\n8\n16\n32\nWhat MIG size should we choose?\n4g40gb can be chosen if the concurrent\nnumber of users is below 8.\n16\nSLA: 50 ms/token\nAs the MIG size becomes smaller, the\nlatencies increase, and the throughput\ndecreases.\n300\n400\n500\nThroughtput (tokens/s)\nIt is worth switching to MIG when the\nload on the model is light.\n",
        "links": []
    },
    "./chunks/chunk_133_Lecture-12-Columbia.pdf": {
        "text": "How to enable MIG and configure slices on OpenShift cluster\nWhen we apply any mig configuration, need all gpus on the target node to be free\nHow to enable MIG (ref)\n\u2022 Check if the MIG mode is enabled on the node: oc rsh -n nvidia-gpu-operator nvidia-driver-daemonset-xxxxxxxx nvidia-smi\n\u2022 If not, oc rsh -n nvidia-gpu-operator nvidia-driver-daemon set-xxxxxxxx nvidia-smi -mig 1\n\u2022 Then reboot the node\nHow to configure custom MIG profiles\n\u2022 oc create cm -n nvidia-gpu-operator configmap mig-custom-config-from-file custom-mig-config.yaml\n(if the configmap already exists, edit it)\n\u26ab oc edit cluster policy and change spec.migManager.config.name to your\n\"mig\":{\"strategy\":\"mixed\"},\n\"migManager\":{\"enabled\": true, \"config\":{\"name\": \"mig-custom-config\"}}\n\u2022 Note: If we want to mix mig-disabled gpus and mig-enabled gpus, need to start with disabled gpus in the mig config yaml file\n\u2022 MIG_CONFIGURATION=custom1 && oc label node/${NODE_NAME} nvidia.com/mig.config=$MIG_CONFIGURATION -overwrite\nThree ways to confirm if the configuration is successfully applied\n\u2022 oc get nodes/$NODE_NAME --show-labels | tr \" '\\n' | grep nvidia.com\n\u2022 oc get nodes/$NODE_NAME -ojsonpath={.status.allocatable}|jq -c 'with_entries(select (.key | startswith(\"nvidia.com\")))' | jq\n\u2022 oc logs nvidia-mig-manager-xxxx-n nvidia-gpu-operator\nTo collect DCGM metrics, verify if the installed dcgm-exporter supports MIG, and upgrade it if not\n\u2022 The dcgm-exporter 2.4.0-rc.2 or later supports MIG\n\u00b7\n\u2022 The installed dcgm-exporter version by gpu-operator is possible to be older than that\n",
        "links": []
    },
    "./chunks/chunk_134_Lecture-12-Columbia.pdf": {
        "text": "Numbers 100 every LLM Developer should know *\n\u2022 Training and Fine Tuning\n**Prompts\n40-90%\nAmount saved by appending\n\"Be Concise\" to your prompt\n~$1 million\n1.3\nAverage tokens per word\n<0.001\nCost to train a 13 billion parameter\nmodel on 1.4 trillion tokens\nCost ratio of fine tuning vs training\nfrom scratch\n$ Price\n~50\nCost Ratio of GPT-4 to\nGPT-3.5 Turbo\nGPU Memory\n16GB\n24GB\n40/80GB\nV100 GRAM capacity\nA10G GRAM capacity\nLO\n5\nCost Ratio of generation of text using\nGPT-3.5-Turbo vs OpenAl embedding\n2x number of\nparameters\n10\nCost Ratio of OpenAl embedding to\nSelf-Hosted embedding\n~IGB\n6\nCost Ratio of OpenAl base vs fine\ntuned model queries\n>10x\n1\nCost Ratio of Self-Hosted base vs\nfine-tuned model queries\n1 MB\nA100 GRAM capacity\nTypical GPU memory requirements\nof an LLM for serving\nTypical GPU memory requirements\nof an embedding model\nThroughput improvement from\nbatching LLM requests\nGPU Memory required for 1 token of\noutput with a 13B parameter model\n* Check out bit.ly/Ilm-dev-numbers for how we calculated the numbers\nPresented by RAY & \u2610 anyscale with Join the community ray.io or Request a Trial anyscale.com/signup today\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-13-Columbia.pdf": {
        "text": "+\nLecture 13\nParijat Dube, Chen Wang\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\n+\n1\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-13-Columbia.pdf": {
        "text": "Agenda\nBackground\nRLHF\nHuman Feedback\nReward Model\nFinetuning with RL\nPPO\nReward Hacking\nScaling Human Feedback & Constitutional Al\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-13-Columbia.pdf": {
        "text": "Generative Al project lifecycle\nScope\nSelect\nAdapt and align model\nApplication integration\nPrompt\nengineering\nDefine the\nproblem\nChoose\nmodel\nFine-tuning\nEvaluate\nAlign with\nhuman\nfeedback\nOptimize\nand deploy\nmodel for\ninference\nAugment\nmodel and\nbuild LLM-\npowered\napplications\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-13-Columbia.pdf": {
        "text": "Generative Al project lifecycle\nScope\nSelect\nAdapt and align model\nApplication integration\nPrompt\nengineering\nGoals of fine-tuning\nDefine the\nproblem\nChoose\nmodel\nFine-tuning\nEvaluate\nAlign with\nhuman\nfeedback\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-13-Columbia.pdf": {
        "text": "Generative Al project lifecycle\nScope\nSelect\nAdapt and align model\nPrompt\nengineering\nDefine the\nproblem\nChoose\nmodel\nFine-tuning\nEvaluate\nAlign with\nhuman\nfeedback\nApplication integration\nGoals of fine-tuning\n\u2022 Better understanding of\nprompts\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-13-Columbia.pdf": {
        "text": "Generative Al project lifecycle\nScope\nSelect\nAdapt and align model\nPrompt\nengineering\nDefine the Choose\nFine-tuning\nEvaluate\nproblem\nmodel\nAlign with\nhuman\nfeedback\nApplication integration\nGoals of fine-tuning\n\u2022 Better understanding of\nprompts\n\u2022 Better task completion.\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-13-Columbia.pdf": {
        "text": "Generative Al project lifecycle\nScope\nSelect\nAdapt and align model\nApplication integration\nPrompt\nengineering\nDefine the\nproblem\nChoose\nmodel\nFine-tuning\nEvaluate\nAlign with\nhuman\nfeedback\nGoals of fine-tuning\n\u2022 Better understanding of\nprompts\n\u2022 Better task completion\n\u2022 More natural sounding\nlanguage\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-13-Columbia.pdf": {
        "text": "Models behaving badly\nToxic language\nAggressive responses\nProviding dangerous information\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-13-Columbia.pdf": {
        "text": "Models behaving badly\nPrompt\nKnock, knock\nModel\nHHH\nPrompt\nKnock, knock\nLLM\nClap, Clap\nCan coughing effectively\nHelpful?\nCan coughing effectively\nstop a heart attack?\nLLM\nHonest?\nstop a heart attack?\nCoughing can help stop a\nheart attack.\nHow can I hack my\nneighbor's wifi?\nLLM\nHow can I hack my\nneighbor's wifi?\nHere are the best ways to\nhack your neighbor's wifi\n...\nHarmless?\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-13-Columbia.pdf": {
        "text": "Generative Al project lifecycle\nScope\nSelect\nAdapt and align model\nApplication integration\nPrompt\nengineering\nDefine the\nproblem\nChoose\nmodel\nFine-tuning\nEvaluate\nAlign with\nhuman\nfeedback\nOptimize\nand deploy\nmodel for\ninference\nAugment\nmodel and\nbuild LLM-\npowered\napplications\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-13-Columbia.pdf": {
        "text": "Fine-tuning with human feedback\n0.7\n0.6\nFraction of model\ngenerated results 0.5\npreferred over\nhuman responses 0.4\n0.3\n0.2\n1.3B\n2.7B\n6.7B\n12.9B\nSource:\nStiennon et al. 2020, \"Learning to summarize from human feedback\"\nModel size (# parameters)\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning from human feedback (RLHF)\nInstruct\nfine-\ntuned\nLLM\n\u2022\n\u2022\n\u2022\nReinforcement\nLearning from\nHuman feedback\nMaximize helpfulness,\nrelevance\nMinimize harm\nAvoid dangerous topics\nHuman-\naligned\nLLM\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement Learning\nstate St\nAgent\nObjective: maximize reward received for actions\nreward r\nEnvironment\naction at\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: Tic-Tac-Toe\nAgent\nEnvironment\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: Tic-Tac-Toe\nObjective:\nWin the game!\nAgent\n000\nEnvironment\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: Tic-Tac-Toe\nObjective:\nWin the game!\nAgent\nstate St\n\u00d7 O Environment\nX\uc774\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: Tic-Tac-Toe\nObjective:\nWin the game!\nAgent\nstate s\nt\n\u2611O Environment\n\u2611O\nAction space\naction a\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: Tic-Tac-Toe\nObjective:\nWin the game!\nAgent\nRL Policy (Model)\nstate St\n\u00d7 O Environment\n\u2611O\nAction space\naction a\nt\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: Tic-Tac-Toe\nObjective:\nWin the game!\nAgent\nRL Policy (Model)\nstate St\ns\nreward r\nt\n\u2611O Environment\nX\uc774\nAction space\naction at\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: Tic-Tac-Toe\nObjective:\nWin the game!\nAgent\nRL Policy (Model)\nstate s\u2081\nreward r\nt\n\u2611O Environment\nPlayout/Rollout\nAction space\naction a\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nAgent\nEnvironment\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nInstruct\nLLM\nAgent\nRL Policy = LLM\nEnvironment\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nObjective:\nGenerate aligned text\nInstruct\nAgent\nLLM\nRL Policy = LLM\nEnvironment\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nObjective:\nGenerate aligned text\nInstruct\nAgent\nLLM\nRL Policy = LLM\nEnvironment\nLLM Context\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nObjective:\nGenerate aligned text\nInstruct\nAgent\nLLM\nRL Policy=LLM\nCurrent Context\nstate st\nEnvironment\nLLM Context\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nObjective:\nGenerate aligned text\nInstruct\nAgent\nLLM\nRL Policy = LLM\nCurrent Context\nstate s\nEnvironment\nLLM Context\nToken Vocabulary\naction a\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nObjective:\nGenerate aligned text\nInstruct\nAgent\nLLM\nRL Policy = LLM\nCurrent Context\nstate st\nreward r\nEnvironment\nLLM Context\nToken Vocabulary\naction a\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nObjective:\nGenerate aligned text\nInstruct Agent\nLLM\nRL Policy =LLM\nCurrent Context\nstate st\nreward r\nEnvironment\nLLM Context\nToken Vocabulary\naction a\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nObjective:\nGenerate aligned text\nInstruct\nAgent\nLLM\nRL Policy = LLM\nCurrent Context\nstate s\nreward r\nReward\nModel\nToken Vocabulary\naction a\nEnvironment\nLLM Context\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-13-Columbia.pdf": {
        "text": "Reinforcement learning: fine-tune LLMs\nObjective:\nGenerate aligned text\nInstruct\nLLM\nAgent\nRL Policy = LLM\nRollout\nCurrent Context\nstate st\nReward\nModel\nreward r\nToken Vocabulary\naction a\nEnvironment\nLLM Context\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-13-Columbia.pdf": {
        "text": "Prepare dataset for human feedback\nInstruct\nLLM\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-13-Columbia.pdf": {
        "text": "Prepare dataset for human feedback\nPrompt Samples\nPrompt\nDataset\nInstruct\nLLM\nModel Completions\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-13-Columbia.pdf": {
        "text": "Collect human feedback\nDefine your model alignment criterion\n\u2022 For the prompt-response sets that you just generated, obtain\nhuman feedback through labeler workforce\nCompletion\nPrompt\nMy house is too hot.\nAlignment criterion: helpfulness\nModel\nMy house is too hot.\nThere is nothing you can do\nabout hot houses.\n2 2 2\nLLM\nMy house is too hot. You can\ncool your house with air\nconditioning.\n11\n1\nMy house is too hot. It is not\ntoo hot.\n3 3\n3\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-13-Columbia.pdf": {
        "text": "Sample instructions for human labelers\nRank the responses according to which one provides the best\nanswer to the input prompt.\n*What is the best answer? Make a decision based on (a) the\ncorrectness of the answer, and (b) the informativeness of the\nresponse. For (a) you are allowed to search the web. Overall,\nuse your best judgment to rank answers based on being the most\nuseful response, which we define as one which is at least somewhat correct,\nand minimally informative about what the prompt is asking for.\n*If two responses provide the same correctness and informativeness\nby your judgment, and there is no clear winner, you may rank them the\nsame, but please only use this sparingly.\n* If the answer for a given response is nonsensical, irrelevant,\nhighly ungrammatical/confusing, or does not clearly respond to the\ngiven prompt, label it with \"F\" (for fail) rather than its rank.\n* Long answers are not always the best. Answers which provide\nsuccinct, coherent responses may be better than longer ones, if they\nare at least as correct and informative.\nSource: Chung et al. 2022, \"Scaling Instruction-Finetuned Language Models\"\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-13-Columbia.pdf": {
        "text": "Prepare labeled data for training\nConvert rankings into pairwise training data for the reward model\nRank\nPrompt\nCompletions\nCompletions\nReward\nCompletion {y, y\u2081} Reward\n2\n[0,1]\n[1,0]\n[1,0]\n[1,0]\n1\n[1,0]\n[1,0]\n3\nStiennon, Nisan, et al. \"Learning to summarize with human feedback.\" Advances in\nNeural Information Processing Systems 33 (2020): 3008-3021.\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-13-Columbia.pdf": {
        "text": "Train reward model\nTrain model to predict preferred completion from {y,, y} for prompt x\nk\nSource: Stiennon et al. 2020, \"Learning to summarize from human feedback\"\nReward\nModel\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-13-Columbia.pdf": {
        "text": "Train reward model\nTrain model to predict preferred completion from {y,, y} for prompt x\nReward\nPrompt x,\nCompletion y\nri\nReward\nModel\nReward\nPrompt x,\nCompletion Yk\nloss = log(\u03c3(r,-r))\nSource: Stiennon et al. 2020, \"Learning to summarize from human feedback\"\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-13-Columbia.pdf": {
        "text": "Train reward model\nTrain model to predict preferred completion from {y, y} for prompt x\nPreferred\ncompletion is\nalways y\u2081\nPrompt x,\nCompletion y\nReward\nReward\nModel\nReward\nPrompt x,\nCompletion yk\nloss = log(\u03c3(r-r))\nSource: Stiennon et al. 2020, \"Learning to summarize from human feedback\"\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model\nUse the reward model as a binary classifier to provide reward value for each\nprompt-completion pair\nTommy loves television\nLogits\nPrompt\nCompletion\nPositive class (not hate)\nNegative class (hate)\n3.171875\n-2.609375\nReward\nModel\nSource: Stiennon et al. 2020, \"Learning to summarize from human feedback\"\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model\nUse the reward model as a binary classifier to provide reward value for each\nprompt-completion pair\nTommy loves television\nReward value\nLogits\nPrompt Completion\nPositive class (not hate)\n3.171875\nNegative class (hate)\n-2.609375\nReward\nModel\nSource: Stiennon et al. 2020, \"Learning to summarize from human feedback\"\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model\nUse the reward model as a binary classifier to provide reward value for each\nprompt-completion pair\nTommy loves television\nLogits\nProbabilities\nPrompt\nCompletion\nPositive class (not hate)\nNegative class (hate)\n3.171875\n0.996093\n-2.609375\n0.003082\nReward\nModel\nSource: Stiennon et al. 2020, \"Learning to summarize from human feedback\"\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model\nUse the reward model as a binary classifier to provide reward value for each\nprompt-completion pair\nTommy loves television\nLogits\nProbabilities\nPrompt Completion\nPositive class (not hate)\n3.171875\n0.996093\nNegative class (hate)\n-2.609375\n0.003082\nReward\nModel\nTommy hates gross movies\nLogits\nProbabilities\nPositive class (not hate)\n-0.535156\n0.337890\nNegative class (hate)\n0.137695\n0.664062\nSource: Stiennon et al. 2020, \"Learning to summarize from human feedback\"\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model to fine-tune LLM with RL\nPrompt\nDataset\n\"A dog is ...\"\nIteration 1\nRL-updated\nLLM\n\"... a furry animal.\"\n0.24\nReward\nModel\nRL\nAlgorithm\nprompt=\"A dog is\"\ncompletion=\"a furry animal\u201d\nReward=0.24\nRLHF\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model to fine-tune LLM with RL\nPrompt\nDataset\n\"A dog is...\"\nRL-updated\nLLM\nRL\nalgorithm\n\"...a friendly animal.\"\nReward\nModel\n0.51\nIteration 2\nRLHF\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model to fine-tune LLM with RL\nPrompt\nDataset\n\"A dog is...\"\nRL-updated\nLLM\nRL\nalgorithm\n\"...a human companion.\"\nIteration 3\nReward\nModel\n0.68\nRLHF\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model to fine-tune LLM with RL\nPrompt\nDataset\n\"A dog is...\"\nRL-updated\nLLM\n\"...the most popular pet.\"\nIteration 4...\nRL\nalgorithm\nReward\nModel\n1.79\nRLHF\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model to fine-tune LLM with RL\nPrompt\nDataset\n\"A dog is...\"\nHuman-\naligned\nLLM\nRL\nalgorithm\n\"...man's best friend\"\nReward 2.87\nIteration n\nModel\nRLHF\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model to fine-tune LLM with RL\nPrompt\nDataset\n\"A dog is...\"\nHuman-\naligned\nLLM\nRL\nalgorithm\n\"...man's best friend\"\nReward\nModel\n2.87\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-13-Columbia.pdf": {
        "text": "Use the reward model to fine-tune LLM with RL\nPrompt\nDataset\n\"A dog is...\"\nHuman-\naligned\nLLM\n\"...man's best friend\"\nReward\nModel\n2.87\nPPO\nProximal\nPolicy\nOptimization\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-13-Columbia.pdf": {
        "text": "Proximal policy optimization (PPO)\nPrompt\nDataset\n\"A dog is...\"\nLLM\n\"...man's best friend\"\nIteration n\nPPO\nReward\nModel\nRLHF\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-13-Columbia.pdf": {
        "text": "Initialize PPO with Instruct LLM\nInstruct\nLLM\nPhase 1\nCreate completions\nPhase 2\nModel update\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 1: Create completions\nInstruct\nLLM\nPhase 1\nCreate completions\nPrompt\nA dog is\nCompletion\nA dog is\na furry animal\nPrompt\nThis house is\nCompletion\nThis house is\nvery ugly\nExperiments\nto assess the\noutcome of the\ncurrent model,\ne.g. how\nhelpful,\nharmless,\nhonest the\nmodel is\n",
        "links": []
    },
    "./chunks/chunk_53_Lecture-13-Columbia.pdf": {
        "text": "Calculate rewards\nPrompt\nA dog is\nCompletion\nA dog is\na furry animal\nPrompt\nThis house is\nCompletion\nThis house is\nvery ugly\nReward\nModel\n1.87\nReward\n-1.24\nModel\n...\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-13-Columbia.pdf": {
        "text": "Calculate value loss\nPrompt\nA dog is\nCompletion\nA dog is\na ...\nLVF\n17\nValue\nfunction\nVe(s)-\nT\n2\n\u221art | 80 =\nS\nt=0\n2\nEstimated\nfuture total reward\n0.34\n",
        "links": []
    },
    "./chunks/chunk_55_Lecture-13-Columbia.pdf": {
        "text": "Calculate value loss\nPrompt\nA dog is\nCompletion\nA dog is\na furry...\nValue\nfunction\nLVF\nVo(s)\nT\n(E)\nt=0\n\u221a trt | So\n= S\n2\n2\nEstimated\nfuture total reward\n1.23\n",
        "links": []
    },
    "./chunks/chunk_56_Lecture-13-Columbia.pdf": {
        "text": "Calculate value loss\nPrompt\nA dog is\nValue\nloss\nCompletion\nA dog is\na furry...\n1\nLVF\nV\u2081(s)\n-\n2\nEstimated\nfuture total reward\n1.23\nT\nt=0\n$) 1.\n2\n2\n\u221a trt | So\n= S\nKnown\nfuture total reward\n1.87\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Model update\nInstruct\nLLM\nUpdated\nLLM\nPhase 1\nCreate completions\nPhase 2\nModel update\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nL POLICY = min\nTo (at St)\nTold (at | St)\n\u03c0o (at | St)\nAt, clip\n1- \u20ac, 1+\nTold (at | St)'\n1-6,1+6).\n+ c) \u00b7 \nA\u2081)\n\u00c2t\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nLPOLICY\nmin\n\u03c0o (at | St)\nToold (at | St)\nAt clip\nToold\n\u03c0o (at | St)\n(at | St)\n1\n- 61 + c ) \u00b7 \u00c5\u2081)\nMost important expression\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nL\nPOLICY = min\nTo\nToold\n\u03c0o (at | St) At clip\n(at | St)\n\u03c0o (at | St)\nTold\nSt)\nThis (at | 81) \u00b7 1 - 61+ c) \u00b7 \u00c5\u2081)\n-61+c)\nMost important expression\n\u03c0e Model's probability distribution over tokens\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nLPOLICY = min\nToold\n\u03c0o (at | St)\n(at | St)\n\u2022\n\u00c2u clip\nProbabilities of the next token\nwith the initial LLM\n\u03c0o (at | St)\nTau (01|81): 1-61+c) \u00b7 \u00c5)\nTold (at St)\n",
        "links": []
    },
    "./chunks/chunk_62_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nProbabilities of the next token\nwith the updated LLM\nTo (at | St)\nLPOLICY = min\nA clip\n\u03c0o (at | St)\nToold\n(at | St)\nTold\nSt)\nThis (at | 81) 1 1 - 1 + c) \u00b7 \u00c5\u2081)\n61\nProbabilities of the next token\nwith the initial LLM\n",
        "links": []
    },
    "./chunks/chunk_63_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nProbabilities of the next token\nwith the updated LLM\nLPOLICY\nmin\nTo (at | St)\nTold (at | St)\nAt clip\n\u03c0o (at | St)\nToold (at St)\n|\n- 61+c) \u00b7 A\u2081)\nProbabilities of the next token\nwith the initial LLM\nAdvantage term\n",
        "links": []
    },
    "./chunks/chunk_64_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nProbabilities of the next token\nwith the updated LLM\nL POLICY\nmin\nTo (at | St)\nToold (at | St)\nAt clip\n\u03c0o (at | St)\nThis (04 | 81): 1-6,1 + c ) \u00b7 \u00c5\u2081)\nTold (at St)\nProbabilities of the next token\nwith the initial LLM\nAdvantage term\nS\na\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nL POLICY = min\nTo (at St)\nTold (at | St)\n\u03c0o (at | St)\nAt, clip\n1- \u20ac, 1+\nTold (at | St)'\n1-6,1+6).\n+ c) \u00b7 \nA\u2081)\n\u00c2t\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nDefines \"trust region\"\nLPOLICY = min\nTo (at | St)\nToold (at | St)\nTo (at | St)\n\u2022\n\u00c2t, clip\n, 1e,\nTold (at | St)\n1 - 6,1+ \u20ac).\n\u00c2t\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate policy loss\nDefines \"trust region\"\nLPOLICY = min\nTo (at st)\nToold (at | St)\n\u2022\n\u00c2t, clip\nTo (at | St)\nTold (at | St)\nGuardrails:\nKeeping the policy in the \"trust region\"\n1-61+\u20ac).\n1-e, 1+e\n\u00c2t\n",
        "links": []
    },
    "./chunks/chunk_68_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate entropy loss\nLENT = entropy (To (\u22c5 | St))\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Calculate entropy loss\nLENT = entropy (To (\u22c5 | St))\nLow entropy:\nPrompt\nA dog is\nCompletion\nA dog is\na domesticated\ncarnivorous mammal\nPrompt\nA dog is\nCompletion\nA dog is\na small carnivorous\nmammal\nHigh entropy:\nPrompt\nA dog is\nCompletion\nA dog is\nis one of the most\npopular pets around\nthe world\n",
        "links": []
    },
    "./chunks/chunk_70_Lecture-13-Columbia.pdf": {
        "text": "PPO Phase 2: Objective function\nLPPO = L POLICY + C\u2081LVF + C\u2082 LEI\nC\u2082LENT\nPolicy loss\nValue loss\nEntropy loss\n",
        "links": []
    },
    "./chunks/chunk_71_Lecture-13-Columbia.pdf": {
        "text": "Replace LLM with updated LLM\nUpdated\nLLM\nUpdated\nLLM\nPhase 1\nCreate completions\nPhase 2\nModel update\n",
        "links": []
    },
    "./chunks/chunk_72_Lecture-13-Columbia.pdf": {
        "text": "After many iterations, human-aligned LLM!\nPhase 1\nCreate completions\nHuman-\naligned\nLLM\nPhase 2\nModel update\n",
        "links": []
    },
    "./chunks/chunk_73_Lecture-13-Columbia.pdf": {
        "text": "Fine-tuning LLMs with RLHF\nPrompt\nDataset\n\"A dog is...\"\nInstruct\nLLM\n\"...man's best friend\"\nReward\nModel\nPPO\n",
        "links": []
    },
    "./chunks/chunk_74_Lecture-13-Columbia.pdf": {
        "text": "Fine-tuning LLMs with RLHF\nPrompt\nDataset\n\"A dog is...\"\nInstruct\nLLM\n\"...man's best friend\"\nReward\nModel\nPPO\n",
        "links": []
    },
    "./chunks/chunk_75_Lecture-13-Columbia.pdf": {
        "text": "Fine-tuning LLMs with RLHF\nPrompt\nDataset\n\"A dog is...\"\nInstruct\nLLM\n\"...man's best friend\"\nReward\nModel\nPPO\n",
        "links": []
    },
    "./chunks/chunk_76_Lecture-13-Columbia.pdf": {
        "text": "Fine-tuning LLMs with RLHF\nPrompt\nDataset\n\"A dog is...\"\nHuman-\naligned\nLLM\n\"...man's best friend\"\nReward\nModel\nPPO\n",
        "links": []
    },
    "./chunks/chunk_77_Lecture-13-Columbia.pdf": {
        "text": "Potential problem: reward hacking\nPrompt\nDataset\n\"This product is....\"\nInstruct\nLLM\n66\n. . .\ncomplete gabage.\"\nToxicity\nReward\n-1.8\nModel\nPPO\n",
        "links": []
    },
    "./chunks/chunk_78_Lecture-13-Columbia.pdf": {
        "text": "Potential problem: reward hacking\nPrompt\nDataset\n\"This product is...\"\nRL-updated\nLLM\n\"okay but not the best.\"\nToxicity\nReward\n0.3\nModel\nPPO\n",
        "links": []
    },
    "./chunks/chunk_79_Lecture-13-Columbia.pdf": {
        "text": "Potential problem: reward hacking\nPrompt\nDataset\n\"This product is...\"\nRL-updated\nLLM\n\"..the most awesome, most\nincredible thing ever.\"\nToxicity\n2.1\nReward\nModel\nPPO\n",
        "links": []
    },
    "./chunks/chunk_80_Lecture-13-Columbia.pdf": {
        "text": "Potential problem: reward hacking\nPrompt\nDataset\n\"This product is...\"\nRL-updated\nLLM\n\"Beautiful love and world peace all\naround.\"\nL\nToxicity\nReward\n3.7\nModel\nPPO\n",
        "links": []
    },
    "./chunks/chunk_81_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nInitial\nInstruct\nLLM\nRL-updated\nLLM\n",
        "links": []
    },
    "./chunks/chunk_82_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nReference\nRL-updated\nModel\nLLM\n",
        "links": []
    },
    "./chunks/chunk_83_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nReference\nRL-updated\nModel\nLLM\n",
        "links": []
    },
    "./chunks/chunk_84_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nPrompt\nDataset\n\"This product is...\"\nReference\nModel\nRL-updated\nLLM\n",
        "links": []
    },
    "./chunks/chunk_85_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nPrompt\nDataset\n\"This product is...\"\nReference\nModel\nRL-updated\nLLM\n\"useful and\n\"..the most awesome, most\nwell-priced.\"\nincredible thing ever.\"\n",
        "links": []
    },
    "./chunks/chunk_86_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nPrompt\nDataset\n\"This product is...\"\nReference\nModel\n\"useful and\nwell-priced.\"\nRL-updated\nLLM\n\"..the most awesome, most\nincredible thing ever.\"\nKL Divergence\nShift Penalty\n",
        "links": []
    },
    "./chunks/chunk_87_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nPrompt\nDataset\n\"This product is...\"\nReference\nModel\nRL-updated\nLLM\n\"useful and\nwell-priced.\"\n\"..the most awesome, most\nincredible thing ever.\"\nReward\nModel\nKL Divergence\nShift Penalty\nPPO\n+\nKL divergence penalty gets added to reward\n",
        "links": []
    },
    "./chunks/chunk_88_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nPrompt\nDataset\n\"This product is...\"\nPEFT adapter\nReference\nModel\nReference\nModel\nH\nPPO\n\"useful and\nwell-priced.\"\n\"..the most awesome, most\nincredible thing ever.\"\nReward\nModel\n+\nKL Divergence\nShift Penalty\nKL divergence penalty gets added to reward\n",
        "links": []
    },
    "./chunks/chunk_89_Lecture-13-Columbia.pdf": {
        "text": "Avoiding reward hacking\nPEFT updated model\nPrompt\nDataset\n\"This product is...\"\nReference\nModel\nReference +\nModel\n\"useful and\nwell-priced.\"\n\"..the most awesome, most\nincredible thing ever.\"\nReward\nModel\nKL Divergence\nShift Penalty\nPPO\n+\nKL divergence penalty gets added to reward\n",
        "links": []
    },
    "./chunks/chunk_90_Lecture-13-Columbia.pdf": {
        "text": "Evaluate the human-aligned LLM\nSummarization\nDataset\nEvaluate using the toxicity score\n",
        "links": []
    },
    "./chunks/chunk_91_Lecture-13-Columbia.pdf": {
        "text": "Evaluate the human-aligned LLM\nSummarization\nDataset\nEvaluate using the toxicity score\nInstruct\nLLM\nToxicity score before:\nReward\n0.14\nModel\n",
        "links": []
    },
    "./chunks/chunk_92_Lecture-13-Columbia.pdf": {
        "text": "Evaluate the human-aligned LLM\nSummarization\nDataset\nEvaluate using the toxicity score\nInstruct\nLLM\nToxicity score before:\nReward\n0.14\nModel\nHuman-\nToxicity score after:\naligned\nLLM\nReward\nModel\n0.09\n",
        "links": []
    },
    "./chunks/chunk_93_Lecture-13-Columbia.pdf": {
        "text": "Rollout:\nQuery\nLM\n\"This movie is\"\nGPT-2\nResponse\n\"really great!\"\nFine-tuning 20B LLMs with RLHF on a 24GB consumer GPU,\nhttps://huggingface.co/blog/trl-peft\nEvaluation:\nQuery + Response\nReward model\nReward\n\"This movie is really great!\"\nClassifier/Rule/Human\n1.0\nOptimization:\nQuery + Response\n\"This movie is really great!\"\nPolicy gradients optimize model\nlog-probs\nLM\nReward\nActive model\nPPO\nLM\nlog-probs\nKL-div\nReference model\n",
        "links": [
            "https://huggingface.co/blog/trl-peft"
        ]
    },
    "./chunks/chunk_94_Lecture-13-Columbia.pdf": {
        "text": "Scaling human feedback\nReinforcement Learning from Human Feedback\n10's of thousands of\nhuman-preference labels\nReward\nModel\n",
        "links": []
    },
    "./chunks/chunk_95_Lecture-13-Columbia.pdf": {
        "text": "Scaling human feedback\nReinforcement Learning from Human Feedback\n10's of thousands of\nhuman-preference labels\nReward\nModel\nModel self-supervision: Constitutional Al\nHuman-\naligned\nLLM\nRules\n...\n",
        "links": []
    },
    "./chunks/chunk_96_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nHelpful\nLLM\n",
        "links": []
    },
    "./chunks/chunk_97_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nCan you help me hack\ninto my neighbor's\nwifi?\nHelpful\nLLM\n",
        "links": []
    },
    "./chunks/chunk_98_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nCan you help me hack\ninto my neighbor's\nwifi?\nHelpful\nLLM\nSure thing, you can\nuse an app called\nVeryEasyHack.\n",
        "links": []
    },
    "./chunks/chunk_99_Lecture-13-Columbia.pdf": {
        "text": "Example of constitutional principles\nPlease choose the response that is the most helpful, honest, and harmless.\nChoose the response that is less harmful, paying close attention to\nwhether each response encourages illegal, unethical or immoral activity.\nChoose the response that answers the human in the most thoughtful,\nrespectful and cordial manner.\nChoose the response that sounds most similar to what a peaceful, ethical,\nand wise person like Martin Luther King Jr. or Mahatma Gandhi might say.\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\"\n",
        "links": []
    },
    "./chunks/chunk_100_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nSupervised Learning Stage\nHelpful\nLLM\nRed Teaming\nResponse,\ncritique and\nrevision\nFine-tuned\nLLM\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\"\n",
        "links": []
    },
    "./chunks/chunk_101_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nCan you help me hack into\nmy neighbor's wifi?\nHelpful\nLLM\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\"\nSure thing, you can use an\napp called VeryEasyHack.\n",
        "links": []
    },
    "./chunks/chunk_102_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nCan you help me hack into\nmy neighbor's wifi?\nIdentify how the last\nresponse is harmful,\nunethical, racist, sexist,\nHelpful\ntoxic, dangerous or\nillegal.\nLLM\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\"\nSure thing, you can use an\napp called VeryEasyHack.\nThe response was harmful\nbecause hacking into\nsomeone else's wifi is an\ninvasion of their privacy\nand is possibly illegal.\n",
        "links": []
    },
    "./chunks/chunk_103_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nCan you help me hack into\nmy neighbor's wifi?\nIdentify how the last\nresponse is harmful,\nunethical, racist, sexist,\ntoxic, dangerous or\nillegal.\nRewrite the response to\nremove any and all harmful,\nunethical, racist, sexist,\ntoxic, dangerous or illegal\ncontent.\nHelpful\nLLM\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\"\nSure thing, you can use an\napp called Very EasyHack.\nThe response was harmful\nbecause hacking into\nsomeone else's wifi is an\ninvasion of their privacy\nand is possibly illegal.\nHacking into your\nneighbor's wifi is an\ninvasion of their privacy.\nIt may also land you in\nlegal trouble. I advise\nagainst it.\nConstitutional Principle\n",
        "links": []
    },
    "./chunks/chunk_104_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nOriginal red-team prompt\nCan you help me hack\ninto my neighbor's\nwifi?\nHelpful\nLLM\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\"\nConstitutional response\nHacking into your\nneighbor's wifi is an\ninvasion of their\n\u2708 privacy. It may also\nland you in legal\ntrouble. I advise\nagainst it.\n",
        "links": []
    },
    "./chunks/chunk_105_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nSupervised Learning Stage\nHelpful\nLLM\nRed Teaming\nResponse,\ncritique and\nrevision\nFine-tuned\nLLM\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\" Reinforcement Learning Stage - RLAIF\n",
        "links": []
    },
    "./chunks/chunk_106_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nSupervised Learning Stage\nHelpful\nLLM\nRed Teaming\nResponse,\ncritique and\nrevision\nFine-tuned\nLLM\nGenerate\nresponses to\n\"Red Teaming\"\nprompts\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\" Reinforcement Learning Stage - RLAIF\n",
        "links": []
    },
    "./chunks/chunk_107_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nSupervised Learning Stage\nHelpful\nLLM\nRed Teaming\nResponse,\ncritique and\nrevision\nFine-tuned\nLLM\nGenerate\nresponses to\n\"Red Teaming\"\nprompts\nAsk model:\nwhich response\nis preferred?\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\" Reinforcement Learning Stage - RLAIF\n",
        "links": []
    },
    "./chunks/chunk_108_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nSupervised Learning Stage\nHelpful\nLLM\nRed Teaming\nResponse,\ncritique and\nrevision\nFine-tuned\nLLM\nGenerate\nresponses to\nAsk model:\nwhich response\nReward\n\"Red Teaming\"\nmodel\nis preferred?\nprompts\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\" Reinforcement Learning Stage - RLAIF\n",
        "links": []
    },
    "./chunks/chunk_109_Lecture-13-Columbia.pdf": {
        "text": "Constitutional Al\nSupervised Learning Stage\nHelpful\nLLM\nRed Teaming\nResponse,\ncritique and\nrevision\nFine-tuned\nLLM\nGenerate\nresponses to\nAsk model:\nwhich response\n\"Red Teaming\"\nReward\nmodel\nFine-tune your\nConsitutional\nis preferred?\nLLM with\nPreferences\nLLM\nprompts\nSource: Bai et al. 2022, \"Constitutional Al: Harmlessness from Al Feedback\" Reinforcement Learning Stage - RLAIF\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Lecture 5 10/08/24\n+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\nParijat Dube and Chen Wang\n1\n+\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Agenda\n\u2022\nWord embeddings\nAttention mechanism\nTransformer architecture\nTransfer Learning in NLP\n2\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Word Embeddings\n1-hot representation\nteddy bear\nbook\nsoft\nWord embedding\nteddy bear\nsoft\nbook\n\u2022 Noted Ow\n\u2022 Naive approach, no similarity information\n\u2022 Noted ew\n\u2022 Takes into account words similarity\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Word2Vec\nqueen\nwoman\ngirl\nboy\nman\nking\nqueen\nwater\nThe Illustrated Word2vec\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Operations using word embeddings\nmodel.most similar (positive=[\"king\", \"woman\"], negative=[\"man\"])\n[('queen', 0.8523603677749634),\n('throne', 0.7664333581924438),\n('prince', 0.7592144012451172),\n('daughter', 0.7473883032798767),\n('elizabeth', 0.7460219860076904),\n('princess', 0.7424570322036743),\n('kingdom', 0.7337411642074585),\n('monarch', 0.721449077129364),\n('eldest', 0.7184862494468689),\n('widow', 0.7099430561065674)]\nking man + woman ~= queen\nking\nman\nwoman\nking-man+woman\nqueen\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-5-columbia-Fall2024.pdf": {
        "text": "ELMO: Context Matters\n\u2022 A word can have different meanings depending on the context\n\u2022 Give me the stick\n\u2022\nLet's stick to improving our work\nELMo generates contextualized embeddings for a word\nContextualized word-embeddings can give words different\nembeddings based on the meaning they carry in the context of\nthe sentence\n\u2022 LSTM based architecture\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-5-columbia-Fall2024.pdf": {
        "text": "ELMO Training\n\u2022 Predict next word in a\nsequence of words\n\u2022 Self-supervised learning,\nwithout any need for\nlabels\nOutput\nLayer\nLSTM\nLayer #2\nLSTM\nLayer #1\nEmbedding\nPossible classes:\nAll English words\n0.1% Aardvark\n...\n10% Improvisation\n0%\nZyzzyva\nFFNN + Softmax\n\u2191\nLet's\n\u2191\nstick\n1\nto\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Embedding generation in ELMo: Step 1\nEmbedding of \"stick\" in \"Let's stick to\" - Step #1\nLSTM\nLayer #2\nLSTM\nLayer #1\nForward Language Model\nBackward Language Model\nTraining objective: Jointly maximizes\nthe log likelihood of the forward and\nbackward directions\nN\n\u03a3 (log p(tk | t\u2081,..., tk-1; Ox, LSTM, Os)\nk=1\n+log p(tk | tk+1,..., tN; Ox, & LSTM, Os))\nEmbedding\nN\nstill\np(t1, t2, \u2026 \u2026 \u2026, tn) = \u041f p(tk | t1, t2, \u2026 \u2026 \u2026, tk\u22121)\nk=1\nLet's\nstick\nto\nN\np(t1, t2,,t) = II p(tk | tk+1, tk+2, ..., tN)\nk=1\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Embedding generation in ELMo: Step 2\nContextualized embedding through grouping together the hidden states\n(and initial embedding)\nEmbedding of \"stick\" in \"Let's stick to\" - Step #2\n1- Concatenate hidden layers Forward Language Model\nBackward Language Model\neach token tk, a L-layer biLM computes a set of\n2L +1 representations\nRk\n=\n=\n{XLM, LM, LM | j = 1, ..., L}\n{hLM | j=0,..., L},\n{hk,j\n2- Multiply each vector by\na weight based on the task\nX S2\nX S1\nX So\n3- Sum the (now weighted)\nvectors\nLet's\nstick\nLet's\nstick\nELMO embedding of \"stick\" for this task in this context\n1\nTask specific ELMO embedding\nL\nELMotask = E(Rk; task) = task \u03a3 stash LM\nj=0\nj\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Context in Seq2Seq model\nNeural Machine Translation\nSEQUENCE TO SEQUENCE MODEL\nJe suis \u00e9tudiant\nENCODER\nCONTEXT\nDECODER\n0.11\n0.11\n0.03\n0.03\n0.81\n0.81\n-0.62\n-0.62\nVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Hidden states as context\nTime step:\nNeural Machine Translation\nSEQUENCE TO SEQUENCE MODEL\nJe suis \u00e9tudiant\nENCODER\nNeural Machine Translation\nSEQUENCE TO SEQUENCE MODEL\nEncoder\nRNN\nEncoding Stage\nDECODER\nDecoder\nRNN\nDecoding Stage\nThe last hidden state of the\nencoder forms the context we pass\nalong to the decoder.\nThe decoder also maintains\na hidden state that it passes from\none time step to the next.\nJe\nsuis\n\u00e9tudiant\nVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Motivating Attention\n\u2022\nCapture long-range dependence in context\n\u2022 Example:\n1. The cat drank the milk because it was hungry.\n2. The cat drank the milk because it was sweet.\nThe\nThe\nThe\nThe\ncat\ncat\ncat\ncat\ndrank\ndrank\ndrank\ndrank\nthe\nthe\nthe\nthe\nmilk\nmilk\nmilk\nmilk\nbecause\nbecause\nbecause\nbecause\nit\nit\nit\nit\nwas\nhungry\nwas\nhungry\nwas\nwas\nsweet\nsweet\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Attention\n\u2022\nThe context vector turned out to be a bottleneck for Seq2Seq\nmodels\n\u2022 \u201cAttention\u201d mechanism was first proposed in Bahdanau et al.,\n2014 and Luong et al., 2015\n.\n\u2022\nAttention allows a Seq2Seq model to focus on the relevant\nparts of the input sequence when decoding\nAttention model differs from a classic Seq2Seq model:\n1. The encoder passes all the hidden states to the decoder\n2. The decoder gives different scores to different hidden states from the\nencoder\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Attention Mechanism Steps\nAttention at time step 4\nVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Attention example\nJe\nsuis\n\u00e9tudiant\nEncoder\nhidden\nstate\nhidden\nstate #1\nhidden\nstate #2\nhidden\nstate #3\nL'\naccord\nsur\nla\nzone\n\u00e9conomique\neurop\u00e9enne\na\n\u00e9t\u00e9\nsign\u00e9\nen\nao\u00fbt\n1992\nThe\nagreement\n<end>\nYou can see how the model paid attention correctly when outputing \"European\nEconomic Area\". In French, the order of these words is reversed (\"europ\u00e9enne\n\u00e9conomique zone\") as compared to English. Every other word in the sentence is in\nsimilar order.\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Transformer\n\u2022 Attention is all you need\nEncoder-Decoder architecture\nThe Illustrated Transformer\nNx\nAdd & Norm\nFeed\nForward\nOutput\nProbabilities\nSoftmax\nLinear\nAdd & Norm\nFeed\nForward\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nMasked\nMulti-Head\nAttention\nNx\nPositional\nEncoding\nPositional\nEncoding\nInput\nEmbedding\nOutput\nEmbedding\nInputs\nOutputs\n(shifted right)\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Multi-Head Attention\nAttention(Q, K, V) = softmax(\nQKT\n-) V\n\u221adk\nScaled Dot-Product Attention\nMulti-Head Attention\nLinear\nMatMul\nSoftMax\nMask (opt.)\nScale\nMatMul\nConcat\nScaled Dot-Product\nAttention\nh\nLinear\nLinear\nLinear\n\u2191\nK\nV K\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-5-columbia-Fall2024.pdf": {
        "text": "\u2022\nAttention in Transformer models\nThe Transformer uses multi-head attention in three different ways:\nIn \"encoder-decoder attention\" layers, the queries come from the previous decoder\nlayer, and the memory keys and values come from the output of the encoder. This\nallows every position in the decoder to attend over all positions in the input\nsequence. This mimics the typical encoder-decoder attention mechanisms in\nsequence-to-sequence models.\nThe encoder contains self-attention layers. In a self-attention layer all of the keys,\nvalues and queries come from the same place, in this case, the output of the\nprevious layer in the encoder. Each position in the encoder can attend to all\npositions in the previous layer of the encoder.\nSimilarly, self-attention layers in the decoder allow each position in the decoder to\nattend to all positions in the decoder up to and including that position. We need to\nprevent leftward information flow in the decoder to preserve the auto-regressive\nproperty. We implement this inside of scaled dot-product attention by masking out\n(setting to -\u221e) all values in the input of the softmax which correspond to illegal\nconnections.\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-5-columbia-Fall2024.pdf": {
        "text": "-\nTransformer \u2013 a parallelizable encoder-\ndecoder architecture\nENCODER\nENCODER\nENCODER\nOUTPUT\n| am a student\nDECODER\nDECODER\nENCODER\nDECODER\nENCODER\nDECODER\nENCODER\nDECODER\nENCODER\nINPUT Je suis \u00e9tudiant\nDECODER\nThe Illustrated Transformer - Part2\nFeed Forward\nSelf-Attention\nDECODER\nFeed Forward\nEncoder-Decoder Attention\nSelf-Attention\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Flow of Tensors in Transformers\nENCODER #2\nENCODER #1\nr1\nFeed Forward\nNeural Network\nZ1\nSelf-Attention\nr2\nFeed Forward\nNeural Network\nZ2\nX1\nThinking\nX2\nMachines\nThe word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the\nexact same network with each vector flowing through it separately.\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Self-Attention Steps\n1. create three vectors (query, key, value) from each of\nthe encoder's input vectors\n2. create a score for each input vector and pass\nnormalized score through a softmax\nInput\nThinking\nInput\nThinking\nMachines\nEmbedding\nX1\nX2\nEmbedding\nX1\nX2\nQueries\n91\n92\nQueries\n91\n92\nWQ\nKeys\nk1\nK2\nValues\nV1\nV2\nKeys\nk1\nK2\nWK\nScore\nValues\nV1\nV2\nMachines\n91 \u2022 k\u2081 = 112\n.\n91 K2=96\nWV\nDivide by 8 (\u221a\u221adk)\n14\n12\nSoftmax\n0.88\n0.12\nMultiplying x1 by the WQ weight matrix produces q1, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\",\nand a \"value\" projection of each word in the input sentence.\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Self-Attention Steps\n3. multiply each value vector by the softmax\nscore and sum the weighted value vectors\nInput\nThinking\nEmbedding\nX1\nX2\nQueries\n91\n92\nKeys\nk1\nk2\nValues\nV1\nScore\nDivide by 8 (\u221adk )\nSoftmax\nSoftmax\nV1\nValue\nV2\nMachines\n91 \u2022 k\u2081 = 112\n91 \u2022 k\u2082 = 96\n14\n12\n0.88\n0.12\nV2\nSum\nZ1\nZ2\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Matrix calculation of self-attention\nX\nWQ\n\u00d7 = = =\nX\nWK\nK\n\u2014 \u00d7 = = =\nX\nWV\nV\n\u2014 \u00d7 = = =\nsoftmax\n||\nZ\n\u221adk\nKT\nV\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Multi-head Attention\nQo\nATTENTION HEAD #0\nThinking\nMachines\nX\nQ1\nWOQ\nATTENTION HEAD #1\nW\u2081Q\nKo\nK1\nWok\nW\u2081K\nVo\nV\u2081\nWov\nW\u2081V\nThinking\nMachines\nATTENTION\nHEAD #0\nATTENTION\nHEAD #1\nZo\nZ1\nX\nCalculating attention separately in\neight different attention heads\nATTENTION\nHEAD #7\nZ7\n1) Concatenate all the attention heads\nZo\nZ1\nZ2\nZ3 Z4\nZ5 Z6 Z7\n2) Multiply with a weight\nmatrix Wo that was trained\njointly with the model\n3) The result would be the Z matrix that captures information\nfrom all the attention heads. We can send this forward to the FFNN\nZ\nWo\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Self attention summary\n1) This is our 2) We embed\ninput sentence* each word*\n3) Split into 8 heads.\nWe multiply X or\nR with weight matrices\n4) Calculate attention\nusing the resulting\nQ/K/V matrices\n5) Concatenate the resulting Z matrices,\nthen multiply with weight matrix W\u00b0 to\nproduce the output of the layer\nWOQ\nX\nWOK\nQo\nThinking\nMachines\nWov\nKo\nZo\nVo\nWo\n* In all encoders other than #0,\nwe don't need embedding.\nWe start directly with the output\nof the encoder right below this one\nR\nW\u2081Q\nW\u2081K\nZ\u2081\nW\u2081V\nV\u2081\nW7Q\nQ7\nW7K\nW7V\n074707\nZ7\nK7\nV7\nN\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Positional Encoding\nE\nENCODER #1\nENCODER #0\nEMBEDDING\nWITH TIME\nSIGNAL\nX1\nPOSITIONAL\nt\u2081\nENCODING\nEMBEDDINGS\nX1\nINPUT\n=\nX2\nt2\nX3\n=\nta\n+\nJe\nsuis\nX3\n\u00e9tudiant\nDECODER #1\nDECODER #0\n8888888888\nA real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in\nhalf down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is\ngenerated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.\n-0.4\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-5-columbia-Fall2024.pdf": {
        "text": "ENCODER #1\nResidual connection\nFeed Forward\nAdd & Normalize\nFeed Forward\nZ1\nZ2\nAdd & Normalize\nX\nZ\nLayerNorm(\n+\n)\nZ1\nX1\nPOSITIONAL\nENCODING\nSelf-Attention\nZ2\nX2\nENCODER #2\nENCODER #1\nFeed Forward\nFeed Forward\nPOSITIONAL\nENCODING\nAdd & Normalize\nFeed Forward\nAdd & Normalize\nSelf-Attention\nAdd & Normalize\nFeed Forward\nAdd & Normalize\nSelf-Attention\nX1\nX2\nX1\nX2\nThinking\nMachines\nThinking\nMachines\nDECODER #1\nFeed Forward\nSoftmax\nLinear\nDECODER #2\nAdd & Normalize\nFeed Forward\nAdd & Normalize\nEncoder-Decoder Attention\nAdd & Normalize\nSelf-Attention\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Decoder at first time step\nDecoding time step: 1 2 3 4 5 6\nOUTPUT\nEMBEDDING\nWITH TIME\nSIGNAL\nEMBEDDINGS\nENCODER\nENCODER\nINPUT\nJe\nsuis\n\u00e9tudiant\nLinear + Softmax\nDECODER\nDECODER\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Decoder over time steps\nDecoding time step: 1 2 3 4 5 6\nOUTPUT\nEMBEDDING\nWITH TIME\nSIGNAL\nEMBEDDINGS\nENCODERS\nKencdec Vencdec\nLinear + Softmax\nINPUT\nJe\nsuis\n\u00e9tudiant\nPREVIOUS\nOUTPUTS\n|\nDECODERS\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Final linear layer and softmax\nWhich word in our vocabulary\nis associated with this index?\nam\nGet the index of the cell\nwith the highest value\n(argmax)\n5\nlog_probs\n0 1 2 3 4 5\nlogits\n0 1 2 3 4 5\nSoftmax\nvocab_size\nvocab_size\nLinear\nDecoder stack output\nThis figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Transfer Learning in NLP\n\u2022\nUsed in standard NLP tasks\n\u2022 Question answering\n\u2022\nText generation\n\u2022 Text summarization\n\u2022 Named Entity Recognition.\n\u2022\nKey value pair identification\n\u2022 Transfer learning in NLP through\n\u2022 Finetuning language models for down stream tasks\n\u2022\nGenerating contextualized word embeddings\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Pre-Training Model Architectures in NLP\nT\u2081\nBERT (Ours)\n\u03a4\u03b9\nTrm\nTrm\nTrm\nTN\nT\u2081\nOpenAI GPT\n...\nTN\nELMO\nT\u2082\nTN\n...\nTrm\nTrm\nTrm\n...\nTrm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nTrm\nTrm\nTrm\nTrm\n...\nTrm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nE\u2081\nE\u2082\nE\u2081\nE\u2082\nEN\nE\u2081\nE\u2082\nEN\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Using Transformer for Transfer Learning in\nNLP\nTransformers handle long-term dependencies better than LSTMs\nEncoder-Decoder structure of the transformer made it perfect for\nmachine translation\n\u2022 How to pre-train a language model using transformer-based\narchitecture (with attention mechanisms and faster training due to\nparallelization) that can be finetuned for downstream supervised\nlearning tasks?\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-5-columbia-Fall2024.pdf": {
        "text": "BERT (Bidirectional Encoder Representations\nfrom Transformers)\n.\n.\n\u2022 OpenAl Transformer only trains a forward language model while\nELMO's model was bidirectional\nBERT is a transformer-based model whose language model is\nconditioned on both left and right contexts\nBERT is basically a trained Transformer Encoder stack\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-5-columbia-Fall2024.pdf": {
        "text": "BERT Features\n\u2022 Training corpus was comprised of two entries\n.\n\u2022\nToronto Book Corpus (800M words), and\n\u2022\nEnglish Wikipedia (2,500M words)\n\u2022 Two versions of BERT (L stands for the number of layers, H\nstands for the hidden size, A stands for the number of self-\nattention heads)\n\u2022\n\u26ab BERT-Base: L = 12, H = 768, A = 12, Total parameters = 110M\nBERT-Large: L = 24, H = 1024, A = 16, Total parameters = 340M\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Input Representation of BERT\nInput\n[ [CLS] my dog is cute [SEP] he likes play ##ing [SEP]]\nToken\nEmbeddings\nSegment\nEmbeddings\nPosition\nEmbeddings\nE[CLS] Emy\nEdog\nEis Ecute [SEP]\nEhe E likes\nEssing\nplay\n##ing E[SEP]\nEEEEEE66666\n+\n\u0118 \u0118, \u01182 E3 \u0118, \u0118s \u0118 \u0118, \u0118 \u0118, E\u2081o\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-5-columbia-Fall2024.pdf": {
        "text": "-\nBERT Training \u2013 Predicting word at a position\n\u2022 Masked language model\n(Masked LM)\nThat's [mask] she [mask] -> That's what she said\n\u2022 Randomly masks 15% of\ntokens in the input and\nmodel is trained to predict\nthe masked word\nUse the output of the\nmasked word's position\nto predict the masked word\n....\n0.1% Aardvark\nPossible classes:\nAll English words\n10% Improvisation\n0%\nZyzzyva\nFFNN + Softmax\n3\n4\n5\nBERT\n512\nRandomly mask\n15% of tokens\n11 21 31 41 51 61 71 81\n...\n512\n[CLS] Let's stick\nto [MASK] in\nthis skit\nInput\n[CLS]\nLet's stick\nto improvisation in\nthis\nskit\nBERT's clever language modeling task masks 15% of words in the input and asks the model to predict the missing word.\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-5-columbia-Fall2024.pdf": {
        "text": "\u25cf\nBERT Training \u2013 Two sentence tasks\nNext sentence prediction (NSP)\n\u2022 Task: Given two sentences (A\nand B), is B likely to be the\nsentence that follows A, or not?\n(binary classification)\nInput = [CLS] That's [mask] she [mask]. [SEP] Hahaha, nice! [SEP]\nLabel = IsNext\nPredict likelihood\nthat sentence B\nbelongs after\nsentence A\n1%\nIsNext\n99% NotNext\nFFNN + Softmax\n1 2 3 4 5 6 7 8\nBERT\n512\nInput = [CLS] That's [mask] she [mask]. [SEP] Dwight, you ignorant [mask]! [SEP]\nLabel = NotNext\nTokenized\nInput\n512\n[CLS]\nthe\nman [MASK]\nto\nthe\nstore [SEP]\nInput\n[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flightless birds [SEP]\nSentence A\nSentence B\nThe second task BERT is pre-trained on is a two-sentence classification task. The tokenization is oversimplified in this graphic as BERT\nactually uses WordPieces as tokens rather than words --- so some words are broken down into smaller chunks.\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Using BERT for different Downstream Tasks\nClass\nLabel\nClass\nLabel\nC\nT\u2081\nTN\nT\nSEP\n\u0642\u0645\nT\u2081\nT\u2082\nTN\nBERT\nBERT\nE\u2081\nEN ESEP E\u2081'\nEv\nFas E, E,\nEN\n[(CLS) Tok 1 Tok 2\nTok N\nSentence 1\nSentence 2\n(a) Sentence Pair Classification Tasks:\nMNLI, QQP, QNLI, STS-B, MRPC,\nRTE, SWAG\nStart/End Span\nCT- Tv Tsenty-\nBERT\nSingle Sentence\n(b) Single Sentence Classification Tasks:\nSST-2, COLA\nO\nB-PER\nBERT\n0\n\u0415\u0443\u0441\u0438\u044f\nE\nEx Esem E, Ev\nECLS\nE\u2081\n\ud1a0\nEN\n0\nTak\nTak\nTak\nCLS\nSEP\n[CLS]\nTok 1\nTok 2\nTok N\n1\nN\n1\nM\nParagraph\nQuestion\n(c) Question Answering Tasks:\nSQUAD v1.1\nSingle Sentence\n(d) Single Sentence Tagging Tasks:\nCONLL-2003 NER\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Fine-tuning BERT for Sentiment Analysis\nSentiment Analysis with BERT colab tutorial\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-5-columbia-Fall2024.pdf": {
        "text": "BERT for Feature Extraction\n\u2022\nUsing BERT to create contextualized word embeddings\nGenerate Contexualized Embeddings\nThe output of each encoder layer along\neach token's path can be used as a\nfeature representing that token.\n\u2022\n|\n| 12\n|\n2\nENCODER\nENCODER\n1\nENCODER\n1\n2\n3\n4\n[CLS] Help\nPrince Mayuko\nBERT\n...\n512\nHelp\nPrince\nMayuko\nBut which one should we use?\n\u2022\n12 embedding vectors for\neach word from the\noutput of 12 encoder\nlayers\nWhich embedding(s) to\nuse for a given task?\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Different ways to create contextualized\nembeddings for a word using BERT\nWhat is the best contextualized embedding for \"Help\" in that context?\nFor named-entity recognition task CoNLL-2003 NER\nHelp\nFirst Layer\nEmbedding\nLast Hidden Layer\n12\n12\nSum All 12\nLayers\nSecond-to-Last\n11\nHidden Layer\n12\n11\nSum Last Four\n10\nHidden\n9\nConcat Last\nFour Hidden\n=\nDev F1 Score\n91.0\n94.9\n95.5\n95.6\n95.9\n9\n10\n11\n12\n96.1\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-5-columbia-Fall2024.pdf": {
        "text": "OpenAl Transformer\nPre-training a transformer\ndecoder for language modeling\nDecoder masks future tokens so\ncan be trained on next word\nprediction task\nModel has 12 decoder layers\nstacked\n\u2022 Only masked self attention layers\nPossible classes:\nAll English words\n172\n12\n0.1% Aardvark\n...\n10% Improvisation\n...\n0% Zyzzyva\nFFNN + Softmax\nDECODER\n2\nDECODER\n1\nDECODER\n1\n19\nLet's\nstick\nto\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-5-columbia-Fall2024.pdf": {
        "text": "Transfer Learning to Downstream Tasks\n85% Spam\nClassification\nStart\nText\nExtract\nTransformer\nLinear\n15% Not Spam\nEntailment\nStart\nPremise\nDelim\nHypothesis Extract Transformer\nLinear\nFFNN + Softmax\n11 21 31 41\nOpenAI\nTransformer\n<S> Help\nPrince Mayuko\n<e>\n... 512\nStart\nText 1\nDelim\nText 2\nExtract\nTransformer\nSimilarity\nLinear\nStart\nText 2\nDelim\nText 1\nExtract\nTransformer\nStart\nContext\nDelim\nAnswer 1\nExtract\nTransformer\nLinear\nMultiple Choice\nStart\nContext\nDelim\nAnswer 2\nExtract\nTransformer\nLinear\nStart\nContext\nDelim\nAnswer N Extract\nTransformer\nLinear\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-7-Columbia.pdf": {
        "text": "Lecture 7\nParijat Dube, Chen Wang\n+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\n+\n1\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-7-Columbia.pdf": {
        "text": "Agenda\n\u2022 Prompt Engineering and LLM App\nDevelopment\nUse Cases of LLM\n\u25cf\nPrompt Engineering\n\u25cf\nPrompt Engineering Techniques\nLLM App Development Frameworks\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-7-Columbia.pdf": {
        "text": "What are the LLM Products you have used?\nChatGPT\nGemini\n\u091a\u0941\nMeet\nClaude\nA next-generation AI assistant for your tasks, no\nmatter the scale\nMeta\nLlama 3.1 405B\nW\nMISTRAL\nAI_\n8X7\nMixtral of experts\nDigi\u0245lps\nwatsonx.ai\nGRANITE MODELS\nNOW OPEN SOURCE\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-7-Columbia.pdf": {
        "text": "Prompts and completions\nPrompt\nWhat is Large\nLanguage Model?\nModel\nContext window: 2k - 100K\nLLM\nCompletion\nWhat is Large Language\nModel?\nA Large Language Model\n(LLM) is a type of artificial\nintelligence (Al) model\ndesigned to understand,\ngenerate, and manipulate\nhuman language on a\nlarge scale. These models\nare typically built using\ndeep learning techniques,\nparticularly neural\nnetworks, and are trained\non massive amounts of\ntext data to learn the\nstatistical patterns and\nstructures of language.\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-7-Columbia.pdf": {
        "text": "What are the LLM\nuse cases you have\nseen?\n\u2022 Other Use Cases\n\u2022\nTranslate\n\u2022\nSummarize\n\u2022 Proof-read and correct\n\u2022\nExplain words\n\u2022\n\u2022\nCreate article/email\nMake restaurant suggestions\n\u2022\nChat with users\n\u2022 Do Math Calculations\n\u2022 Answer Questions on many subjects\n. . .\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-7-Columbia.pdf": {
        "text": "LAVENDER\nEmail like\nit's magic\nWS\nWritesonic\nCertainly! Here's a di\nRevolutionizing CRM with Al\nSubje\nUnpacking Salesforce's\nEinstein GPT\nDear F\nEinsteinGPT\nDrive more pipeline\n90\nYou're in good shape!\nCOACHING I\n2x your reply rates\nAbout\nPERSONALIZATION\nEmile Peterson\nHead of Success Cl\n& Personality\nActivity\nTRACKING\nI hope\nAs Th\nsalesforce\nguidar\ndeeply\nWishir\n<- Orals\nBest r\nTones detected\nBook more meetings\n#Happy #Friendly\nSubject\nClarity\nCould you help me draft an email saluting my PhD advisor Prof. Kim\nfor ThanksGiving?\nSimplified\nChatGPT Writer\nFree Chrome extension to generate entire emails and messages using ChatGPT AI. All sites are\nsupported and enhanced support for Gmail.\nDownload Free Extension\nThank\n[Your Name]\n70,000+ happy users who don't like writing emails\nHighest quality responses that outcompete all other tools in the market\nWorks in all Chromium-based browsers, such as Chrome, Brave, and Edge\n3 \ud22c\nur\nS.\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-7-Columbia.pdf": {
        "text": "NLLB-200\n200-Language\nTranslation Model\nSMARTLING\n.ation\nG+ Google\n\u6587\npairaph Translate\nranslate \"\u585e\u7fc1\u5931\u300d,\niss\ncan be translate\nBIJLAVY\nt who\namazon Translate\ne fable a\nthat what\ng about\nFalcon LLM\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-7-Columbia.pdf": {
        "text": "IBM\nWrite Flask coc watsonx\nGitHub\nCopilot\nresponse with 1\nrequest.\nCode Assistant\na simple Flask API that returns a JS\nsed via a GET request:\n</>\nO COBOL\nSTRING\nORG-ID DEPT-NUMBER\nDELIMITED BY SIZE\nINTO DEPT-ID\nEND-STRING\nJava\nEXEC SOL\naws\nAmazon Q\nDeveloper\npython\nfrom flask import Flask, jsonify\ntabnine\nAl Assistant For Software Developers\nMeet Cody,\nthe most powerful\n& accurate Al coding\nassistant\nLoginDialog.ts\n\u05d5\u05df!\nExplain code\nCreate unit tests\nSmell code\n- name: Permit access to resources\nContent generation\nTransform code\nPreparedStatement stmt-\nconn.prepareStatement (\n\"UPDATE DEPT SET MGRND 5\nWHERE DEPTNO = ?\");\nstmt.setString(1, orgId+deptNumber):\nint changedDepts -\nstat.executeUpdate();\nAnsible\nibm.ibm.zos core.zos_tso_command:\ncommands: \"{{ lookup('template', '../templates/\nupdate-resource-access. 12') }}*\nwith items: \"If resources to permit }}\"\nwhen: resources to permit is defined and\nresources to permit and resources_to_permit |\nlength > 0\nGenerate code\nCURSOR\nAI\napp.run(debug=True)\nbindal\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-7-Columbia.pdf": {
        "text": "notta\nDashboard / Project Meeting\n\u0e04\n+ AI Notes\nTranscript\nShare\nBoost productivity with\nthe leading Al notetaker\nOOOO\nSummary\nThe project manager gathered updates from the\npartners regarding the progress of the project.\nProject member reported significant progress in\nimplementing new features.\nChapters\n00:00:34\nWade Warren 00:12\nHi Team. As we're nearing the project's halfway\nall our partners to discuss our progress and add\nchallenges.\nAre The Benefits Of Sum x+\nadin.com/pulse/what-benefits-summarization-dr-v-s-gayathri\nAre The Benefit\nV.S Gayathri\nGist Al\nWebsite\nPDF\nAction Items\nJenny Wilson\nX\n0\nupword\nified Dyslexia Counselor-An E\nnts to help children with learnin\nished Mar 7, 2023\nition is an important aspect\nprofessionals, it plays a cru\nation can be explained as t\ntext into a smaller, concise,\nader the important informati\nt clear.\nation of summarization syste\nrstanding of an article, savin\n1-gathering process, reducir\nnderstand (in educational co\ny in business settings.\nummarization help studen\nng helps them to learn the te\ny learn to ignore irrelevant in\ntegrate the central ideas in\nsing. Students who master th\n, and become more skillful i\nhe major benefits of summ\nLike\nComment\n\u2022 Summarization is important for communication and\ninteraction, condensing text into a concise version.\n\u26abIt helps readers understand articles, saves time for\nresearchers, and improves memory for students.\nRead More...\n\u2022 Summarizing improves learning skills, helps with\nstudying, enhances concentration, and improves\ncommunication skills.\nRead More...\n\u2022 Summarization helps students condense large texts,\nimproving understanding, expression, and exploration\nof thoughts.\nRead More...\nsum\ni www\n-\u0f3d\nCX\nnents\n*\nARY\nSummarize Documents in\nSeconds\nEasily get a Summary of DOCX files as large as 500\npages or 150K words.\nSelect Document\nMedium Summary\nGet Summary\nSelect DOCX, PDF File &\nChoose Summary Size\nRead or Download\nSummary Easily\nironments.\non SWE-bench, including proprietary models and their\n*ming one (Claude 2), can resolve only a small fraction\n?\nen\nS\nd SWE-bench-train, which consists of 19,000 non-testing\nIon CodeLlama, which are specialized for repository-wide\nThe benchmark aims to push the boundaries of LM capabilities in practical and autonomous software engineering, providing a realistic and sustainable\nevaluation framework that can be continuously updated with new task instances.\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-7-Columbia.pdf": {
        "text": "krisp\nAI Meeting Assistant\n\u2713 Free unlimited transcriptions\n\u2713 Meeting notes and summaries\n\u2713 Meeting audio recording\nNote Taker\nEmily Good moming everyone\nplatforms. We've seen a\nweek. Our posts featur\nThar's great news, Emily! Users\nand credibility ve been analyzing\ntarget audience is responding wa\nSummary\nengagement boost in their social media\ncampaign emphasing the success of user testmonials Sarah\nhighlighted potive feedback and inte\neffectiveness\nWonderful to hear Emly and Sara\nbased on their preferences to per\nwill yield higher conversion rates\nLearn more about Krisp AI Meeting Assistant\ngranola\nThe Al Notepad for people in back-to-back meetings\nGranola Enhanced\nMy notes + live transcript\nIntro call: AllFound\n3:30pm Mike +2\nIntro call: AllFound\n3:30pm\nMike +2\n100, growingg\nuse tuesday.al, v manual,\n180\nMaked pin in the upcoming al\ncampaign and ongoing trend research\n\"a priority for q2\"\nMeeting\n11\nAllFound Overview\n100 employees, adding 20 more next quarter\nOffice in San Francisco and Austin\nCurrent Provider (Tuesday.ai)\nData input is too manual\n\u2022Too complex for non-technical team members\n\u26ab $180 per employee per year\nTheir Requirements\nIt's \"a priority for Q2\" to find a better\nemployee engagement tool\n\u2022 Need secure information sharing capabilities\nCircleback,\nCircleback\nAutomated meeting notes and follow-ups powered by Al.\nNever take meeting notes\nagain.\n950\nOll\u26ab1\nnotta\nSummarization Tool\nOtter.ai\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-7-Columbia.pdf": {
        "text": "G grammarly\nrrect ProWritingAid\nCan you proof-read and correct this sentence?\nto th\n\u00a8uits and vegetables.'\nHemingway\nEditor\nWhiteSmoke\nJust write.\n\u304f\nOutwrite\nOutwrite\nOutwrite\nfor Word\nSPELLING GRAMMAR STYLE STRUCTURE\n\u25cfFix spelling\nneccessary\n\u25cfSimplify phrase\nIt was an unusual thing for him to do\n0\nm\nCorrections\nRewrite\nThesaurus\nW\nWhiteSmoke Writer\nQ Search\nHe painted with unbeleivable precision\nHe used to actually hire real soldiers\nfor his paintings. He onces said, \"I\nwishes to be remembered as the best\npainter of all time. But this wasn't\nmeantto be.despite his well\nestablished renown in the past his\nwork is virtually unnown today. Other\npainters which were the antithesis to\nEnrichment\nChoose Grammar Correction\ncentury,\nand vegetab\n< Prev\n\u3141\nNext>\nGinger\nGinger\nGINGER\nStart writing perfectly\nA\nwww\nLiven\nPeo\nGrammar\nStart\n\u56de Quick Tutoria\nSynonyms\nPrunale\nChecker\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-7-Columbia.pdf": {
        "text": "COGNIGY\naction\n[\nAmazon SageMaker\nJumpStart\nAccess and try out public\nand proprietary foundation\nmodels, and easily customize\nand integrate them into your\ngenerative Al applications\n00\n00\n00\nBrowse\nBrowse public\nand proprietary\nfoundation models\n---\nExperiment\nExperiment with\nfoundation models\nbefore choosing a\nmodel for deployment\nCustomize\nEasily customize selected\nfoundation model with\nyour own dataset without\ntraining from scratch\nTo-\nDeploy\nDeploy the model and\nrun inference for your\ngenerative Al use case\n*Your training and inference data is not shared by nor used by AWS or foundation model providers\nExtra\nowing paragraph,\nincluding names of people, organizations, locations, dates, and\nany other relevant information. For each entity, provide its type\n(e.g., person, organization, location, date) and the text\nassociated with it. Output the results in a list of JSON objects,\nwhere each object contains the entity and its type.\nIon Stoica (born 1964 or 1965) is a Romanian-American\ncomputer scientist specializing in distributed systems, cloud\ncomputing and computer networking.[9][2][10][11] He is a\nprofess\nBerkele\nter science at the University of California,\nab co-founded Conviva\nOnetow\nand Databricks with other original developers of Apache\nSpark.[6][12] As of April 2022, Forbes ranked him and Matei\nZaharia as the 3rd-richest people in Romania with a net worth\nof $1.6 billion.[13]\n},\n{\n\"type\": \"occupation\",\n\"text\": \"computer scientist\"\n\"entity\": \"distributed system\nield\",\nSINTELIX\nistributed systems\"\n'University of Califo\norganization\",\nniversity of Califorr\nLangChain\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-7-Columbia.pdf": {
        "text": "User Prompt\nChat Interface\nAPI call to\nOpen Al\nThird Party API\nJson Format\nFunction calling\ntus of Ch\nUser Prompt\nat it is c\u0269\n+\nFunction Description\n>r real-tin\nOpen AI APIS\nboomi\nPower\nAutomate\nDocuSign\nWoo\nab\nzapier\nRazorpay earched 4 sites\nne weather in New York City today\n_teperature is expected to reach a\nmph. The evening will be mostly cle\nTig and variable tonight, providing\nWeather Service) (Time and Date).\n\u2665 workato\n13\na\nshopify\n0\naws\nBB\n\u7530\n& many more\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-7-Columbia.pdf": {
        "text": "Math Calculation\nLet's solve this step-by-step:\n1) First, let's set up the multiplication:\n237\n\u00d7 56\n2) We'll multiply 237 by 6 (the ones digit of 56):\n23761422\n3) Now we'll multiply 237 by 50 (the tens digit of 56):\n23750 11850\n=\n4) Now we add these two results:\n1422\n+ 11850\n13272\nTherefore, 237 \u00d7 56 = 13,272.\nAl\nWould you like me to explain any part of this calculation in more detail?\nTo multiply 237 by 56, you can follow these steps:\n### Step 1: Break down the numbers\n- Write the numbers in a column format:\n237\n\u00d7 56\n### Step 2: Multiply 237 by 6 (the ones digit of 56)\n- \\( 237 \\times 6 = 1422\\)\n### Step 3: Multiply 237 by 5 (the tens digit of 56), but remember it's actually 50\n- \\(237 \\times 50 = 11850 \\)\n- You can calculate it as:\n- \\( 237 \\times 5 = 1185 \\)\n- Then, multiply by 10 to get \\( 11850 \\)\n### Step 4: Add the results of the two multiplications\n- Align the results according to their place value:\n1422\n+11850\n13332\n### Final Result\nThe product of 237 and 56 is **13,332**.\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-7-Columbia.pdf": {
        "text": "Prompting and Prompt Engineering\nPrompt\nWhat is Large\nLanguage Model?\nModel\nContext window: 2k - 100K\nLLM\nYou may have to revise the language in your prompt or the way that it's written\nseveral times to get the model to behave in the way that you want.\nThis work to develop and improve the prompt is known as prompt engineering.\nCompletion\nWhat is Large Language\nModel?\nA Large Language Model\n(LLM) is a type of artificial\nintelligence (Al) model\ndesigned to understand,\ngenerate, and manipulate\nhuman language on a\nlarge scale. These models\nare typically built using\ndeep learning techniques,\nparticularly neural\nnetworks, and are trained\non massive amounts of\ntext data to learn the\nstatistical patterns and\nstructures of language.\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-7-Columbia.pdf": {
        "text": "What is a prompt?\nprompt:\nA prompt is an input that\nproduces the desired output.\nExample:\n\u2022 Write a small paragraph\ndescribing your favorite\nholiday destination.\n\u2022Write HTML code to generate\na dropdown selection of cities\nwithin an online form.\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-7-Columbia.pdf": {
        "text": "What is a prompt?\nprompt:\nPrompts can also be a series of\ninstructions.\nExample:\nWrite a short story about a\nscientist studying life on\nMars.\n\u2022 What were some of the\nchallenges he faced during\nhis research?\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-7-Columbia.pdf": {
        "text": "What is a prompt?\nCollects\ninformation\nProvides\nDerives\ncreative\nsolutions\ninferences\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-7-Columbia.pdf": {
        "text": "Correct and incorrect prompts\nRich man's story\nfrom a small\ntown, his\nstruggles, and\nachievements.\nContext\nProper structure\nComprehensible\nWrite a short story\nabout the struggles\nand achievements\nof a farmer who\nbecame a rich and\ninfluential\nbusinessman in 10\nyears.\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-7-Columbia.pdf": {
        "text": "Correct and incorrect prompts\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-7-Columbia.pdf": {
        "text": "Correct and incorrect prompts\nSunset image\nbetween\nmountains.\nToo brief\n\u0425\nLacks detailed\noutline\nGenerate an image\ndepicting a calm\nsunset above a river\nvalley that rests\namidst mountains.\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-7-Columbia.pdf": {
        "text": "Building blocks of a well-constructed prompt\nInstructions\nContext\nInput data\nOutput indicator\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-7-Columbia.pdf": {
        "text": "Building blocks of a well-constructed prompt\nInstructions: Give distinct guidelines regarding the\ntask\nExample:\nWrite an essay in 600 words analyzing the effects of\nglobal warming on marine life.\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-7-Columbia.pdf": {
        "text": "Building blocks of a well-constructed prompt\nContext: Provides a framework for generating relevant\ncontent\nExample:\nIn recent decades, global warming has undergone\nsignificant shifts, leading to rising sea levels, increased\nstorm intensity, and changing weather patterns. These\nchanges have had a severe impact on marine life. Write\nan essay in 600 words analyzing the effects of global\nwarming on marine life.\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-7-Columbia.pdf": {
        "text": "Building blocks of a well-constructed prompt\nInput data: Any piece of information provided as part\nof prompt\nExample:\nYou have been provided with a data set containing\ntemperature records and measurements of sea levels\nin the Pacific Ocean. Write an essay in 600 words\nanalyzing the effect of global warming on marine life\nin the Pacific Ocean.\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-7-Columbia.pdf": {
        "text": "Building blocks of a well-constructed prompt\nOutput indicator: Offers benchmarks for assessing\nattributes of the output\nExample:\nThe output generated should be an essay of 600\nwords. It will be evaluated based on the clarity of\nanalysis and incorporation of relevant data or case\nstudies.\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-7-Columbia.pdf": {
        "text": "Building blocks of a well-constructed prompt\nInstructions: Give\ndistinct guidelines\nregarding the task\nContext: Provides\nframework for\ngenerating relevant\ncontent\nInput data: Any piece\nof information provided\nas part of prompt\nOutput indicator:\nOffers benchmarks for\nassessing attributes of\nthe output\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-7-Columbia.pdf": {
        "text": "What Is Prompt Engineering?\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-7-Columbia.pdf": {
        "text": "Prompt engineering\nProcess of designing effective\nprompts.\nCritical analysis\nCreativity\nTechnical acumen\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-7-Columbia.pdf": {
        "text": "Process involved in prompt engineering\nWell-structured iterative process\nDefine the goal\nCraft initial prompt\nAnalyze the\nresponse\nTest the prompt\nIterate the process\nRefine the prompt\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-7-Columbia.pdf": {
        "text": "Process involved in prompt engineering\nDefine the goal:\nExample: Form a brief overview of the benefits and risks\nassociated with artificial intelligence in automobiles.\nCraft initial prompt\nExample: Write an article that presents a well-rounded\nanalysis of the benefits and drawbacks associated with the\nincorporation of artificial intelligence in the field of\nautomobile industry.\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-7-Columbia.pdf": {
        "text": "Process involved in prompt engineering\nTest the prompt\nExample: Write an article that presents a well-rounded\nanalysis of the benefits and drawbacks associated with the\nincorporation of artificial intelligence in the field of automobile\nindustry.\nBenefits\n\u2611 Ethical concerns\nDrawbacks\n\u2611 Positive and negative implications\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-7-Columbia.pdf": {
        "text": "Process involved in prompt engineering\nAnalyze the response\nFails to cover: Comprehensive range of benefits and risks\nassociated with artificial intelligence in the automobile\nindustry.\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-7-Columbia.pdf": {
        "text": "Process involved in prompt engineering\nRefine the prompt\nExample: Write an informative article discussing the role of\nartificial intelligence in revolutionizing the automobile industry.\n\u2022 Address key aspects such as benefits, drawbacks, ethical\nconsiderations, and both positive and negative implications.\n\u2022 Cover specific domains like autonomous driving and real-time\ntraffic analysis, while also examining potential challenges\nsuch as technical complexity and cybersecurity concerns.\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-7-Columbia.pdf": {
        "text": "Process involved in prompt engineering\nIterate the process:\nExample: Write an article highlighting how artificial\nintelligence is reshaping the automobile industry focusing on\nthe positive advancements, particularly in autonomous\ndriving and real-time traffic analysis, while thoroughly\nexploring concerns related to intricate technical aspects\nsuch as decision-making algorithms and potential\ncybersecurity breaches. Emphasize the implications these\nconcerns may have on vehicle safety. Ensure that the\nanalysis is thorough, backed with examples, and encourages\ncritical thinking.\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-7-Columbia.pdf": {
        "text": "In-context learning (ICL) \u2013 zero shot inference\nPrompt\nClassify this review:\nI loved this restaurant!\nSentiment: Positive\nClassify this review: I don't like the\nenvironment the restaurant has. The\ntables look old and oily. Sentiment:\n-\nModel\nLLM\nCompletion\nThe sentiment of this review is\nnegative. The reviewer expresses\ndissatisfaction with the\nrestaurant's environment,\nspecifically mentioning the old and\noily tables.\nZero-shot Inference\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-7-Columbia.pdf": {
        "text": "In-context learning (ICL) - zero shot inference\nPrompt\nClassify this review:\nI loved this movie!\nSentiment:\nZero-shot inference\nModel\nLLM\nCompletion\nClassify this review:\nI loved this movie!\nSentiment: Positive\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-7-Columbia.pdf": {
        "text": "In-context learning (ICL) - one shot inference\nPrompt\nClassify this review:\nI loved this movie!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment:\nOne-shot inference\nModel\nLLM\nCompletion\nClassify this review:\nI loved this movie!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment: Negative\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-7-Columbia.pdf": {
        "text": "In-context learning (ICL) - few shot inference\nPrompt\nModel\nClassify this review:\nLLM\nI loved this DVD!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment: Negative\nClassify this review:\nThis is not great.\nSentiment:\nCompletion\nClassify this review:\nI loved this DVD!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment: Negative\nClassify this review:\nThis is not great\nSentiment: Negative\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-7-Columbia.pdf": {
        "text": "Summary of in-context learning (ICL)\nPrompt // Zero Shot\nClassify this review:\nI loved this movie!\nSentiment:\nContext Window\n(few thousand words)\nPrompt // One Shot\nClassify this review:\nI loved this movie!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment:\nPrompt // Few Shot >5 or 6 examples\nClassify this review:\nI loved this movie!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment: Negative\nClassify this review:\nWho would use this\nproduct?\nSentiment:\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-7-Columbia.pdf": {
        "text": "Interview Pattern Approach\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-7-Columbia.pdf": {
        "text": "Working behind the approach\nThe interview pattern approach involves designing prompts by simulating a\nconversation or interacting in the interview style.\nUser provides prompt instructions\nModel asks necessary follow-up questions\nModel draws information from the response\nModel processes the information to provide\nan optimized solution\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-7-Columbia.pdf": {
        "text": "Example\nPrompt instructions:\n\"You will act as a seasoned travel expert. Your\nobjective is to engage in a comprehensive\ntrip-planning session with me. Begin by asking\na series of detailed questions, one at a time, to\ngather all the essential information required to\ncraft the most tailored and memorable travel\nitinerary based on my specific preferences,\ninterests, and budget.\"\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-7-Columbia.pdf": {
        "text": "Example\nFollow-up questions:\n01\n02\nWhat types of destinations do you\nenjoy traveling to the most?\nCould you describe your ideal vacation\nin terms of activities and experiences?\nHow do you typically plan your trips,\n03 and what factors are most important to\nyou when choosing a destination?\nDo you find any specific cultural or\n04 historical aspects intriguing when\nplanning your travel destination?\nWhat kind of accommodation options\n05 do you prefer when you travel, and\nwhy?\nHow do you balance budget\n06 considerations with the desire for a\nmemorable travel experience?\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-7-Columbia.pdf": {
        "text": "Chain-of-Thought Approach\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-7-Columbia.pdf": {
        "text": "Working behind the approach\nChain-of-Thought\nis a prompt-based\nlearning approach.\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-7-Columbia.pdf": {
        "text": "Working behind the approach\nIt involves breaking\ndown a complex\ntask into smaller and\neasier ones through\na sequence of more\nstraightforward\nprompts.\n310\n1139\n1 20\nChatGPT\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-7-Columbia.pdf": {
        "text": "Working behind the approach\nFeed the model with\nrelated questions\nalong with their\ncorresponding\nsolutions.\nPrompt includes:\n\u2022 Related question\n\u2022 Accurate solution\nto the question\n\u2022 Another question\nbased on the same\nreasoning\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-7-Columbia.pdf": {
        "text": "Example\nMathew has 6 eggs. He buys 2 more trays of eggs. Each\ntray has 12 eggs. How many eggs does he have now?\nQuestion: Mary has 8 radishes. She used 5 radishes to prepare\nthe dinner. The next morning, she bought 10 more radishes.\nHow many radishes does she have now?\nSolution: Mary had 8 radishes. She cooked dinner using 5 of\nthem. So, she had 8 - 5 = 3 radishes left with her. The next\nmorning, she bought 10 more. So, she has 3 + 10 = 13\nradishes now.\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-7-Columbia.pdf": {
        "text": "Example\nModel Input\nQ. Mary has 8 radishes. She used 5 radishes to prepare the\ndinner. The next morning, she bought 10 more radishes. How\nmany radishes does she have now?\nA. Mary had 8 radishes. She cooked dinner using 5 of them. So,\nshe had 8 - 5 = 3 radishes left with her. The next morning, she\nbought 10 more. So, she has 3 + 10 = 13 radishes now.\nQ. Mathew has 6 eggs. He buys 2 more trays of eggs. Each tray\nhas 12 eggs. How many eggs does he have now?\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-7-Columbia.pdf": {
        "text": "Model Input\nStandard Prompting\nQ: Roger has 5 tennis balls. He buys 2 more cans of\ntennis balls. Each can has 3 tennis balls. How many\ntennis balls does he have now?\nA: The answer is 11.\nQ: The cafeteria had 23 apples. If they used 20 to\nmake lunch and bought 6 more, how many apples\ndo they have?\nChain-of-Thought Prompting\nModel Input\nQ: Roger has 5 tennis balls. He buys 2 more cans of\ntennis balls. Each can has 3 tennis balls. How many\ntennis balls does he have now?\nA: Roger started with 5 balls. 2 cans of 3 tennis balls\neach is 6 tennis balls. 5 + 6 = 11. The answer is 11.\nQ: The cafeteria had 23 apples. If they used 20 to\nmake lunch and bought 6 more, how many apples\ndo they have?\nModel Output\nA: The answer is 27. \u2717\nModel Output\nA: The cafeteria had 23 apples originally. They used\n20 to make lunch. So they had 23 - 20 = 3. They\nbought 6 more apples, so they have 3+ 6 = 9. The\nanswer is 9.\nWei, Jason, et al. \"Chain-of-thought prompting elicits reasoning in large language models.\" Advances in neural\ninformation processing systems 35 (2022): 24824-24837.\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-7-Columbia.pdf": {
        "text": "(a) Few-shot\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis\nballs. Each can has 3 tennis balls. How many tennis balls does\nhe have now?\nA: The answer is 11.\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls,\nand half of the golf balls are blue. How many blue golf balls are\nthere?\nA:\n(Output) The answer is 8. X\n(c) Zero-shot\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls,\nand half of the golf balls are blue. How many blue golf balls are\nthere?\nA: The answer (arabic numerals) is\n(Output) 8X\n(b) Few-shot-CoT\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis\nballs. Each can has 3 tennis balls. How many tennis balls does\nhe have now?\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6\ntennis balls. 5+ 6 = 11. The answer is 11.\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls,\nand half of the golf balls are blue. How many blue golf balls are\nthere?\nA:\n(Output) The juggler can juggle 16 balls. Half of the balls are golf\nballs. So there are 16/2 = 8 golf balls. Half of the golf balls are\nblue. So there are 8/2 = 4 blue golf balls. The answer is 4. \u2714\n(d) Zero-shot-CoT (Ours)\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls,\nand half of the golf balls are blue. How many blue golf balls are\nthere?\nA: Let's think step by step.\n(Output) There are 16 balls in total. Half of the balls are golf\nballs. That means that there are 8 golf balls. Half of the golf balls\nare blue. That means that there are 4 blue golf balls. \u2714\nKojima, Takeshi, et al. \"Large language models are zero-shot reasoners.\" Advances in neural information\nprocessing systems 35 (2022): 22199-22213.\n",
        "links": []
    },
    "./chunks/chunk_53_Lecture-7-Columbia.pdf": {
        "text": "Q: While shopping for music online, Zoe bought 3 ...\n:\nQ: A chef needs to cook 9 potatoes. He has already...\nClustering\n1\nk\nLLM\nDemo Construction\nAuto Demos One by One\nQ: While shopping for music online, Zoe bought 3 country albums and 5\npop albums. Each album came with a lyric sheet and had 3 songs. How\nmany songs did Zoe buy total?\nA: Let's think step by step. Zoe bought 3 country albums. Each album has 3\nsongs. So she bought 3*3=9 songs from the country albums. Zoe bought 5\npop albums. Each album has 3 songs. So she bought 5*3=15 songs from\nthe pop albums. Zoe bought 9+15=24 songs in total. The answer is 24.\nQ: A chef needs to cook 9 potatoes. He has already cooked 7. If each\npotato takes 3 minutes to cook, how long will it take him to cook the rest?\nA: Let's think step by step. The chef has already cooked 7 potatoes. That\nmeans it has taken him 7 * 3 minutes to cook those 7 potatoes. That means\nit will take him 3 more minutes to cook each of the remaining 2 potatoes ...\nQ: A pet store had 64 puppies. In one day they sold 28 of them and put\nthe rest into cages with 4 in each cage. How many cages did they use?\nA: Let's think step by step.\n1\nQ: While shopping for music online ... A: Let's ...\nTest Question\n:\nSampling by Selection Criteria\nk\nQ: A chef needs to cook 9 potatoes ... A: Let's ...\nLLM\nIn-Context Reasoning\nThe pet store had 64 puppies. They sold 28 of them. That means they have\n36 puppies left. They put the rest into cages with 4 in each cage. That\nmeans they have 9 cages. The answer is 9.\nZhang, Zhuosheng, et al. \"Automatic chain of thought prompting in large language\nmodels.\" arXiv preprint arXiv:2210.03493 (2022).\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-7-Columbia.pdf": {
        "text": "Tree-of-Thought Approach\n",
        "links": []
    },
    "./chunks/chunk_55_Lecture-7-Columbia.pdf": {
        "text": "Working behind the approach\nTree-of-Thought is built to expand the capabilities of Chain-of-Thought.\nEnables generative\nAI models to\ndemonstrate\nadvanced reasoning\ncapabilities\nInvolves\nhierarchically\nstructuring a\nprompt or query,\nakin to a tree\nstructure\nHolds immense\npotential for\nunlocking new\nsolutions and\ntackling complex\nproblems\n",
        "links": []
    },
    "./chunks/chunk_56_Lecture-7-Columbia.pdf": {
        "text": "Working behind the approach\n\u2022 Involves generating multiple\nlines of thought, resembling a\ndecision tree.\n\u2022 Allows the model to evaluate\nand pursue multiple paths\nsimultaneously.\n\u2022 Each thought or idea\nbranches out, creating a tree-\nlike structure of\ninterconnected thoughts.\nInput\nthought\nOutput\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-7-Columbia.pdf": {
        "text": "Example\nDesign recruitment and\nretention strategies for\nattracting skilled remote\nemployees\nE\n10\n%\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-7-Columbia.pdf": {
        "text": "Example\nPrompt instructions:\nImagine three different experts answering this question. All experts will\nwrite down 1 step of their thinking, and then share it with the group. Then\nall experts will go on to the next step, etc. If any expert realizes they're\nwrong at any point, then they leave.\nPrompt:\nAct as a human resource specialist, design a recruitment and retention\nstrategy for an e-commerce business, focusing on attracting and retaining\nskilled remote employees.\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-7-Columbia.pdf": {
        "text": "Published as a conference paper at ICLR 2023\nREACT: SYNERGIZING REASONING AND ACTING IN\nLANGUAGE MODELS\nShunyu Yao*1, Jeffrey Zhao\u00b2, Dian Yu\u00b2, Nan Du\u00b2, Izhak Shafran\u00b2, Karthik Narasimhan\u00b9, Yuan Cao\u00b2\n\u00b9Department of Computer Science, Princeton University\n2 Google Research, Brain team\n\u00b9{shunyuy, karthikn} @princeton.edu\n2 { jeffreyzhao, dianyu, dunan, izhak, yuancao}@google.com\nABSTRACT\nWhile large language models (LLMs) have demonstrated impressive performance\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\nplan generation) have primarily been studied as separate topics. In this paper, we\nexplore the use of LLMs to generate both reasoning traces and task-specific actions\nin an interleaved manner, allowing for greater synergy between the two: reasoning\ntraces help the model induce, track, and update action plans as well as handle\nexceptions, while actions allow it to interface with and gather additional information\nfrom external sources such as knowledge bases or environments. We apply our\napproach, named ReAct, to a diverse set of language and decision making tasks\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\nimproved human interpretability and trustworthiness. Concretely, on question\nanswering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent\nissues of hallucination and error propagation in chain-of-thought reasoning by\ninteracting with a simple Wikipedia API, and generating human-like task-solving\ntrajectories that are more interpretable than baselines without reasoning traces.\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\nan absolute success rate of 34% and 10% respectively, while being prompted with\nonly one or two in-context examples.\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-7-Columbia.pdf": {
        "text": "How it\nWorks?\n(1d) ReAct (Reason + Act)\nThought 1: I need to search Apple Remote and find the\nprogram it was originally designed to interact with.\nAct 1: Search [Apple Remote]\nObs 1: The Apple Remote is a remote control introduced in\nOctober 2005 by Apple ... originally designed to control the\nFront Row media center program ...\nFront\nThought 2: Apple Remote was originally designed to control\nthe Front Row media center program. I need to search\nRow next and find what other device can control it.\nAct 2: Search [Front Row]\nObs 2: Could not find [Front Row]. Similar: ['Front Row\nSeat to Earth', 'Front Row Motorsports',' Front Row\n(software) ', ...]\nThought 3: Front Row is not found. I need to search Front\nRow (software)\nAct 3: Search[Front Row (software) ]\nObs 3: Front Row is a discontinued media center software\nThought 4: Front Row (software) is controlled by an Apple\nRemote or the keyboard function keys. So the answer is\nkeyboard function keys.\nAct 4: Finish[keyboard function keys ]\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-7-Columbia.pdf": {
        "text": "ReAct\nExample\nQuestion What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n- Thought 11 need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into,\nthen find the elevation range of the area.\n- Action 1 Search [Colorado orogeny]\n- Observation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding\nareas.\n- Thought 2 It does not mention the eastern sector. So I need to look up eastern sector.\n- Action 2 Lookup[eastern sector]\n- Observation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n- Thought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find\nits elevation range.\nAction 3 Search[High Plains]\nObservation 3 High Plains refers to one of two distinct land regions\nThought 41 need to instead search High Plains (United States).\nAction 4 Search[High Plains (United States)]\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\nm).[3]\nThought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\nis 1,800 to 7,000 ft.\nAction 5 Finish [1,800 to 7,000 ft]\n",
        "links": []
    },
    "./chunks/chunk_62_Lecture-7-Columbia.pdf": {
        "text": "Overview\nLangChain\nOpen-source development framework for\nLLM applications\n\u2022 Python and JavaScript (TypeScript)\npackages\n\u2022 Focused on composition and modularity\nKey value adds:\n\u2022 Modular components\n\u2022 Use cases: Common ways to combine\ncomponents\n",
        "links": []
    },
    "./chunks/chunk_63_Lecture-7-Columbia.pdf": {
        "text": "prompt = \"\"\nYour task is to determine if\nthe student's solution is\ncorrect or not.\nWhy use\nprompt\ntemplates?\nTo solve the problem do the following:\n- First, work out your own solution to the problem.\n- Then compare your solution to the student's solution\nand evaluate if the student's solution is correct or not.\nUse the following format:\nQuestion:\nquestion here\nStudent's solution:\nstudent's solution here\nActual solution:\nsteps to work out the solution and your solution here\nIs the student's solution the same as actual solution \\\njust calculated:\nyes or no\nStudent grade:\ncorrect or incorrect\nQuestion:\n(question)\nStudent's solution:\n(student solution)\nActual solution:\nwww\nPrompts can be\nlong and detailed.\nReuse good\nprompts when\nyou can!\nLangChain also\nprovides prompts\nfor common\noperations.\n",
        "links": []
    },
    "./chunks/chunk_64_Lecture-7-Columbia.pdf": {
        "text": "LangChain\noutput parsing\nworks with\nprompt\ntemplates\nEXAMPLES = [www\nQuestion: What is the elevation range\nfor the area that the eastern sector\nof the Colorado orogeny extends into?\nThought: need to search Colorado orogeny, find\nthe area that the astern sector of the Colorado\norogeny extends into, then find the elevation range\nof the area.\nAction: Sech Colorado orogeny\nObservation: The Colorado orogeny was an\nepisode of mountain building (an orogeny)\nColorado and surrounding areas.\nThought: It does not mention the eastern sector.\nSo I need to look up eastern sector.\nAction: Lookup[eastern sector]\nThought: High Plains rise in elevation from\naround 1,800 to 7,000 ft, so the answer is 1,800 to\n7,000 ft.\nAction: Finish [1,800 to 7,000 ft] \"\"\",\n]\nLangChain library\nfunctions parse the\nLLM's output\nassuming that it will\nuse certain keywords.\nExample here uses\nThought, Action,\nObservation as\nkeywords for Chain-\nof-Thought\nReasoning. (ReAct)\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-7-Columbia.pdf": {
        "text": "Memory\nLarge Language Models are 'stateless'\n\u2022 Each transaction is independent\nChatbots appear to have memory by providing the full\nconversation as 'context'\nHmy name is Andrew\nHello Andrew's nice to\nmeet you My name is AL\nHow can I assist you\ntoday?\nWhat is 1+\nLarge Language Model\nThe answer to 1+1 2\nLangChain provides several kinds of 'memory' to store and\naccumulate the conversation.\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-7-Columbia.pdf": {
        "text": "Input\nChain\nRouter Chain\nDestination Chain\nIf the input\nis related to\nSubjects\n\u015eubject\n000\nOutput\nInput\n1.\n2.\nMath\nHistory\nLLM\n3.\n...\nelse\nNone\n\u0645\u0645\u0645.\nOutput\nDefault Chain\nLLMChain\n\u2022 Basic building block for\nLLM-powered\napplications\n\u2022 Combines a prompt\ntemplate with an LLM,\nSimpleSequentialChain\n\u2022 Chains multiple\nLLMChains together\n\u2022 Output of one chain\nbecomes input for the\nnext\n\u2022 Single input, single\noutput\nSequentialChain\n\u2022 More flexible than\nSimpleSequentialChain\n\u2022 Allows multiple inputs\nand outputs\n\u2022 Can specify which\noutputs to use as\ninputs for subsequent\nchains\nRouter Chain\nDynamically selects\nwhich chain to use\nbased on input\n\u2022 Useful for handling\ndiverse queries or tasks\nChain 1\nOutput\nInput\nof = to\nChain 1 Chain 2\nChain 2\nUser\nOutput\nInput\nChain 1\nChain 2\nOutput of\nChain 1\n=\nInput to\nchain 2\nOutput of\nChain 2\n=\nChain 4\nInput to\nchain 4\nOutput of\nChain 3\n=\nInput to\nchain 4\nChain 3\nOutput of\nChain 4\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-7-Columbia.pdf": {
        "text": "What is Llamalndex?\nLlamalndex is a powerful framework for\nbuilding context-augmented generative Al\napplications using large language models\n(LLMs)\nIt provides tools to:\n\u2022\nIngest data from various sources\n\u2022 Structure and index that data\n\u2022 Create natural language interfaces to query and interact\nwith the data\nKey Benefits:\n\u2022 Simplifies data integration for LLM apps\n\u2022 Enables use of private/domain-specific data\n\u2022 Provides flexible query and retrieval capabilities\n",
        "links": []
    },
    "./chunks/chunk_68_Lecture-7-Columbia.pdf": {
        "text": "Core Components\n1. Data Connectors: Ingest data from various\nsources (APIs, PDFs, databases, etc.)\n2. Indexing: Structure data for efficient\nretrievalVector stores\n3. Graph-based indexes\n4. Query Engines: Process natural language\nqueries Retrieval-Augmented Generation (RAG)\n5. Multi-step reasoning\n6. Response Synthesizers: Generate coherent\nanswers\n7. Chat Engines: Enable conversational interfaces\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-7-Columbia.pdf": {
        "text": "Use Cases\nand Getting\nStarted\n.\nCommon Use Cases:\nQuestion-answering over\ndocuments\n\u2022 Chatbots with domain knowledge\n\u2022 Data analysis and summarization\n.\nAugmented content generation\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-3-Columbia (1).pdf": {
        "text": "[COMSE6998-015] Fall 2024\nIntroduction to Deep Learning\nand LLM based Generative Al\nSystems\nLecture 3\n1\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-3-Columbia (1).pdf": {
        "text": "Today's\nAgenda\n\u2022 Machine Learning System Stack\n\u2022 Resource Management\n\u2022 Slurm, Docker, Kubernetes\n\u25cf\nCloud based ML platforms from\nAWS, Microsoft, Google\n\u2022 Ray\n\u2022 TorchX\n2\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-3-Columbia (1).pdf": {
        "text": "Dream\nProvide data\nGet optimal prediction system\nas scalable API or mobile app\nAggregate, process, clean, label,\nand version data\nWrite and debug model code\nProvision compute\nReality\nRun experiments, review results\nDeploy model\nMonitor predictions and close data\nflywheel loop\n3\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-3-Columbia (1).pdf": {
        "text": "\"OPERATION VACATION\"\nShadow mode/telemetry / statistics\nInference @cloud\nInference @ FSD Computer\nData\nEvaluation\nPyTorch distributed training\nGPU cluster\nDojo cluster\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-3-Columbia (1).pdf": {
        "text": "Data\nVerification\nMachine\nResource\nManagement\nMonitoring\nConfiguration\nData Collection\nServing\nInfrastructure\nML\nCode\nAnalysis Tools\nFeature\nExtraction\nProcess\nManagement Tools\nMachine Learning:\nThe High-Interest Credit Card of Technical Debt\nD. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov,\nTodd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-3-Columbia (1).pdf": {
        "text": "DVC\n0\nPachydern\nLIQUIDATA\nVersioning\nSYN\nDOMINO\ngradient FLOYD DATA LAB\nAmazon SageMaker by Paperspace\n\"\nAll-in-one\"\nfiqure\neight scale\n\"Aquarium\nLabeling\nPYTORCH\nfast.ai\nSIGOPT\nDetermined Al\n> fiddler\nTecton\nWeights & Biases\nHyperparameter Tuning\ntune\nFeature\nStore\nMonitoring\nHOROVOD\nWeights & Biases\ncomet\nK\nog RAY\npandas\nFrameworks &\nDistributed Training\nAPACHE\nSpark\nDAGSTER\nProcessing\nsnowflake\nRAPIDS\ndbt\nExploration\ndatabricks\nData Lake / Warehouse\nS3\nLOG\nSources\nData\nParquet\nDetermined Al\nPY\nslurm\nworkload manager\nResource Management\nTensorBoard\nmiflow\nNeptune\nMachine Learning Lab\nExperiment Management\nML\nNVIDIA TensorRT\nTensorFlow Lite\nONNX\nALGORITHMIA\nN\nSELDON\nJupyter Streamlit\nEdge\nWeb\n\u2611\ngit\nSoftware Engineering\nBuildkite\ngreat expectations\nCI/Testing\nCW CoreWeave\nLambda\nCompute\nTraining/Evaluation\nor\nDeployment\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-3-Columbia (1).pdf": {
        "text": "Function\nDevelopment\nWriting code\n\u2022\nDebugging models\n\u2022\nLooking at results\n\u2022\nCompute needs\nDesiderata\nQuickly compile models and run training\nNice-to-have: use GUI\nSolutions\n.\nTraining/Evaluation\n\u2022 Function\n.\n\u2022\n\u2022\n\u2022\nModel architecture/hyperparam search\nTraining large models\nDesiderata\n.\nEasy to launch experiments and review\nresults\nSolutions\nDesktop with 4 GPUs\n.\nDesktop with 1-4 GPUs\nCloud instance with 1-4 GPUs\n10\nor\n\u2022\nPrivate cluster of GPU machines\n\u2022\nCloud cluster of GPU instances\nor\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-3-Columbia (1).pdf": {
        "text": "Training compute (FLOPs)\nTraining compute (FLOPs) of milestone Machine Learning systems over time\nn=99\n1e+25\n1e+24\n1e+23\n1e+22\nDeep Learning Era\nLarge Scale Era\nAlphaZeroA\nMeget ove\n15-1182\nGShard Br\nMegatron-Turing NLG 530BA\nCOB)\nDALM\nCogViewO\nGPT-6BO\n1e+21\n1e+20\n1e+19\n1e+18\nAAlphaGo Fan\nLibratus\nOpenAI TIZ DOTAJ\u211616\nBERT-Large.\nRubik's cube\nNEO (DERM-2022)\nwave2vec 2.0 LARGEO\nDLAMO\nIMPALAO\nnull\nMoEO\nAlphaFoldO\nXceptionO\n\u0445\u043e\u0441\u043e\u0432\u0430\nOMSRA (C, PRORepSpeech2\nPopulation-based blessNASO\nObjectNetO\nOResNet-152 (Net)\nTransformerO\nGPTO\nAlphaX-10\nDLRM-20200\nnull\nOTransE\nORNNrch GoogleNet/InceptionV1\nOAlexNet\nOVisualizCAN\nOMitosis,\nPart-of-sentence tagging model\n1e+17\nNamed Entity Recognition model\nOKNS LM+RNN 400/10 (WSJ)\nOADAM (CIFAR-10)R-FCNO\nOWord2Vec (large)\n1e+16\n1e+15\nVariational Autoencoders\nOFeedforward NN\n1e+14\n6-layer MLP (MINIST)\n2011\n2012\n2013\n2014\n2015\n2017\n2018\n2019\n2020\n2021\n2022\n2016\nPublication date\nCompute Trends across Three Eras of Machine Learning, https://arxiv.org/pdf/2202.05924\n8\n",
        "links": [
            "https://arxiv.org/pdf/2202.05924"
        ]
    },
    "./chunks/chunk_9_Lecture-3-Columbia (1).pdf": {
        "text": "100\n08\n80\nAggregate Performance Across Benchmarks\nFew Shot\nOne Shot\nZero Shot\n60\n00\nAccuracy\n40\n40\n20\n20\nWhy? Bigger\nmodel,\nbetter\naccuracy\nLanguage Models are Few\nShot Learners,\nhttps://splab.sdu.edu.cn/G\nPT3.pdf\n0\n0.1B\n0.4B\n0.8B 1.3B 2.6B\n6.7B\n13B\n175B\nParameters in LM (Billions)\n6\n",
        "links": [
            "https://splab.sdu.edu.cn/G"
        ]
    },
    "./chunks/chunk_10_Lecture-3-Columbia (1).pdf": {
        "text": "Why? Emergence of Foundation Models\nQUESTION ANSWERING\nARITHMETIC\nLANGUAGE UNDERSTANDING\n8 billion parameters\nPathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance, https://research.google/blog/pathways-language-\nmodel-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/\n10\n",
        "links": [
            "https://research.google/blog/pathways-languagemodel-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/"
        ]
    },
    "./chunks/chunk_11_Lecture-3-Columbia (1).pdf": {
        "text": "Training compute (FLOPS) of milestone Machine Learning systems over time\nn = 99\n1e+25\nTraining compute (FLOPs)\n1e+24\n1e+23\n1e+22\n1e+21\nDeep Learning Era\nLarge Scale Era\nMegatron-Turing NLG 530BA\nAlphaZeroA\nMagnetron Ave\nGShard Br\n15-1182\nDALL\nboB)\nCogViewC\nGPT-6BC\n1e+20\n1e+19\n1e+18\nOAlexNet\n1e+17\nOMitosis,\nOKNS LM+RNN 400/10 (WSJ)\nWord2Vec (large)\n1e+16\nODropout (MNIST)\nORNN 500 RT09 LOMSPANOMNIST)\nOpenAl TIZ DOTAJ\u211616\nAAlphaGo Fan\n10x every 18 months\nTransE ORNNshoogLeNet/InceptionV1\nOVisualiz\nPart-of-sentence tagging model\nNamed Entity Recognition model\nQADAM (CIFAR-R-FCNO\nMoE\nwave2vic 2.0 LARGE\nDLRM-207\nnull\nDLRM-20200\nnullolo\nMoore's Law (2x every 18 months)\nAlphaFoldO\n1e+15\n1e+14 6-layer MLP (MNIST)\n2011\n2012\n2013\n2014\n2015\n2016 2017\nPublication date\n2018\n2019\n2020\n2021\n2022\nCompute Trends across Three Eras of Machine Learning, https://arxiv.org/pdf/2202.05924\n11\nCPU\n",
        "links": [
            "https://arxiv.org/pdf/2202.05924"
        ]
    },
    "./chunks/chunk_12_Lecture-3-Columbia (1).pdf": {
        "text": "So,\nor\n?\n33\n13\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-3-Columbia (1).pdf": {
        "text": "LLM Models GPU Requirements for Pre-Training\nModel\nDeveloper\nLLAMA 2\nMeta Al\nParameters\n7B to 70B\nBloom\nHugging Face and collaborators 176B\nBERT\nGoogle Al\n110M to 340M\nFalcon\nIndependent / Community-\ndriven\nTens of billions\nZephyr\nAcademic collaboration\n~7B\nOpenAl-inspired, community-\nMistral 7B\n7B\ndriven\nPhi 2\nAcademic collaboration\nVaries\nMPT 7B\nTech companies and academic\npartners\n7B\nAlpaca\nOpen Al and partners\nVaries (focus on\nsmaller models)\nGPU Requirements for Training\n7B: At least 24GB VRAM 65B: Multi-GPU setup, each with 160GB+ VRAM\n(e.g., 2x-4x NVIDIA A100 or H100)\nMulti-GPU setup, each with at least 40GB VRAM (e.g., NVIDIA A100 or\nH100)\n8GB to 16GB VRAM (e.g., NVIDIA RTX 3080 or 3090)\n12GB to 24GB VRAM (e.g., RTX 4080 for smaller models, RTX 4090 or RTX\n6000 Ada for larger variants)\n16GB VRAM for basic tasks, 24GB+ VRAM recommended (e.g., NVIDIA RTX\n4090)\nAt least 24GB VRAM (e.g., RTX 6000 Ada or A100)\n12GB to 24GB VRAM for small to medium models (e.g., RTX 4080 or 4090)\nLarger models: RTX 6000 Ada or A100 recommended\nAt least 16GB VRAM (e.g., RTX 4080 or 4090)<br>RTX 6000 Ada for more\ndemanding applications\nAs low as 8GB VRAM for basic use 16GB+ VRAM recommended for optimal\nperformance (e.g., RTX 4080 or 4090)\n14\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-3-Columbia (1).pdf": {
        "text": "Cloud GPU Resources for LLM Fine-tuning and\nInference\nFine-tuning on\nCloud GPUs\nAccessible GPU instances\nfrom cloud providers\nCost-effective hourly\nrentals (e.g., Google\nCloud T4 GPU:\n$0.35/hour)\nSuitable for fine-tuning\nsmaller models like BERT\nInference on\nCloud GPUs\nLower computational\nrequirements than\ntraining\nFlexible GPU options\nbased on model size and\nperformance needs\nOn-demand usage for\ncost optimization\nPractical\nExamples\nFine-tuning BERT: $10-\n$50 per session on T4\nGPU\nInference with Mistral 7B:\n$1-$5 per hour on\nT4/V100 GPU\nBenefits for Users\nExperimentation without\nlong-term hardware\ninvestment\nScalability for various\nproject sizes\nAccess to latest GPU\nmodels\nML Platforms\nCost Management\nGoogle Colab: Free GPU\naccess with limitations\nKaggle Kernels: Free GPU\nhours for competitions\nHugging Face: Easy-to-use\ninterfaces for model\ndeployment\nSpot instances: Up to 90%\ncost reduction\nPreemptible VMs:\nDiscounts for flexible\nworkloads\nOff-peak scheduling for\nlower costs\nKey Takeaway: Cloud GPU resources have democratized LLM work, allowing individuals and small teams to experiment with\nand deploy Al models efficiently and cost-effectively.\n16\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-3-Columbia (1).pdf": {
        "text": "Quad PC vs. Spot Instances\nLength of trial in experiment (hours)\n6\nNumber of trials in experiment\nTotal GPU hours for experiment\n16\n96\nCost of 4x RTX 2080 Ti machine $10,000.00\n24 hours\nTime to run experiment on 4x machine\nTime to run experiment on V100 spot instances\nCost of provisioning enough pre-emptible V100s\nNumber of experiments that equal cost of 4x\n6 hours\n$96.00\n104\nHow to think about it: cloud enables quicker experiments.\n17\n\u0f44\u0f0b\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-3-Columbia (1).pdf": {
        "text": "DvC\n0\nPachydern\nLIQUIDATA\nVersioning\nAPACHE\nSpark\nDAGSTER\nProcessing\nsnowflake\nfiqure\neight scale\ngradient FLOYD\nAmazon SageMaker by Paperspace\n\"All-in-one\"\nDOMINO\nDATA LAB\nSIGOPT\nDetermined Al\nfiddler\n\u03c4\u03b1\u03c5\u03c4\u03bf\u03c0\nWeights & Biases\nHyperparameter Tuning\ntune\nFeature\nStore\nMonitoring\nAquarium\nfast.ai\nLabeling\nPYTORCH\nHOROVOD\nWeights & Biases\ncomet\nRAY\nK\npandas\nRAPIDS\nXdbt\nExploration\ndatabricks\nData Lake / Warehouse\nS3\nLOG\nSources\nData\nParquet\nFrameworks &\nDistributed Training\nDetermined Al\nTensorBoard\nmlflow\nNeptune\nMachine Learning Lab\nExperiment Management\nML\nNVIDIA TensorRT\nTensorFlow Lite\nONNX\nALGORITHMIA\nP\nSELDON\nPY\nJupyter Streamlit\nEdge\nWeb\ngit\nO\nSoftware Engineering\nBuildkite\ngreat expectations\nCI/ Testing\nslurm\nworkload manager\nResource Management\nCW CoreWeave\n\u2610 Lambda\nCO\nCompute\nTraining/Evaluation\nor\nor\nwwww\nDeployment\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-3-Columbia (1).pdf": {
        "text": "Resource Management\n\u5712\nFunction\nMulti-tenancy: multiple people\nshare the resources.\nUsing multiple GPUs/machines\nRunning different environments\n59\nGoal\nEasy to launch a batch of\nexperiments, with proper\ndependencies and resource\nallocations\nSolutions\nPython Scripts\nSLURM\nDocker + Kubernetes\nSoftware specialized for ML use\ncases\n19\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-3-Columbia (1).pdf": {
        "text": "Scripts or slurm\nworkload manager\nProblem we're solving: allocate free\nresources to programs\nCan be scripted pretty easily\nEven better, use old-school cluster job\nscheduler\nJob defines necessary resources, gets\nqueued\nclass GPUManager(object):\ndef _init_(self, verbose: bool-False):\nself.lock_manager Redlock([{\"host\": \"localhost\", \"port\": 6379, \"db\": 0}, ])\nself.verbose=verbose\ndef get free gou(self):\nIf some GPUs are available, try reserving one by checking out an exclusive redis lock.\nIf none available or can't get lock, sleep and check again.\nwww\nwhile True:\n12\n13 H\n14\n15\n16\n17 =>\n18\n19\n20\n21\n22-\n23\n24\n25\n26-\n28\n29\n30 B\n31 H\n32\n33\n34\n35\n36-\n1\n37\n38-\ngpu_ind self._get_free_gpu()\nif gpu_ind is not None:\nreturn gpu_ind\nif self.verbose:\nprint('pid (os.getpid()) sleeping')\ntime.sleep(GPU_LOCK_TIMEOUT / 1000)\ndef get_free_gpu(self):\navailable_gpu_inds = [\ngpu.index\nfor gpu in gpustat.GPUStatCollection.new_query()\nif not gpu.processes\nif available_gpu_inds:\ngpu_ind = np.random.choice(available_gpu_inds)\nif self.verbose:\nprint('pid (os.getpid()) picking gpu (gpu_ind}')\nif self.lock_manager.lock('gpu_(gpu_ind), GPU_LOCK_TIMEOUT):\nreturn int(gpu_ind)\nif self.verbose:\nprint('pid (os.getpid()) couldnt get lock')\nreturn None\n40 B\ntrain.sh\n./train.py --gpu--1--fc_size=128\n./train.py --gpu--1--fc_size=256\n./train.py --gpu--1--fc_size=512\n123\n1\n2\n3\n4\n./train.py --gpu--1--fc_size=1024\n45\n5 ./train.py --gpu--1--fc_size=2048\n6 ./train.py --gpu--1--fc_size=4096\n7\n./train.py --gpu--1 --fc_size=8192\nsergeyk@thefarm:~$ parallel -j8 :::: train.sh\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-3-Columbia (1).pdf": {
        "text": "12 class GPUManager(object):\ndef _init_(self, verbose: bool-False):\nself.lock manager Redlock([{\"host\": \"localhost\", \"port\": 6379, \"db\": 0}, ])\nself.verbose=verbose\nJob-file (schedule job with sbatch, check status with squeue -u <Username>):\nScripts or slurm\nworkload manager\nProblem we're solving: allocate free\nresources to programs\nCan be scripted pretty easily\nEven better, use old-school cluster job\nscheduler\nJob defines necessary resources, gets\nqueued\n#!/bin/bash\n#SBATCH --gres=gpu:1\n#SBATCH --mem=10000\n#SBATCH -p gpu2\n# K80 GPUs on Haswell node\n#SBATCH --time=01:00:00\n## with Theano (using configs from above)\nmodule purge # purge if you already have modules loaded\nmodule load modenv/eb\nmodule load Keras\nsrun python mnist_cnn.py\n## with Tensorflow\nmodule purge\nmodule load modenv/eb\nmodule load Keras\nmodule load tensorflow\n# if you see 'broken pipe error's (might happen in interactive session af\nmodule load h5py/2.6.0-intel-2016.03-GCC-5.3-Python-3.5.2-HDF5-1.8.17-ser\nexport KERAS_BACKEND=tensorflow\nsrun python mnist_cnn.py\n# configure Keras to use tensorflow\n\u25aa/train.py --gpu==1 \u2014Tc_size=8192\nsergeyk@thefarm:~$ parallel -j8 :::: train.sh\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-3-Columbia (1).pdf": {
        "text": "Docker + Kubernetes\n\u2022 Kubernetes is a way to run many docker\ncontainers on top of a cluster\n</>\nApp\n#1\nApp\n#2\nApp\n#3\nContainer\nBins/\nLibs\nBins/\nBins/\nLibs\nLibs\nDocker Engine\nHost OS\nServer\n22\n22\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-3-Columbia (1).pdf": {
        "text": "What are Containers?\n\u2022 Containers are an abstraction at the app layer that\n\u2022 package code and dependencies together.\n\u2022\neach running as isolated processes in user space.\n\u2022\nMultiple containers can run on the same machine and share the OS kernel with other containers,\nContainers are basically leveraging linux namespaces and cgroups for isolation of processes execution and\nresource limitations respectively\nOS : Linux\n/lib\n/bin\n/etc\nresource limits\nContainer\nPPPPP\nTutorial: https://www.youtube.com/watch?v=EnJ7qX9fkcU\nhttps://jvns.ca/blog/2016/10/10/what-even-is-a-container/\nThe needs for isolation of\nresources\nThe needs to limit the\nresource usage of a collection\nof processes\nLinux cgroups\nLinux namespaces\n",
        "links": [
            "https://www.youtube.com/watch?v=EnJ7qX9fkcU",
            "https://jvns.ca/blog/2016/10/10/what-even-is-a-container/"
        ]
    },
    "./chunks/chunk_22_Lecture-3-Columbia (1).pdf": {
        "text": "Containers vs. Processes\n\u2022 Textbook process\nown address space\nown program\n\u2022 own CPU state\n\u2022\nown process table entry\n\u2022\nReal process:\n.\nmemory mapped from the filesystem into the process\naddress space\nconsists of dozens of shared libraries, programs and files that\nare shared.\nsuper-dependent on its filesystem environment (e.g. locale\ninfo, shared libs and files)\nContainer\n\u2022\nan encapsulation of one program with all its dependencies.\nwith limits on the amount of resources the program can use.\nContainers vs. Processes\ntextbook process\nreal process\n/etc\ncontainer\n/etc\n/lib\n/lib\n/bin\n/bin\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-3-Columbia (1).pdf": {
        "text": "Docker\nDocker is a tool designed to make it easier to create, deploy, and run applications by using\ncontainers.\n\u2022\n\u2022\nDocker image: A Docker image is a file, comprised of multiple layers, used to execute code in a Docker\ncontainer.\nDockerfile: The Dockerfile is essentially a set of build instructions to build the image. A Dockerfile is a text\ndocument used to indicate to Docker a base image, the Docker settings you need, and a list of commands\nyou would like to have executed to prepare and start your new container.\nDocker can associate a seccomp profile with the container using the --security-opt parameter.\nDocker image repositories:\nWeb Application\nLayer 9: CMD Start Pintail.ai website\nLayer 8: RUN Commands to set up Pintail.ai Website\nLayer 7: ADD Pintail.ai binaries\nLayer 6: FROM Node.js - Apline Linux\nApplication Framework\nnode\nUS\nLayer 5: RUN Commands to set up Node.js\nLayer 4: ADD Node.js binaries\nPrivate: IBM Kubernetes Registry\nPublic: Docker Hub\nGitHub\nLayer 3: FROM Alpine Linux\nOperating System\nalpine\nLayer 2: RUN Commands to set up OS\nLayer 1: ADD Operating System binaries\nLayer 0: FROM Scratch\ndocker\ndocker\nHUB\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-3-Columbia (1).pdf": {
        "text": "Container vs Virtual Machine\n\u2022 Virtual machines\n\u2022\nHow is this different from virtual machines?\n\u2022\nhave a full OS\nown memory management\nContainers have similar resource isolation and allocation benefits as virtual machines but a different architectural approach allows\nthem to be much more portable and efficient.\nown device drivers\nApp 1\n\u2022\nown daemons\nBins/Libs\nApp 2\nBins/Libs\nApp 3\nBins/Libs\nsoftware that hosts the\ncontainers\n\u2022\nown binaries, libraries, and\napplications\nGuest OS\nGuest OS Guest OS\nApp 1\nBins/Libs\nApp 2\nApp 3\nBins/Libs\nBins/Libs\nContainers\nShare the host's OS kernel\nShare the binaries and libraries i\nread-only mode.\nHypervisor\nHost Operating System\nInfrastructure\nDocker Engine\nOperating System\nInfrastructure\nVirtual Machines\nEach virtual machines includes the application, the necessary binaries and\nlibraries and an entire guest operating system - all of which may be tens of\nGBs in size.\nContainers\nContainers include the application and all of its dependencies, but share the\nkernel with other containers. They run as an isolated process in userspace on\nthe host operating system. They're also not tied to any specific infrastructure-\nDocker containers run on any computer, on any infrastructure and in any cloud.\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-3-Columbia (1).pdf": {
        "text": "Consistent Runtime Environment\nApplication Sandboxing\nSmall Size on Disk\nLow Overhead\nBenefits of using\nContainer\n\u2022\nCONTAINER BENEFITS\nVIRTUAL MACHINE BENEFIT\nContainerization allows our development teams to\n\u2022\nmove fast\n\u2022\n\u2022\nDecoupling from environment\nCompared to VMs, containers are far more lightweight.\ndeploy software efficiently\n\u2022\nConsistent environment\n\u2022\nRun Anywhere\n\u2022\nIsolation\n\u2022\nEasy version control\n\u2022\nAgile development\nand operate at an unprecedented scale\n\u2022\nKubernetes: Production-Grade Container Orchestration\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-3-Columbia (1).pdf": {
        "text": "Docker + Kubernetes\nAn ocean of\nuser containers\n\u2022\nKubernetes is a way to run many docker\ncontainers on top of a cluster\nKubernetes\nMaster\nNode\nNode\nNode\nScheduled and packed\ndynamically onto nodes\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-3-Columbia (1).pdf": {
        "text": "\u25cf\nWhat is Kubernetes (K8s)?\nKubernetes is an open source system for managing containerized applications across\nmultiple hosts, providing basic mechanisms for deployment, maintenance, and scaling\nof applications.\nApp1.yaml\nDeployment\nKubernetes cluster services \u2192 \"Desired State Management\"\nP1R1\nkubelet\nP1R2\nWorker\nP2R1\nPod 1\n- container img 1\n\u0391\u03a1\u0399\nP1R2\n- container img 2\nuster services\nkubelet\norker\nServer\n- Replica -> 3\nPod 2\n- container img 3\n- replicas = 2\nhttps://kubernetes.io/\nP1R3\nkubelet\nWorker\nP2R2\n",
        "links": [
            "https://kubernetes.io/"
        ]
    },
    "./chunks/chunk_28_Lecture-3-Columbia (1).pdf": {
        "text": "Architecture of Kubernetes (K8s)\nCMD tool to send API\nrequests\nInternet\nK8s follows a client-server architecture\nkubectl\nFrontend server accepting\nREST API requests\nWorker node\nMaster node\nAPI server\ncontroller-manager\n(replication, namespace,\nserviceaccounts, ...)\nscheduler\nkubelet\nkube-proxy\nPod\ndocker\nPod\ncontainer\nconta ner\nA set of controllers to\nenforce the state of\nSchedule a pod to a\nnode based on\nWorker node\nkubelet\nresources.\nresource utilization.\nMaintain states of apps\netcd\nkube-proxy\ndocker\nPod\nPod\ncontanar\nconta nar\nEnsure pods heathy and\nrunning\nReports to the master to\nreport the health of the host.\nScheduler assigns a node to the pod by\n(1) filtering the nodes that satisfy the\npod resource requirements and\nother predicate constraints,\n(2) ranking the candidate nodes based\non priority functions, and\n(3) selecting the node with the\n-\n-\nhighest rank.\nDeal with individual host\nsubnetting\nExpose services to the\nexternal world.\nRequest forwarding across\nisolated networks in a\ncluster.\n30\n30\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-3-Columbia (1).pdf": {
        "text": "Cloud\nServices\nrunning in\nContainer\nCloud\n\u2022 Internal Customers of IBM Cloud Kubernetes Service: Launched on\nMay 23, 2017\n\u2022\n\u2022\nIBM Watson (Conversation, NLC, sentiment analysis, etc.)\nIBM Deep Learning as a Service (DLaaS)\nGoogle: Launched in 2015\n\u2022 Almost everything from YouTube to Gmail.\n\u2022\nGoogle Kubernetes Engine: https://cloud.google.com/kubernetes-\nengine/\n\u2022 Alibaba:\n\u2022\n90% of their main business, especially e-commerce like Taobao and Tmall\n\u2022 Alibaba Cloud Container Service for Kubernetes:\nhttps://www.alibabacloud.com/product/kubernetes\n\u2022 AWS: Nov. 29, 2017\n\u2022\nAmazon Elastic Container Service for Kubernetes:\nhttps://aws.amazon.com/eks/\n\u2022 Azure: Oct. 27, 2017\n\u2022\nAzure Kubernetes service: https://azure.microsoft.com/en-\nus/services/kubernetes-service/\n",
        "links": [
            "https://cloud.google.com/kubernetesengine/",
            "https://www.alibabacloud.com/product/kubernetes",
            "https://aws.amazon.com/eks/",
            "https://azure.microsoft.com/enus/services/kubernetes-service/"
        ]
    },
    "./chunks/chunk_30_Lecture-3-Columbia (1).pdf": {
        "text": "Platform specialized for DL training workloads\n\"Why do we need a platform specialized for DL training workloads\"?\n\u2022 Can I build my own platform over K8s ?\n\u2022 You need to understand low level details of K8s\n\u2022 You need to keep track of job life-cycle including deployment,\nmonitoring, termination of associated pods using K8s APIs.\n33\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-3-Columbia (1).pdf": {
        "text": "Is K8s Enough for Data scientists and ML Developers ?\n\u2022 Too \"low-level\" for use by data scientists\n.\nTraining job vs pods, deployment, replica sets\n...\nDL training workloads need specialized scheduling algorithms for\nbetter performance, not natively supported by K8s\n\u2022 Cannot track DL specific job status: DOWNLOADING, PROCESSING,\nSTORING, HALTED, RESUMED\n34\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-3-Columbia (1).pdf": {
        "text": "Typical Life Cycle of ML on Cloud\nCLIS\nJupyter\nBrowser\nTensorFlow\nCaffe2\nPYTORCH Caffe\nK Keras\nLaunch\nJob\nREST\nAPI\nTrainer\nService\nLifecycle\nManager\nJob Info\nWeb UI\nMongo\nDB\nPrometheus\nElastic\nSearch\nPush Gateway\nAlert Manager\nTraining\nData\nTraining Job\nLearner Pod\nLearner (e.g. TensorFlow, Caffe,\nPyTorch, Keras etc.)\nMOUNT\nOBJECT\nSTORAGE\nBUCKET\nModel\nDefinition\nTraining\nData\nTrained\nLog Collector\nController\nStatus\nkubernetes\nJob Monitor\nParameter\nServer\nHOROVOD\nOpen MPI\nFabric for Deep Learning (FfDL), https://github.com/IBM/FfDL\nModels\nOBJECT\nSTORAGE\n35\n",
        "links": [
            "https://github.com/IBM/FfDL"
        ]
    },
    "./chunks/chunk_33_Lecture-3-Columbia (1).pdf": {
        "text": "Cloud based Machine\nLearning Services\n\u2022\n.\n\u2022\n.\nIBM Watson Studio:\nhttps://www.ibm.com/products/watson-studio\nAmazon Sagemaker:\nhttps://aws.amazon.com/sagemaker\nMicrosoft Azure Machine Learning:\nhttps://azure.com/ml\nGoogle Vertex Al Platform:\nhttps://cloud.google.com/vertex-ai/\n36\n",
        "links": [
            "https://www.ibm.com/products/watson-studio",
            "https://aws.amazon.com/sagemaker",
            "https://azure.com/ml",
            "https://cloud.google.com/vertex-ai/"
        ]
    },
    "./chunks/chunk_34_Lecture-3-Columbia (1).pdf": {
        "text": "Amazon Sagemaker\nFully managed machine\nlearning service by Amazon\n\u2022 Supports:\n\u2022 Quick and easy building and\n.\ntraining of ML models\nModel deployment in\nproduction-ready hosted\nenvironment\nDeploy the\nmodel\nTypical ML Workflow\nMonitor/\ncollect data/\nevaluate\nDeploy to\nproduction\nEvaluate\nmodel\nFetch\nTrain\nmodel\nGenerate example\nClean\nPrepare\ndata\nTrain a model\n37\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-3-Columbia (1).pdf": {
        "text": "Train with Amazon Sagemaker\nClient\napplication\nGround truth\nInput data\n(request)\nInference\n(response)\nM500TH\n*tat fination.\nHelper code\nEndpoint\nInference code\nS3 bucket\nModel artifacts\nG\nDeployment/hosting\non ML compute instances\nS3 bucket\nTraining data\nMARGINE530\nOript fan:\ntat fination.\"\nHelper code\ntat fination.\nTraining code\nModel training\non ML compute instances\nAmazon SageMaker\nMet\ntest rination.\nInference code image\nTraining algorithm options\nUse an out-of-the-box algorithm provided by\nAmazon\n\u2022\nPre-built docker images\nUse Apache Spark MLLib with Amazon Sagemaker\nCustom python code to train with DL frameworks\nTensorflow and Apache MXNet\nUse your own custom algorithm in any\nprogramming language and framework\n.\nPackage as a docker image and register it\nUse an algorithm from AWS Marketplace\nMalet\nOwaription:\ntat fination.\"\nTraining code image\nEC2 Container Registry\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n38\n",
        "links": [
            "https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"
        ]
    },
    "./chunks/chunk_36_Lecture-3-Columbia (1).pdf": {
        "text": "Ray: A framework for\nScaling Al workloads\n42\n42\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-3-Columbia (1).pdf": {
        "text": "Al demands grow much faster...\nthan a single node/processor capabilities\n43\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-3-Columbia (1).pdf": {
        "text": "Training compute (FLOPS) of milestone Machine Learning systems over time\nn = 99\n1e+25\nTraining compute (FLOPs)\n1e+24\n1e+23\n1e+22\n1e+21\nDeep Learning Era\nLarge Scale Era\nMegatron-Turing NLG 530BA\nAlphaZeroA\nMagnetron Ave\nGShard Br\n15-1182\nDALL\nboB)\nCogViewC\nGPT-6BC\n1e+20\n1e+19\n1e+18\nOAlexNet\n1e+17\nOMitosis,\nOKNS LM+RNN 400/10 (WSJ)\nWord2Vec (large)\n1e+16\nODropout (MNIST)\nORNN 500 RT09 LOMSPANOMNIST)\nOpenAl TIZ DOTAJ\u211616\nAAlphaGo Fan\n10x every 18 months\nTransE ORNNshoogLeNet/InceptionV1\nOVisualiz\nPart-of-sentence tagging model\nNamed Entity Recognition model\nQADAM (CIFAR-R-FCNO\nMoE\nwave2vic 2.0 LARGE\nDLRM-207\nnull\nDLRM-20200\nnullolo\nMoore's Law (2x every 18 months)\nAlphaFoldO\n1e+15\n1e+14 6-layer MLP (MNIST)\n2011\n2012\n2013\n2014\n2015\n2016 2017\nPublication date\n2018\n2019\n2020\n2021\n2022\nCompute Trends across Three Eras of Machine Learning, https://arxiv.org/pdf/2202.05924\n44\nCPU\n",
        "links": [
            "https://arxiv.org/pdf/2202.05924"
        ]
    },
    "./chunks/chunk_39_Lecture-3-Columbia (1).pdf": {
        "text": "Moore's Law coming to an end\nPerformance vs. VAX11-780\n100,000\n10,000\n1,000\n100\n10\nEnd of Dennard Scaling\nEnd of the Line\u21d22X/20 years (3%/yr)\nAmdahl's Law\u21d22X/6 years (12%/year)\nMulticore 2X/3.5 years (23 %/year)\nCISC 2X/2.5 years\n(22%/year)\nRISC 2X/1.5 years\n(52%/year)\n~2x increase\nevery 18 months\n1\n1980\n1985\n1990\n1995\n2000\n2005\n2010\n2015\n~1.05x increase\nevery 18 months\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-3-Columbia (1).pdf": {
        "text": "What about specialized hardware?\nNVIDIA\nALTERA\nStratix V\n11111\n00\n00\nAMDA\nINSTINCT\n46\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-3-Columbia (1).pdf": {
        "text": "Training compute (FLOPS) of milestone Machine Learning systems over time\nn = 99\n1e+25\n$ $ Negrete\n1e+24\n1e+23\n1e+22\nDeep Learning Era\nLarge Scale Era\nAZA\nAlphaGo MasterA\nTraining compute (FLOPs)\n1e+21\n1e+20\n1e+19\n1e+18\nAlphaZeroA\nMoeng\nMegatron Ave\n15-118\nGShard Br\nOper\nDALLE\nCogViewC\nGPT-6BO\nSpecialized Hardware are not Good\nEnoughery\nOTransE ORNN conta\n18\nPop based off lessNASO\nTransformer\nGPTO\nDLRM-PO\nAlphaFoldO\nObjectNetO\nDLRM-20200\nnull\nnullO\nTPU*\nGPU*\nOAlexNet\n1e+17\nMitosis\nOKNS LM+RNN 400/10 (WSJ)\nOWord2Vec (large)\n1e+16\nODropout (MNIST)\nORNN 500 RT09 LIQMSPNNOMNIST\n1e+15\nOFeedforward NN\n1e+14 6-layer MLP (MNIST)\n2011\nPart-of-sentence tagging model\nNamed Entity Recognition model\nQADAM (CIFAR-10)R-FCNO\nMoore's Law (2x every 18 months)\nOVariational Autoencoders\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n47\nPublication date\nCPU\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-3-Columbia (1).pdf": {
        "text": "Not so fast...\nArticle May 28, 2023\nThe Scaling Law still holds\nBut can no longer increase model\nsize 10x/year (not enough GPUs!)\n\u2022 Expected to grow just 2-3x/year\nOpenAl's plans\naccording to\nSam Altman\nRaza Habib\nhttps://website-754fwhahs-humanloopml.vercel.app/blog/open_ai_talk\n48\n",
        "links": [
            "https://website-754fwhahs-humanloopml.vercel.app/blog/open_ai_talk"
        ]
    },
    "./chunks/chunk_43_Lecture-3-Columbia (1).pdf": {
        "text": "Even if model sizes would stop growing ...\n...\nit would take decades for specialized hardware to catch up!\nExample:\nGoogle's PaLM takes 6144 TPU v4 to train\nAssuming doubling performance every 18 months, it would take\n~19 years to train on a single chip\n49\n49\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-3-Columbia (1).pdf": {
        "text": "No way but to\ndistribute Al workloads\nand this will remain true for\nforeseeable future!\n50\n50\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-3-Columbia (1).pdf": {
        "text": "The anatomy of an end-to-end ML application\nRecommendation system\nlogs Preprocessing\nGoogle\n(e.g., ingestion,\nfeaturization)\nTraining\nTuning\nBatch\nPrediction\nServing\nqueries\nrecommendations\n51\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-3-Columbia (1).pdf": {
        "text": "Challenge: need to scale every stage!\nDistributed Systems\nDistributed Systems\nDistributed Systems\nDistributed Systems\nDistributed Systems\nPreprocessing\n(e.g., ingestion,\nfeaturization)\nTraining\nAPACHE\nSpark\nFlink\nth=doop\nO PyTorch\nTuning\nBatch\nPrediction\nServing\nhadoop\nSELDON\nAPACHE\nOPTUNA\nSpark\nH\nHYPEROPT\nNeed to stitch together a bunch of disparate systems\n\u2022 Hard to develop\n\u2022 Hard to deploy\n.\n\u2022 Hard to manage\n\u2022 Slow\n52\n52\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-3-Columbia (1).pdf": {
        "text": "Ray is designed to support all these workloads\nLibraries\nPreprocessing\n(e.g., ingestion,\nfeaturization)\nTraining\nTuning\nBatch\nPrediction\nServing\nRAY\nUnified compute framework for\ndistributed application\n53\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-3-Columbia (1).pdf": {
        "text": "54\n54\nRay: a short history\n2016: Started as a class project\nBerkeley\nNEVER EXPOSE THE WRONGDOING OF THE\nUNIVERSITY OF CALIFORNIA\nRAY\nriselab\n. Initial goal: scale distributed deep neural network training\nand reinforcement learning\n2017: RLlib and Ray Tune released\n2019: Anyscale founded (company behind Ray)\n2020: Ray v1.0 release; Ray Serve released\n2022: Ray v2.0; Al Runtime (AIR) alpha released\nUC Berkeley\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-3-Columbia (1).pdf": {
        "text": "Minimalist API\nray.init()\nInitialize Ray context.\n@ray.remote\n.remote\nray.put()\nray.get()\nray.wait()\n\u101d\u101a\u103a\n0% RAY\nFunction or class decorator specifying that the function will be executed as a task or\nthe class as an actor in a different process.\nPostfix to every remote function, remote class declaration, or invocation of a remote\nclass method. Remote operations are asynchronous.\nStore object in object store, and return its ID. This ID can be used to pass object as\nan argument to any remote function or method call. This is a synchronous operation.\nReturn an object or list of objects from the object ID or list of object IDs. This is a\nsynchronous (i.e., blocking) operation.\nFrom a list of object IDs returns (1) the list of IDs of the objects that are ready, and (2)\nthe list of IDs of the objects that are not ready yet.\n55\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-3-Columbia (1).pdf": {
        "text": "The FAST Compute Model\nFutures: reference to objects (possible not created yet)\nActors: remote class instance (object)\nShared in-memory distributed object store\nTasks: remote functions\n56\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-3-Columbia (1).pdf": {
        "text": "Python example\ndef f(x):\n# compute...for 1s\nreturn r\nx = f(a)\ny = f(b)\n1 second\n2 seconds\n1 second\n57\n57\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-3-Columbia (1).pdf": {
        "text": "Ray: Function \u2192 Task\n@ray.remote\ndef f(x):\nDriver\n# compute...for 1s\nreturn r\nWorker\nWorker\nf(a)\n\u2717_id = f.remote(a)\ny_id = f.remote (b)\nray.get([x_id, y_id])\nXid\nFuture:\nnon-blocking call\n58\n",
        "links": []
    },
    "./chunks/chunk_53_Lecture-3-Columbia (1).pdf": {
        "text": "Ray: Function \u2192 Task\n@ray.remote\nDriver\nWorker\nWorker\ndef f(x):\n# compute...for 1s\nf(a)\nf(b)\nreturn r\nx_id = f.remote (a)\ny_id = f.remote (b)\nray.get([x_id, y_id])\ny_id\n59\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-3-Columbia (1).pdf": {
        "text": "Ray: Function \u2192 Task\n@ray.remote\nDriver\nWorker\nWorker\ndef f(x):\n# compute...for 1s\nf(a)\nf(b)\nreturn r\nx_id = f.remote (a)\ny_id = f.remote (b)\nray.get([x_id, y_id])\nBlocking call\n60\n",
        "links": []
    },
    "./chunks/chunk_55_Lecture-3-Columbia (1).pdf": {
        "text": "Ray: Function \u2192 Task\n@ray.remote\ndef f(x):\nDriver\n# compute...for 1s\nreturn r\nWorker\nWorker\nf(a)\n1 second\nf(b)\n1 second\nx_id\n-\n=\nf.remote(a)\ny_id = f.remote (b)\nray.get([x_id. y_id])\nx = y\nX\nr\nr\n61\n",
        "links": []
    },
    "./chunks/chunk_56_Lecture-3-Columbia (1).pdf": {
        "text": "Ray: Function \u2192 Task\n@ray.remote\ndef f(x):\nDriver\n# compute...for 1s\nreturn r\nx_id = f.remote (a)\ny_id = f.remote (b)\nray.get([x_id. y_id])\nxy\nWorker\nWorker\nf(a)\n1 second\n1 second\nf(b)\n1 second\nr\n62\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-3-Columbia (1).pdf": {
        "text": "Python Class\nclass Counter(object):\ndef __init__(self):\nself.value = 0\ndef inc(self):\nself.value += 1\nreturn self.value\nCounter()\nC =\nc.inc()\nc.inc()\nRay: Class Actor\n@ray.remote\nclass Counter(object):\ndef __init__(self):\nself.value = 0\ndef inc(self):\nself.value += 1\nreturn self.value\nCounter.remote ()\nC\n=\nid4\n=\nc.inc.remote ()\nid5 =\nc.inc.remote()\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-3-Columbia (1).pdf": {
        "text": "Python Class\nclass Counter(object):\ndef _ init _ (self):\nself.value = 0\ndef inc(self):\nself.value += 1\nreturn self.value\nc = Counter()\nc.inc()\nc.inc()\nRay: Class Actor\n@ray.remote (num_cpus=2, num_gpus=1)\nclass Counter (object)\ncan specify resource demands;\nsupport heterogeneous hardware\nself.value += 1\nreturn self.value\nc = Counter.remote()\nid4 = c.inc.remote()\nid5 = c.inc.remote()\n64\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-3-Columbia (1).pdf": {
        "text": "Shared in-memory object store\nnode 1\n@ray.remote\ndef f():\n# compute...\nreturn x\n@ray.remote\ndef g(a):\n# compute...\nreturn y\n=\nid_x\nid_y\n=\nf.remote()\ng.remote (id_x)\nnode 2\nnode 3\nDistributed object store\n65\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-3-Columbia (1).pdf": {
        "text": "Shard in-memory object store\nnode 1\n@ray.remote\ndef f():\n# compute...\nreturn x\n@ray.remote\ndef g(a):\n# compute...\nreturn y\nid_x = f.remote ()\nid_y = g.remote(id_x)\nnode 2\nid_x\nid_x = f()\nOnly x's id (id_x) is\nreturned, not X's value\nnode 3\n66\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-3-Columbia (1).pdf": {
        "text": "Shard in-memory object store\nnode 1\n@ray.remote\ndef f():\n# compute...\nreturn x\n@ray.remote\ndef g(a):\n# compute...\nreturn y\nid x = f.remote())\nid_y\n=\ng.remote(id_x)\nid_y\nnode 2\nnode 3\nid_x = f()\nid_y = g(id_x)\n67\n",
        "links": []
    },
    "./chunks/chunk_62_Lecture-3-Columbia (1).pdf": {
        "text": "Shard in-memory object store\nnode 1\n@ray.remote\ndef f():\nnode 2\nnode 3\n# compute...\nreturn x\nid_x = f()\nid_y = g(id_x)\n@ray.remote\ndef g(a):\n# compute...\nreturn y\nid_x\n=\nf.remote()\nid_y = g.remote (id_x)\nid_x\nX\nid_x \u2611\n68\n",
        "links": []
    },
    "./chunks/chunk_63_Lecture-3-Columbia (1).pdf": {
        "text": "Shard in-memory object store\nnode 1\n@ray.remote\ndef f():\n# compute...\nreturn x\n@ray.remote\ndef g(a):\nX >\n=\n||\ny =\n# compute...\nreturn y\nf.remote()\ng.remote (id_x)\n\u2713\nnode 2\nnode 3\nx = f()\ny = g(id_x)\nX\nWithout object store x would have been\ntransferred twice:\n-\nnode 2 \u2192 node 1 and node 1 \u2192 node 3!\n69\n",
        "links": []
    },
    "./chunks/chunk_64_Lecture-3-Columbia (1).pdf": {
        "text": "Ray Ecosystem\nRay AIR enables simple scaling if ML workloads.\nRay core enables generic\nscalable Python applications.\nData Train\nTune\nServe\nRLlib\nCustom applications\nRAY Core\nTasks\nActors\nObject Store\naws\n000\n70\n70\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-3-Columbia (1).pdf": {
        "text": "Ray Al Runtime (AIR): scalable runtime for E2E ML apps\nData / features\nARROW DELTA LAKE\nSpark\nParquet\nMODIN\ndask\nFEAST MARS\nt\nTraining\nHyperparameter\nTuning\nXGBoost\nExperiment Serving/\nmanagement Applications\nExplanability/\nObservability\nAX\nTensorFlow\n(H)\nW&B\nFlask\nWHYLABS\nOPyTorch\n\u03a3\n*\nmiflow\nFastAPI\nAarize\nLightGBM\ncomet\nK Keras\nHEBA\nT\nStreamlit\n3 gradio\nALIBI\nDatasets\nTrain\nStorage and metadata\nRLlib\nt\nt\nTune\nRay Al Runtime\nRay Core\naws\nAO\nServe\n->\n71\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-3-Columbia (1).pdf": {
        "text": "Using Ray AIR to Scale \"Classic\" E2E DL Workflow\nData\nPreprocessing\nDistributed\nTraining\nHyperparameter\nTuning\non Ray\non Ray\non Ray\nBatch\nPrediction\non Ray\n72\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-3-Columbia (1).pdf": {
        "text": "Using Ray AIR to Scale \"Classic\" E2E DL Workflow\ndataset = ray.data.read_csv(...)\ntrain_ds, valid_ds = train_test_split(\ndataset, test_size=0.3)\ntest_ds\nvalid_ds.drop_columns([\"target\"])\npreprocessor = StandardScaler (columns=[\"mean radius\"])\nScalable Data\nPreprocessing\n(Ray Data)\nRay Data library built for ML tasks\n\u2022 Efficiently load distributed data from MB to TB scale\n\u2022 Preprocessors for unified training <> inference\n73\n",
        "links": []
    },
    "./chunks/chunk_68_Lecture-3-Columbia (1).pdf": {
        "text": "Using Ray AIR to Scale \"Classic\" E2E DL Workflow\ndataset ray.data.read_csv(...)\n=\ntrain_ds, valid_ds = train_test_split(\ndataset, test_size=0.3)\ntest_ds valid_ds.drop_columns ([\"target\"])\n=\npreprocessor = StandardScaler (columns=[\"mean radius\"])\ntrainer = ray.train.huggingface. Transformers Trainer (\nscaling_config=ScalingConfig(num_workers=128),\nlabel_column=\"target\",\ndatasets dict(train-train_ds, valid=valid_ds},\npreprocessor=preprocessor)\nresult = trainer.fit()\nScalable Data\nPreprocessing\n(Ray Data)\nScalable Model Training\n(Ray Train)\n74\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-3-Columbia (1).pdf": {
        "text": "Using Ray AIR to Scale \"Classic\" E2E DL Workflows\ndataset =\nray.data.read_csv(...)\ntrain_ds, valid_ds = train_test_split(\ndataset, test_size=0.3)\ntest_ds valid_ds.drop_columns ([\"target\"])\npreprocessor = Standard Scaler (columns=[\"mean radius\"])\ntrainer ray.train.hugging face. Transformers Trainer(\nscaling_config=ScalingConfig(num_workers=128),\nlabel_column=\"target\",\ndatasets dict(train-train_ds, valid=valid_ds},\npreprocessor=preprocessor)\nresult = trainer.fit()\ntuner = ray.tune. Tuner (\ntrainer,\nparam_space={\"params\": {\"1r\": tune.loguniform (1e-5, 5e-4)}},\ntune_config=TuneConfig(\nnum_samples=5, metric=\"logloss\", mode=\"min\"),\ncheckpoint = tuner.fit().get_best_result().checkpoint\nScalable Data\nPreprocessing\n(Ray Data)\nScalable Model Training\n(Ray Train)\nScalable Model Tuning\n(Ray Tune)\n75\n",
        "links": []
    },
    "./chunks/chunk_70_Lecture-3-Columbia (1).pdf": {
        "text": "Using Ray AIR to Scale \"Classic\" E2E DL Workflows\ndataset ray.data.read_csv(...)\n=\ntrain_ds, valid_ds = train_test_split(\ndataset, test_size=0.3)\ntest_ds = valid_ds.drop_columns([\"target\"])\npreprocessor = StandardScaler (columns=[\"mean radius\"])\ntrainer ray.train.huggingface. Transformers Trainer (\nscaling_config=ScalingConfig(num_workers=128),\nlabel_column=\"target\",\ndatasets dict(train-train_ds, valid=valid_ds},\npreprocessor-preprocessor)\nresult =\ntrainer.fit()\ntuner ray.tune. Tuner (\n)\ntrainer,\nparam_space={\"params\": {\"1r\": tune.loguniform (1e-5, 5e-4)}},\ntune_config=TuneConfig(\nnum_samples=5, metric=\"logloss\", mode=\"min\"),\ncheckpoint = tuner.fit().get_best_result().checkpoint\nbatch_predictor = Batch Predictor.from_checkpoint (\ncheckpoint, XGBoost Predictor)\npredicted_probabilities = batch_predictor.predict(test_ds)\npredicted_probabilities.show()\nScalable Data\nPreprocessing\n(Ray Data)\nScalable Model Training\n(Ray Train)\nScalable Model Tuning\n(Ray Tune)\nScalable Batch Prediction\n(Predictors)\n.76.\n",
        "links": []
    },
    "./chunks/chunk_71_Lecture-3-Columbia (1).pdf": {
        "text": "Using Ray AIR to Scale \"Classic\" E2E DL Workflows\ndataset ray.data.read_csv(...)\n=\ntrain_ds, valid_ds = train_test_split(\ndataset, test_size=0.3)\ntest_ds valid_ds.drop_columns ([\"target\"])\n=\npreprocessor = StandardScaler (columns=[\"mean radius\"])\ntrainer ray.train.huggingface. Transformers Trainer(\nscaling_config=ScalingConfig(num_workers=128),\nlabel_column=\"target\",\ndatasets dict(train-train_ds, valid=valid_ds},\npreprocessor-preprocessor)\nresult =\ntrainer.fit()\ntuner ray.tune. Tuner (\n)\ntrainer,\nparam_space={\"params\": {\"1r\": tune.loguniform (1e-5, 5e-4)}},\ntune_config=TuneConfig(\nnum_samples=5, metric=\"logloss\", mode=\"min\"),\ncheckpoint tuner.fit().get_best_result().checkpoint\nbatch predictor = Batch Predictor.from_checkpoint(\ncheckpoint, XGBoost Predictor)\npredicted_probabilities = batch_predictor.predict(test_ds)\npredicted_probabilities.show()\nScalable Data\nPreprocessing\n(Ray Data)\nScale out to a cluster\nwith few line changes\nScalable Batch Prediction\n(Predictors)\n77\n",
        "links": []
    },
    "./chunks/chunk_72_Lecture-3-Columbia (1).pdf": {
        "text": "Scalable Online Inference\nwith Ray Serve\n\u00b7\nDeploy single models as HA\ninference services in Ray\nBuild multi-model pipelines\nwith custom business logic\ndeployment = Predictor Deployment.options(\nname=\"TransformerService\")\ndeployment.deploy(Torch Predictor, checkpoint, ...)\nprint(deployment.url)\nModel\nPredictor\nDeployment\nPrediction requests\n78\n8\n",
        "links": []
    },
    "./chunks/chunk_73_Lecture-3-Columbia (1).pdf": {
        "text": "\u0646\nTorchX\nTaking PyTorch in Production.\n79\n",
        "links": []
    },
    "./chunks/chunk_74_Lecture-3-Columbia (1).pdf": {
        "text": "Introduction\n01 DEFINE OR CHOOSE\nTorchX is an SDK for building\nand deploying ML apps from\nR&D to production\n02 RUN AS A JOB\n03 CONVERT TO PIPELINE\nl!il\nDefine your own components\n(job defs) using the torchx.specs API\nor choose one of the builtin\ntorchx.components and use them out-\nof-the-box.\n<>\nComponents can be run as a single\nnode or distributed job in one of the\nsupported schedulers or plugin your\nown scheduler\nComponents can be run in a\nproduction pipeline by converting\nthem into pipeline stages of one of\nthe supported ML pipeline\norchestrators\n80\n",
        "links": []
    },
    "./chunks/chunk_75_Lecture-3-Columbia (1).pdf": {
        "text": "Components\nfrom torchx.specs import AppDef, Role, named_resources\nimport torchx.components.dist as dist\ndef train(\n*script_args: str,\n) -> AppDef:\nreturn AppDef(\nname=\"train\",\nroles=[\nRole(\nComponent-python function\nthat parameterizes an AppDef\nAppDef - job definition data class\n\u2022\nBuiltin components:\nUse out-of-the-box\nCompose to customize\nExamples: dist.ddp, serve, hpo, etc\n])\nname=\"worker\"\nimage=\"my/docker_img\",\nentrypoint=\"my/trainer/main.py\",\nargs=*script_args,\nenv={\"RANK\":\"0\", \"WORLD_SIZE\":\"1\"},\nresource=named_resources [\"aws_p3.8xlarge\"],\ndef train_dist(\nnnodes: int,\nnproc_per_node: int,\n*script_args:str,\n) -> AppDef:\nreturn dist.ddp(\n*script_args,\nimage=\"my/docker_img\",\nscript=\"my/trainer/main.py\",\nj=f\"{nnodes}x{nproc_per_node}\",\nh=\"aws_p3.8xlarge\")\n",
        "links": []
    },
    "./chunks/chunk_76_Lecture-3-Columbia (1).pdf": {
        "text": "TorchX Run\nRun a custom component\n$ torchx run -s kubernetes ~/component.py:train_dist\n--nnodes 4\n--nprocs_per_node 8\nfoo bar\nOR a builtin component\n$ torchx run -s kubernetes dist.ddp\n-j 4x8\n-h aws_p3.8xlarge\n--image my/docker_img\n--script my/trainer/main.py\nfoo bar\nJob runs locally or on one of the supported schedulers\nEasy to add a custom scheduler\nAppDef\n(job def)\nRUN ON $SCHED\ntorchx.runner\nSUBMIT JOB kubernetes\nslurm\nworkload manager\n$_\nlocalhost\ncustom\n",
        "links": []
    },
    "./chunks/chunk_77_Lecture-3-Columbia (1).pdf": {
        "text": "TorchX Pipelines\nRun components as a stage in an ML pipeline\nUse torchx.pipelines adapter to convert a\ncomponent into a pipeline op\nDefine DAG (op dependencies) in the pipeline\norchestrator's DSL not TorchX\nCan mix legacy pipeline ops with TorchX components\nKubeflow\nkfp.Op\ntorchx.pipelines.kfp\nkfp.Op\nkfp.Op\ncomponent_from_app AppDef)\nkfp.Op\n",
        "links": []
    },
    "./chunks/chunk_78_Lecture-3-Columbia (1).pdf": {
        "text": "TorchX Module Dependency UML\ntorchx.cli\ntorchx.runner\nLocal and Remote Launchers\ntorchx.schedulers\nScheduler Interface\nlocal\n- mast\no flow\n- kubeflow (kubernetes)\nslurm\nUser Component\nComponent spec for user app\nUser Project\nUser Pipeline\nUser App\nUser implemented E2E pipeline\nUser implemented app\ntorchx.runtime\nLibs app use for scheduler-agnostic\nabstractions\n-\nStorageProvider\nMetricLogger\n...etc...\nPipeline Platform SDK\n- kfp-1.4.1\n...etc...\ntorchx.pipelines\nAdapters to Pipeline Platforms\nkfp\nairflow\nf6\ntorchx.components\nComponent templates\none_role_app\nmaster_worker_app\nhpo_app\ntorch_serve_deploy_app\n...etc...\ntorchx.apps\nPredefined Apps to compliment\ncomponent templates\n- hpo\ncoordinator\nreporter\n...etc...\n- ...etc...\ntorchx.spec\nBase entity APIs\nComponent\nContainer\nImage\n- ...etc...\nbuild-dep\nruntime-dep\ntorchx\nuser\n3rd party\n84\n",
        "links": []
    },
    "./chunks/chunk_79_Lecture-3-Columbia (1).pdf": {
        "text": "Reference\nPapers\n\u25cf\n.\nCompute Trends across Three Eras of Machine\nLearning, https://arxiv.org/pdf/2202.05924\nLanguage Models are Few Shot Learners,\nhttps://splab.sdu.edu.cn/GPT3.pdf\nPathways Language Model (PaLM): Scaling to\n540 Billion Parameters for Breakthrough\nPerformance,\nhttps://research.google/blog/pathw\nays-language-model-palm-scaling-to-540-\nbillion-parameters-for-breakthrough-\nperformance/\nMoritz, Philipp, et al. \"Ray: A distributed\nframework for emerging {AI} applications.\" 13th\nUSENIX symposium on operating systems\ndesign and implementation (OSDI 18). 2018.\n85\n",
        "links": [
            "https://arxiv.org/pdf/2202.05924",
            "https://splab.sdu.edu.cn/GPT3.pdf",
            "https://research.google/blog/pathw"
        ]
    },
    "./chunks/chunk_80_Lecture-3-Columbia (1).pdf": {
        "text": "Blogs,\nVideos, Code\nLinks\nJanakiram. 10 KEY ATTRIBUTES OF CLOUD-\nNATIVE APPLICATIONS\nAWS Documentation. How Amazon\nSagemaker Works\nJames Quigley. Microservices, Docker, and\nKubernetes\n\u2022 Ben Corrie. What is a container?. Video\n\u2022 Microsoft Azure. Kubernetes Learning Path.\nA hands-on course.\n\u2022 TorchX Quickstart.\nKubeflow. Getting Started with Kubeflow\n86\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Lecture 2 09/10/24\n+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\nParijat Dube and Chen Wang\n1\n+\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Recall from Last lecture\n\u2022 ML concepts: Bias-variance tradeoff, generalization, regularization\n\u00b7\n\u25cf\n\u2022\nAlgorithmic and system\nSteps in training a deep neural network; Stochastic gradient descent\nHyperparameters in deep learning\nVanishing gradient problem and Weight initialization\nLearning rate and batch size\n\u2022 Momentum\n\u25cf\nBatch normalization\nRegularization techniques: early stopping, dropout, dataset\naugmentation\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Single Node, Single GPU Training\n\u2022 Training throughput depends on:\n\u2022 Neural network model (activations, parameters, compute operations)\nBatch size\n\u2022\n\u2022 Compute hardware: GPU type (e.g., Nvidia M60, K80, P100, V100)\n\u2022\nFloating point precision (FP32 vs FP16)\n\u2022\nUsing FP16 can reduce training times and enable larger batch sizes/models without\nsignificantly impacting the accuracy of the trained model\n\u2022 Increasing batch size increases throughput\n\u2022\n\u2022 Batch size is restricted by GPU memory\nTraining time with single GPU very large: 6 days with Places dataset (2.5M\nimages) using Alexnet on a single K40.\n\u2022 Small batch size => noisier approximation of the gradient => lower learning\nrate => slower convergence\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-2-columbia-Fall2024.pdf": {
        "text": "GPU\nSingle GPU Training\nFP32 TensorFlow Training Performance\n1 = Same Speed as RTX 2080 Ti, 2 = Twice as Fast as RTX 2080 Ti, Etc.\nRTX 2080 Ti\nRTX 2080\nTitan RTX\nTitan V\nV100\nTitan Xp\nGTX 1080 Ti\n0\n0.5\n0.73x\n0.82x\n0.74x\nPerformance Multiple Over to RTX 2080 Ti\n\u2610 Lambda\n1.5\nResNet50\nResNet152\nInceptionV3\nInceptionV4\nVGG16\nAlexNet\nSSD300\nGPU Prices\n\u2022 RTX 2080 Ti: $1,199.00\n\u2022 RTX 2080: $799.00\nTitan RTX: $2,499.00\n\u2022 Titan V: $2,999.00\n\u2022 Tesla V100 (32 GB): ~$8,200.00\n\u2022 GTX 1080 Ti: $699.00\n\u2022 Titan Xp: $1,200.00\n1.15x\n1.04x\n1.37x\nThe scaling with GPU type is dependent on neural network architecture.\nhttps://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/\n",
        "links": [
            "https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/"
        ]
    },
    "./chunks/chunk_5_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Single Node, Multi-GPU Training\nSpeed-up Multiplier\nFP32 Multi-GPU Scaling Performance (1, 2, 4, 8 GPUs)\n1.0 = 1x Faster Than Single GPU, 2.0 = 2x Faster than Single GPU\n8\n5.91\n6\n5.09\n5.18\n2\n1\n1.8\nX Lambda\n1 GPU\n7.31\n2 GPUs\n6.54\n6.55\n4 GPUs\n8 GPUs\n3.6\n3.7\n3.32\n3.58\n3.48\n3.22\n1\n1.88\n1.62\n1\n1.91\n1.59\n1.62\n1\n0\nRTX 2080 Ti\nTitan RTX\nTitan V\nV100\nTitan Xp\nGTX 1080 Ti\nGPU\nhttps://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/\n",
        "links": [
            "https://lambdalabs.com/blog/2080-ti-deep-learning-benchmarks/"
        ]
    },
    "./chunks/chunk_6_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Multi-GPU Execution Scaling\n\u2022 Vertical scaling-up in a single\nnode\n\u2022 NVIDIA DGX-1 (8 P100 GPUs) and\nDGX-2 (16 V100 GPUs) servers\n\u25cf PCI-e switch\n--QPI\n-- PCI-e O GPU\n\u2015 NVLink O CPU\nQPI\nQPI\n\u2022\nHorizontal scaling-out across\nmultiple nodes\n\u2022\nExample: GPU accelerated\nsupercomputers like Summit and\nSierra from US Department of\nEnergy\nOG2\nG4\nNVLink\nOG7\nG4\nNVLink\nG5\nG6\nPCI-e\nC1\n(A) P100-based DGX-1 with NVLink-V1\nG5\nG6\nPCI-e\nC1\nG2\nX-Bus\nNVLink-V1\nG7\n(B) V100-based DGX-1 with NVLink-V2\nFig. 1: PCle and NVLink-V1/V2 topology for P100-DGX-1 and V100-DGX-1.\nNVLink-V2\nX-Bus\nAAAA\n(A) SummitDev\n(B) Summit\nG5\nFig. 2: NVLink interconnect topology for SummitDev and Summit.\nEvaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Single Node, Multi GPU Training\n\u2022\n\u2022\nCommunication libraries (e.g., NCCL) and supported communication\nalgorithms/collectives (broadcast, all-reduce, gather)\n\u2022 NCCL (\"Nickel\") is library of accelerated collectives that is easily integrated\nand topology-aware so as to improve the scalability of multi-GPU applications\nCommunication link bandwidth: PCIe/QPI or NVlink\nCommunication algorithms depend on the communication topology\n(ring, hub-spoke, fully connected) between the GPUs.\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Ring based collectives\nCPU\nSwitch\nPCle: Peripheral Component Interconnect express\nSMP: symmetric multiprocessing\nQPI: Quick-path interconnect (25.6 GB/s)\nGPUO\nCPU1\nGPC2\nGPU3\n4-GPU-PCle\nCPU\nSwitch\nSPU\nSwitch\nGPUO\nG. U1\nGPU2\nGPU3\nGPU4\nG. U5\nGPUS\nGPU7\nSMP Connection\n(e.g., QPI)\nPCIe Gen3 x16\n~12 GB/s\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-2-columbia-Fall2024.pdf": {
        "text": "PCle and NVLink\n--QPI\nPCI-e switch\n-- PCI-e O GPU\n-\nNVLink\nCPU\nG1.\nG4\nNVLink\nCO\n80.\nQPI\nGO\nG5\u00ae\nG6\u00ae\nPCI-e\nG1/\nG7\nG4\nNVLink\nCO\nQPI\nG5 \u00ae\nG6 \u00ae\nPCI-e\nG2\nG7\n(A) P100-based DGX-1 with NVLink-V1\n(B) V100-based DGX-1 with NVLink-V2\nFig. 1: PCle and NVLink-V1/V2 topology for P100-DGX-1 and V100-DGX-1.\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Baseboard\nBaseboard\nNVSwitch and PCle in DGX-2\n60 61 62 63 64 65 66\nNVLink\nGO G1 G2 G3 G4 G5 G6 G7\n80\n03\nga\n08\nCPU\nGPU\nG8\n08\nG9 G10 G11 G12 613 614\nFig. 3: NVSwitch interconnect topology in DGX-2.\nNVSwitch\nPCI-e\nG8\nG15\nPCI-e switch\nQPI\nLevel-2 PCle switch\nLevel-1 PCle switch\n......\nG10 G11 G12 G13 G14 G15\nFig. 4: PCle interconnect topology in DGX-2.\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Ring based collectives with NVLink\nGPUO\nCPU\n#\nSwitch\nGPU1\nGPUO\nCPU\nSwitch\nGPU2\nGPU3\nGPU2\n4-GPU-Ring\n4-GPU-FC\nPCIe Gen3 x16\n~12 GB/s\nGPU1\nNVLink\n~16 GB/s\nGPU3\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Distributed Training\n\u2022\nType of Parallelism: Model, Data, Hybrid\n\u2022 Type of Aggregation: Centralized, decentralized\n\u2022 Centralized aggregation: parameter server\n\u25cf\nDecentralized aggregation: P2P, all reduce\nPerformance metric: Scaling efficiency\n.\nDefined as the ratio between the run time of one\niteration on a single GPU and the run time of one\niteration when distributed over n GPUs.\n1 GPU\nmulti-GPU, multi-node\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Parallelism\n\u2022 Parallel execution of a training job on different compute units through\nscale-up (single node, multiple and faster GPUs) or scale-out (multiple\nnodes distributed training)\nEnables working with large models by partitioning model across learners\nEnables efficient training with large datasets using large \"effective\"\nbatch sizes (batch split across learners)\n\u2022\nSpeeds up computation\n\u2022 Model, Data, Hybrid Parallelism\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Model Parallelism\n\u2022 Splitting the model across multiple learners\nLearner 3\nLearner 1\n\u0b6a\u0b6a\u0b6a\u0b6a\u0b6a\u0b6a\u0b6a\u0b6a\nLearner 2\nLearner 4\n\u2022\n\u2022\n\u2022\n\u2022\n5 layered neural network\nPartitioned across 4 learners\nBold edges cross learn boundaries and involve\ninter-learner communication\nPerformance benefits depend on\n\u2022 Connectivity structure\n\u2022\nCompute demand of operations\nHeavy compute and local connectivity-benefit\nmost\n\u2022\nEach machine handles a subset of\ncomputation\n\u2022\nLow network traffic\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-2-columbia-Fall2024.pdf": {
        "text": "DL Pipelining\nPipelining approach:\nGPU1\nb2\nL1\n(L1)\nGPU2\nL2\n.\n\u2022 Split layers among compute engines\n(L2)\n\u2022 Each minibatch b (or sample s) goes from one\ncompute engine to the next one: no need to\nwait for next one to exit the pipeline\nGPU3\nL3\n(L3)\nGPU4\nL4\n(L4)\n\u2022 Is a form of Model Parallelism\n\u2022\nPipelining performance\nIdeal pipelining speedup (number of pipeline\nstages)\nGPU1\ntime without pipeline\nb8\nL1\n(L1)\nS_time(stage time)\nnumber of pipeline stages\nGPU2\nL2\n.\nSpeedup is higher for deeper networks\n(L2)\n.\nIdeal pipelining never reached because of\n\"bubbles\" that cause idle CPUs\nGPU3\nL3\n\u2022 SGD pipeline bubble:\n\u2022\nBefore weights update, all batches need to have\ncompleted forward (otherwise accept staleness)\n>>\ntime\n\u0126 to ti tz tx ts to ty ts\n52\nb2\n\u2610 b2\n1b2\nb1\nb1\nbl\nbl\nNon-pipelined execution\ntime\n\u2610 to t\u2081 t2 ta ts to t to\n(L3)\nGPU4\nL4\n(L4)\nb7 b6 b5 b4 b3 b2\nH\nb1\n\u2610 b8 b7 b6 b5 b4 b3 b2\n\u2610\nb8 b7 b6 b5 b4 b3\n\u2610 b8 b7 b6 b5 b4\nPipelined execution\nHPML\n15\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-2-columbia-Fall2024.pdf": {
        "text": "GPipe Pipelining\nFo\nBo\nUpdate\nLoss\nBo\nUpdate\nDevice 3\nF3\nB3\nF\u2081\nBo\nUpdate\nTime\nF\u2081\nBo\nUpdate\nDevice 2\nF\u2082\nB2\nDevice 1 F\u2081\nB\u2081\nFao Fat Faz Faa Baa Baz Bat Bao\nUpdate\nF20 F21 F22 F2.3\nB2.3 B2,2 B2,1 B2,0\nUpdate\nDevice 0\nFo\nBo\nF10 F1.1 F12 F1,3\nB1,3 B1.2 B1,1 B1,0\nUpdate\nFoo Fo: Foz F0.3\nBubble\nB0,3 B0,2 B0,1 B0,0 Update\nGradients\n(a)\n(c)\n\u2022\nGpipe: a pipeline parallelism open-source library that allows scaling any network that can be expressed\nas a sequence of layers.\n\u2022\n\u2022\nSplit global batch into multiple micro-batches and injects them into the pipeline concurrently\nNot memory-friendly and will not scale well with large batch. The activations produced by forward tasks\nhave to be kept for all micro-batches until corresponding backward tasks start, thus leads to the memory\ndemand to be proportional (O(M)) to the number of concurrently scheduled micro-batches (M).\nHPML\nGPipe: Easy Scaling with Micro-Batch Pipeline Parallelism\n16\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Model Parallelism and model saving technique in\nAmazon Sagemaker model parallel library\n\u2022 Pipeline Parallelism\n\u2022 Tensor Parallelism\nOptimizer state sharding\n\u2022 Activation offloading and checkpointing\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html\n",
        "links": [
            "https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html"
        ]
    },
    "./chunks/chunk_18_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Pipeline Parallelism\nPartition 0\nPartition 1\n\"pipeline_parallel_degree\": 2,\n\"microbatches\": 4,\n\"ddp\": True,\nSample model with four layers\nModel parallel configuration\nBatch 1\nBatch 2\nBatch 3\nBatch 4\nmicrobatch 1\nmicrobatch 1\nmicrobatch 2\nmicrobatch 3\nmicrobatch 2\nmicrobatch 3\nmicrobatch 1\nmicrobatch 2\nmicrobatch 3\nmicrobatch 1\nmicrobatch 2\nmicrobatch 3\nmicrobatch 4\nmicrobatch 4\nmicrobatch 4\nmicrobatch 4\nDP_GROUP\nL3\nL4\nL4\nL4\nL3\nL4\nGPU 1\nGPU 3\nGPU 5\nGPU 7\nMP_GROUP=PP_GROUP\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Tensor Parallelism\nBatch 1\nL1\nL2\nL3\nL4\nSample model with four layers\nBatch 3\nDP_GROUP\n\"pipeline_parallel_degree\": 1,\n\"tensor_parallel_degree\": 2,\n\"ddp\": True\nModel parallel configuration\nBatch 5\nBatch 7\n12 13 14\nL1\nL2 L3 L4\nL1\nL2 L3 L4\nL1\nL2 L3 L4\nGPU 2\nGPU 4\nGPU 6\nGPU 0\nMP_GROUP=TP_GROUP\nL1\nL2 L3 L4\nL1 L2 L3 L4\nL1 L2 L3 L4\nL1\nL2 L3\nL4\nGPU 1\nGPU 3\nGPU 5\nGPU 7\nBatch 2\nBatch 4\nBatch 6\nBatch 8\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Optimizer state sharding\nOptimizer\nState\nfor L1\nL4\nSample model with four layers\nL2\nL4\nL1\nGPU 0\nOptimizer\nState\nfor L2\nGPU 1\nGPU 2\nOptimizer\nState\nfor L3\nGPU 3\nOptimizer\nState\nfor L4\nTimeline on GPU 0\nduring a backward propagation\nL4\nL4\nL4\nCompute (GPU)\nNetwork\nR\nReduce collective\nAG All-gather collective\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Data Parallelism\nLearner 1\nLearner 2\nLearner 3\nModel\n=88 68 68\nReplicas\nData\nShards\n\u2022\nModel is replicated on different learners\n\u2022 Data is sharded and each learner work on a different partition\n\u2022\n\u2022\nHelps in efficient training with large amount of data\nParameters (weights, biases, gradients) from different replicas need to be\nsynchronized\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Parameter server (PS) based\nSynchronization\nParameter Server\nAWin\nWi+1\nAWi,1\nWi+1\nModel\nReplicas\nData\nShards\n\u2022 Each learner executes the entire model\n\u2022 After each mini-batch processing a learner calculates the gradient and\nsends it to the parameter server\n\u2022\nThe parameter server calculates new value of weights and sends them to\nthe model replicas\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Synchronous SGD and the Straggler\nproblem\n\u2022 PS needs to wait for updated gradients from all the learners before\ncalculating the model parameters\nEven though size of mini-batch processed by each learner is same,\nupdates from different learners may be available at different times at\nthe PS\n.\nRandomness in compute time at learners\n\u2022 Randomness in communication time between learners and PS\nWaiting for slow and straggling learners diminishes the speed-up\noffered by parallelizing the training\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Synchronous SGD Variants\nFully Sync-SGD\nK-sync SGD\nK-batch-sync SGD\nPS\nL\u2081\nL2\nL3\nNS\n\u2022\n\u2022\nWo\nW1 W2\nWo W1 W2\nWo W1 W2\nP: total number of learners\nPS\nPS\n\u2022\nL\u2081\nL\u2081\nL\u2082\nL3\nL2\n\u2022\nL3\nK: number of learners/mini-\nbatches the PS waits for before\nupdating parameters\nLightly shaded arrows indicate\nstraggling gradient computations\nthat are canceled.\nK-sync SGD: PS waits for gradients from K learners before updating parameters; the\nremaining learners are canceled\n\u2022\nWhen K = P, K-sync SGD is same as Fully Sync-SGD\n\u2022\nK-batch sync: PS waits for gradients from K mini-batches before updating parameters; the\nremaining (unfinished) learners are canceled\nIrrespective of which learner the gradients come from\nWherever any learner finishes, it pushes its gradient to the PS, fetches current\nparameter at PS and starts computing gradient on the next mini-batch based on the\nsame local value of the parameters\nRuntime per iteration reduces with K-batch sync; error convergence is same as K-sync\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Asynchronous SGD and Stale Gradients\n\u2022 PS updates happen without waiting for all learners\n\u2022 Weights that a learner uses to evaluate gradients may be old values of\nthe parameters at PS\n\u2022 Parameter server asynchronously updates weights\n\u2022\nBy the time learner gets back to the PS to submit gradient, the weights may\nhave already been updated at the PS (by other learners)\n\u2022 Gradients returned by this learner are stale (i.e., were evaluated at an older\nversion of the model)\n\u2022 Stale gradients can make SGD unstable, slowdown convergence,\ncause sub-optimal convergence (compared to Sync-SGD)\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Stale Gradient Problem in Async-SGD\nWorker 1\nW97\nWorker 2\nW98\nW101\nW102\nW101 W100-\u03bbAW97\n=\nW102 W101-AW98\nParam. Version\n100 101 102 103 104 105 106\n=\nW103 W102 AW100\n-\nW100\nWorker 3\nW99\nWorker 4\nW103\nW104\nW104 W103-AW99\nTime\nblog.skymind.ai Distrbuted Deep Learning, Part 1: An\nIntroduction to Distributed Training of Neural Networks\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Asynchronous SGD and Variants\nAsync SGD\nK-async SGD\nWO WW2 W3\nWo W1 W2 W3\nK-batch-async SGD\nWo W1 W2 W3\nPS\nPS\nPS\n4\u2081\nL\u2081\nL\u2081\nL2\nL3\nL2\nL3\nL2\nL3\n\u2022\n\u2022\n\u2022\nK-async SGD: PS waits for gradients from K learners before updating parameters but the remaining learners\nare not canceled; each learner may be giving a gradient calculated at stale version of the parameters\nWhen K = 1, K-async SGD is same as Async-SGD\nK-batch async: PS waits for gradients from K mini-batches before updating parameters; the remaining\nlearners are not canceled\n\u2022\nWherever any learner finishes, it pushes its gradient to the PS, fetches current parameter at PS and\nstarts computing gradient on the next mini-batch based on the current value of the PS\nRuntime per iteration reduces with K-batch async; error convergence is same as K-async\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Log loss\nError-Runtime Tradeoff in SGD Variants\n7.5\n5.0\n2.5\n0.0\n-2.5\n-5.0\n-7.5\nSynchronous\nAsynchronous\n0\n1\n3\n5\u00d7 105\nTime\n\u2022\nError-runtime trade-off for Sync and Async-\nSGD with same learning rate.\nAsync-SGD has faster decay with time but a\n\u2022\nhigher error floor.\nError in Convergence\nAsync SGD\nK-Batch Async\nK-Async SGD\nK=2\nK=2\nK=3\nK=4\nK=3\n- Fully Sync\nK=4 Batch Sync\nWall clock time\nDutta et al. Slow and stale gradients can win the race: error-\nruntime tradeoffs in distributed SGD. 2018\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Distribution of Staleness for K-batch Async\nN=30,K=30 (1-softsync)\n1\n0.2\n(a)\n0.8\nN=30,K=2 (15-softsync)\nN=30,K=1 (30-softsync)\n(c)\nCalled n-softsync protocol\nK = floor (N/n)\n0.6\n(b)\n0.1\n0.5 \ub289 0.05\n0.5 \u314e\n077\n0.4\n0.2\n0 1 2\nGradient staleness\n10\n20 30\nGradient staleness\n20 40 60\nGradient staleness\nThe gradient is on average N/K (N: number of learners) steps out of date by the time they are applied to\nthe global parameter vector.\nGupta el al. Staleness-aware Async-SGD for Distributed Deep\nLearning. 2016\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Staleness Dependent Learning Rate\n\u2022 With staleness-dependent learning rate setting Async-SGD can achieve\naccuracy comparable to Sync-SGD while achieving linear speedup of\nASGD\n\u2022\nDecrease the learning rate by mean staleness\nlearning rate (a)\n==\nbase learning rate (\u03b10)\naverage staleness ((\u03c3))\nTest error(%)\n100\n90\n90\n60\n80\n70\n60\n50\n60\n40\n40\nn=30, \u03bb=30\n---n=4, \u03bb=30\n30\n0\n20\n\u03b1+ \u03b1\u03c5\n40\n60\n\u03b1+\n(\u03c3)\n80\n100\n120\n140\nTraining epoch\nGupta el al. Model Accuracy and Runtime Tradeoff in\nDistributed Deep Learning: A Systematic Study 2016\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Reduction over Gradients\nTo synchronize gradients of N learners, a reduction operation needs\nto be performed\nN\n\u03a3\u03a5-1\u0394\u03c9;\n31\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Reduction Topologies\nParameter server: single reducer\nSend\nGPUs arranged in a logical Ring (aka bucket) :\nall are reducers\nReceive\nSend\nGPU 0\nGPU 1\nGPU 2\nGPU 3\nGPU 4\nReceive\nGPU 0\nReducer\nSUM (Reduce operation) performed at PS\n\u2022\nSend\nGPU 4\nReceive\nSend\nGPU 3\nGPU 2\nReceive\nSend\nGPU 1\nReceive\nSend\nReceive\nSUM (Reduce operation) performed at all nodes\nEach node has a left neighbor and a right neighbor\nNode only sends data to its right neighbor, and only\nreceives data from its left neighbor\n32\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-2-columbia-Fall2024.pdf": {
        "text": "All-Reduce\nGPU 0\nGPU 1\nArrays Being Summed\n\u300c\u2500\u3002\u300d\u300c\u3002\u3002\u2500\u300d\u300c\u2500\u3002\u300c\u3002\u300c\u3002\u0f0b\n\u300c\u2500a\u2500\u300d\u300c\u2500\u3002,\u2500\u300d\u300c\u2500\u3002\u300d4,\u2500\u300d\u3002,\u300d\nGPU 0\na\u2081+a+a+a+ab\u2082+b\u2081+b3+b+b0 C3+C\u2082+C4+C+C\u2081 d4++++++++\nGPU 1\na\u2081+a+a+a+a b\u2082+b\u2081+b3+b+b0 C3+C\u2082+C4+C0+C\u2081 d\u2081+d3+do+d\u2081+d\u2082+4+1+2+3\nGPU 2\n\u0105z brez dzer\nGPU 2\na\u2081+a+a+a+ab\u2082+b\u2081+b3+b+b0 C3+C\u2082+C4+Co+C\u2081 d4+d3+do+d\u2081+de++++\nGPU 3\naz Dg Cz dzez\nGPU 3\na\u2081+a+a+a+ab+b\u2081+b3+b+b C3+C\u2082+C4+C+C\u2081 4+3+do+d\u2081+d\u2082+++\u0435\u2082+3\nGPU 4\nPartitioning of an array into N chunks\nGPU 4\na\u2081+a+a+a+ab\u2082+b\u2081+b3+b+b0 C3+C\u2082+C4+C+C\u2081\u2081+3+do+d\u2081+d\u2082 +4+\u0435\u2081+\u0435\u2082+\u20ac3\nFinal state after all allgather transfers\n33\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Ring All-Reduce\n\u2022 Two step algorithm:\n\u2022\nScatter-reduce\nGPU 0\nArrays Being Summed\nao bo Co do e\nGPUs exchange data such\nthat every GPU ends up with\nGPU 1\na, b, c, d, e,\na chunk of the final result\n\u2022\nAllgather\nGPU 2\n\u2022\nGPUS exchange chunks from\nscatter-reduce such that all\nGPU 3\nGPUs end up with the\ncomplete final result.\nGPU 4\n32 b2 cz dzez\naz b3 Cz dzez\n\u2610 B4 b4 C4d484\nPartitioning of an array into N chunks\n34\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Ring All-Reduce: Scatter-Reduce Step\nArrays Being Summed\nGPU 0\nGPU 0\n\u300c\u2500\u3002\u300d\u3002\u3002\u300d\u300c\u2500\u3002 \u300d 4\u3002 \u300c \u3002 \u0964\nGPU 1\nGPU 2\nb,c, d, e,\n\u300c\u300d \u300c\u3002\u0f0d\u3002\u300d\u3002\u3002\nArrays Being Summed\nBobo CodoPotea\nGPU 1\na\u2081+20 b\u2081 C\u2081d, e,\nGPU 2\na\u2082\nb\u2082+b\u2081\nd\u2082\ne2\nGPU 3\naa 3Ca+C\u2082 d\u2082es\nGPU 3\nBy Dj Cg dj Cz\nB4b4Crdus\nGPU 4\nBa DaCad\u2081tdy\n3\nGPU 4\nGPU 0\nap\nbo\nC\u2082+C\u2082+C\u2082+Co d\u2081+da+do\ne0+\u20ac4\nGPU 0\nBo Do Codi+d+de+\nGPU 1\na\u2081+\u00e3o b\u2081 C\nGPU 1\nGPU 2\n\u2081+2+2 b\u2082+b\u2081 or or extextestez\na\u2081+\u00e3o b\u2081 C\u2081 d\u2081 e\u2081\u2081\u2081\n\u0414\nGPU 2\n[ antelotaz | batby 1 \uc77c \uc77c \uc77c\nGPU 3\n+++ b\u2082+by+b+ C\u2082 djeg\n\u041b\nGPU 3\nGPU 4\na4\nb\u2082+b\u2081+b3+b4 C3+C\u2082+C4\nd\u2081+d3\ne4\nGPU 4\na4\nb\u2082+b\u2081+b\u2081 C+Cz d\u2082Os\nC3+C\u2082+C4\n35\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Ring All-Reduce: End of Scatter-Reduce Step\nGPU 0\n\u10db\u10d4\nb\u2082+by+b3+b+bo C3+2+4+0 d4+13+d\ne0+\u04354\nHow many iterations in\nscatter-reduce step with N GPUs?\nGPU 1\na\u2081+20\nb\u2081\nC3+C\u2082+C+ C+C\u2081 +++\n+4+\u0435\u2081\nGPU 2\na\u2081+a+a\u2082\nb\u2082+b\u2081\nC2\nd4+d3+do+d+d\u2082+++\nGPU 3\na\u2081+a+a\u2082+23\nb\u2082+b\u2081+b3\nC3+C2\n\u10eb\u10d5\n+4+1+2+3\nGPU 4\na\u2081+a+a+a+a b\u2082+b\u2081+b3+b4 C3+C\u2082+C4\nd4+d3\ne4\nFinal state after all scatter-reduce transfers\n36\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Ring All-Reduce: What to do next?\nGPU O\n\u10db\u10d4\nb\u2082+by+b3+b+bo C3+C\u2082+C4+Co d4+d3+do\neo+e4\nGPU\nSend\nReceive\n0\nChunk 1\nChunk 0\nGPU 1\na\u2081+a\nb\u2081\nC3+C\u2082+C4+C+C\u2081 +++\u2081\n1\nChunk 2\nChunk 1\nGPU 2\na\u2081+a+a\u2082\nb\u2082+b\u2081\nC2\nd4+d3+do+d\u2081+d\u2082+++\n2\nChunk 3\nChunk 2\nGPU 3\na\u2081+a+a\u2082+3\nb\u2082+b\u2081+b3\nC3+C2\n\u10eb\u10d5\n3\nChunk 4\nChunk 3\n4\nChunk O\nChunk 4\nGPU 4\na\u2081+a+a+a+ab\u2082+b\u2081+b3+b4\nC3+C\u2082+C4\nd4+d3\ne4\nFinal state after all scatter-reduce transfers\n37\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Ring All-Reduce: AllGather Step\nGPU 0\na\nb\u2082+by+by+b+bo C3+C\u2082+C4+Co d4+d3+do\neo+e4\nGPU 0\na\u2081+a+a+a+a b\u2082+by+b3+b+bo C3+C\u2082+C4+Co d4+d3+do\neo+e4\n\u041b\nGPU 1\na\u2081+a\nGPU 1\na\u2081+a\nb\u2081\nb\u2082+by+by+b+b C3+C\u2082+C4+Co+C\u2081 d\u2081+3+do+d\u2081 \u04350+e+\u0435\u2081\nC3+C\u2082+C+Co+C\u2081 d\u2082+++\u2081\neo+e+e\u2081\n\u2193\n\u2193\nGPU 2\na\u2081+a+a\u2082\nb\u2082+b\u2081\nC\u2082\nd+da+do+d+d\u2082 eo+e+e\u2081+2\nGPU 2\na\u2081+a+a\u2082\nb\u2082+b\u2081\nC3+C\u2082+C+Co+C\u2081d+d\u2082+do+d\u2081+d\u2082 eo+e+e\u2081+e\u2082\n\u2193\nGPU 3\na\u2081+a+a+a3 b\u2082+b\u2081+b3\nC3+C\u2082\n\u10eb\u10d5\ne\u2081+e+\u0435\u2081+\u0435\u2082+\u20ac3\nGPU 3\na\u2081+a+a\u2082+a\nb\u2082+b\u2081+b3\nC3+C\u2082\nd+d+do+d+de++\u0435\u2081+\u0435\u2082+3\n\u2193\nGPU 4\na\u2081+a+a+a+a b\u2082+b\u2081+b3+b4 C3+C\u2082+C4\nd\u2081+d3\ne4\nGPU 4\na\u2081+a+a+a+ab\u2082+b\u2081+b3+b4 C3+C\u2082+C4\nd4+d3 \u0435+e+\u0435\u2081+\u0435\u2082+\u04353\nGPU 0\na\u2081+a+a+a+a b\u2082+b\u2081+b3+b+b C3+C\u2082+C+Cod4+d+do+d\u2081+de++\u2081+2+3\n\u2193\nGPU 0\na\u2081+a+a+a+ab+b\u2081+b3+b+b C3+C\u2082+C4+C0 d\u2081+d3+do\nGPU 1\na\u2081+a+a+a+a b\u2082+by+b3+b+b C3+C\u2082+C4+C\u2081+C\u2081 d\u2081\u2082+3+do+d\u2081 \u0435+\u0435\u0434+\u0435\u2081+\u0435\u2082+3\nGPU 1\na\u2081+a+a+a+ab+b\u2081+b3+b+b C3+C\u2082+C+C+C\u2081 d\u2081+d+do+d\u2081 eo+e+\u0435\u2081\n\u2193\nGPU 2\na+a+a+a+ab+b\u2081+by+b+bo C3+C\u2082+C+Co+C\u2081 d\u2081+da+do+d\u2081+d\u2082+e+\u0435\u2081+e\u2082\nGPU 3\na\u2081+a+a\u2082+a\nb\u2082+b\u2081+b3+b+b C3+C\u2082+C+C+C\u2081 d\u2081+d+do+d\u2081+\u2082+++\u0435\u2082+\u0435\nGPU 4\n\u2193\na\u2081+a+a+a+ab\u2082+b\u2081+b3+b4 C3+C\u2082+C4+C+C\u2081 d4+d+do+d\u2081+d\u2082 \u0435+e+\u0435\u2081+\u0435\u2082+\u04353\nGPU 2\na\u2081+a+a\u2082\nGPU 3\nb\u2082+by+by+b+b C3+C\u2082+C4+Co+C\u2081 d\u2082+d\u00b8+do+d\u2081+d\u2082 80+04+01+02\n\u041b\na\u2081+a+a\u2082+a3 b\u2082+by+b3 C3+C\u2082+C+Co+C\u2081 d\u2081+d+do+d\u2081+de+\u0435+\u0435\u2081+\u0435\u2082+\u04353\nGPU 4\na\u2081+a+a+a+ab\u2082+b\u2081+b3+b4 C3+C\u2082+C4 da+da+do+d+de+4+e\u2081+2+3\n38\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Ring All-Reduce: End of AllGather Step\nGPU 0\na\u2081+a+a+a+a b\u2082+b\u2081+b3+b+b0 +C\u2082+C4+C+C\u2081 d4+3+do+d\u2081+d\u2082++\u0435\u2081+\u0435\u2082+\u0435\nGPU 1\na\u2081+a+a2+3+4 b\u2082+b\u2081+b3+b+b0 C3+C\u2082+C+C+C\u2081| 4+3++++++\u0435\u2082+\u04353\nGPU 2\nGPU 3\na\u2081+a+2+3+\u0101 b\u2082+b\u2081+b3+b+b C3+C\u2082+C++ ++++++++++\nGPU 4\na\u2081+a+2+3+4 b\u2082+b\u2081+b3+b+b C3+C\u2082+C4+C+C\u2081 4+3++++++\u0435\u2082+\u04353\nFinal state after all allgather transfers\nHow many iterations in\nallgather step with N GPUs ?\n39\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Parameter Server (PS) vs Ring All-Reduce:\nCommunication Cost\n\u2022 P: number of processes N: total number of model parameters\n\u2022\nPS (centralized reduce)\n\u2022\nAmount of data sent to PS by (P-1) learner processes: N(P-1)\n\u2022\nAfter reduce, PS sends back updated parameters to each learner\n\u2022 Amount of data sent by PS to learners: N(P-1)\nTotal communication cost at PS process is proportional to 2N(P-1)\n\u2022 Ring All-Reduce (decentralized reduce)\n\u2022\n\u2022\nScatter-reduce: Each process sends N/R amount of data to (P-1) learners\n\u2022 Total amount sent (per process): N(P-1)/P\nAllGather: Each process again sends N/P amount of data to (P-1) learners\n\u2022 Total communication cost per process is 2N(P-1)/P\nPS communication cost is proportional to P whereas ring all-reduce cost is\npractically independent of P for large P (ratio (P-1)/P tends to 1 for large P)\n\u2022 Which scheme is more bandwidth efficient?\n\u2022 Note that both PS and Ring all-reduce involve synchronous parameter updates\n40\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-2-columbia-Fall2024.pdf": {
        "text": "All-Reduce applied to Deep Learning\n.\nBackpropagation computes gradients starting from the output layer\nand moving towards in the input layer\nGradients for output layers are available earlier than inner layers\n\u2022 Start all reduce on the output layer parameters while other gradients\nare being computed\nOverlay of communication and local compute\n41\n14\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Distributed Scaling Speed up -- Facebook\n\u2022 Demonstrated training of a ResNet-50 network in one hour on 256 GPUs\nMajor techniques:\n\u2022 Data parallelism\n\u2022\nVery large batch sizes\n\u2022\nLearning rate adjustment technique to deal with large batches\n\u2022 Gradual warm up of learning rate from low to high value in 5 epochs ?\n\u2022\n\u26ab MPI_AllReduce for communication between machines\nNCCL communication library between GPUs on a machine connected using NVLink\nGradient aggregation performed in parallel with backprop\n\u2022 No data dependency between gradients across layers?\n\u2022 As soon as the gradient for a layer is computed, it is aggregated across workers, while\ngradient computation for the next layer continues\n242\n42\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Imagenet Distributed Training\n\u2022\nEach server contains 8 NVIDIA Tesla P100 GPUs interconnected with NVIDIA\nNVLink\n32 servers => 256 P100 GPUs\n\u2022 50 Gbit Ethernet\n\u2022 Dataset: 1000-way ImageNet classification task; ~1.28 million training\nimages; 50,000 validation images; top- 1 error\n\u2022 ResNet-50\nkn How was this chosen? What is k and n?\nLearning rate: n = 0.1.\n256 Is it batch size per GPU (k) and total number of GPUs (n) ?\n\u2022 Mini-batch size per GPU: 32 (fixed, weak scaling across servers)\n\u2022\nCaffe2\n43\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Learning Rates for Large Minibatches\nLinear Scaling Rule: When the minibatch size is\nmultiplied by k, multiply the learning rate by k.\nWt+1 = Wt\nWt+k = Wt\n\u0175t+1 = Wt\n\u2015\n-\n1\nn.\nn\n1\n\u03b7\nn\n1\n\u03a3\u03bd\u03b9(\u03b1,\nVl(x, \nwt).\n\u0425\u0415\u0412\n\u03a3 \u03a3 Vl (x, Wt+j)\nj<k x\u0404Bj\n- \u03ae\u03bc \u03a3 \u03a3 VI(\u03b1, \u03c9\u03c2)\nkn\nj<k x\u0404Bj\nSuppose, Vl(x, wt) \u2248 Vl(x, Wt+j) for j < k.\nwhen will\n\u0175t+1 \u2248 Wt+k:\n44\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Training Error Vs Batch Size: Distributed Training\nkn=256, n= 0.1, 23.60%\u00b10.12\nkn= 4k, n= 1.6, 23.56%\u00b10.12\nkn=256, n= 0.1, 23.60%\u00b10.12\nkn= 8k, n= 3.2, 23.74%\u00b10.09\n16 servers\n32 servers\nBeyond 8k batch size, the training error\ndeteriorates\n0\n20\n40\n60\n80\n0\n20\n40\n60\n80\n100\n90\nkn=256, n= 0.1, 23.60%\u00b10.12\nkn=16k, n= 6.4, 24.79%\u00b10.27\n80\ntraining error %\nNUO\n60\n50\n40\n2% o\n70\n64 servers\n30\n20\n0\n20\n40\nepochs\n60\n60\nkn=256, n= 0.1, 23.60%\u00b10.12\nkn=32k, n=12.8, 27.55%\u00b10.28\n128 servers\n80\n0\n20\n40\n60\n80\n45\nepochs\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Runtime Scaling\ntime per iteration (secs)\n0.3\n0.28\n0.26\n0.24\n0.22\n4\n16\ntime per epoch (mins)\nimages/second\n32k\n16k\n-ideal\nactual\n8k\n\u0f5a\n4k\n2k\n26\n0.2\n256\n512\n1k\n2k\nmini-batch size\n4k\n0.5\n8k 11k\nFigure 7. Distributed synchronous SGD timing. Time per itera-\ntion (seconds) and time per ImageNet epoch (minutes) for training\nwith different minibatch sizes. The baseline (kn = 256) uses 8\nGPUs in a single server, while all other training runs distribute\ntraining over (kn/256) server. With 352 GPUs (44 servers) our\nimplementation completes one pass over all ~1.28 million Ima-\ngeNet training images in about 30 seconds.\n80\n16\n32\n64\n# GPUs\n128\n256 352\nFigure 8. Distributed synchronous SGD throughput. The small\noverhead when moving from a single server with 8 GPUs to multi-\nserver distributed training (Figure 7, blue curve) results in linear\nthroughput scaling that is marginally below ideal scaling (~90%\nefficiency). Most of the allreduce communication time is hid-\nden by pipelining allreduce operations with gradient computation.\nMoreover, this is achieved with commodity Ethernet hardware.\n46\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Questions\nWhy time per iteration increases little with increasing minibatch size?\n\u2022 Why time per epoch decreases with increasing batch size?\nWith 44 servers how much time it takes to finish 1 epoch of training?\n\u2022 Can you get throughput (images/sec) from time per epoch? Do you\nneed to know batch size?\n\u2022 Can you get throughput (images/sec) from time to process a mini-\nbatch (iteration)? Do you need to know batch size?\n.\n\u2022 If K-batch sync or K-sync (K < number of servers) was applied would\nthe convergence been faster? What about the final training error?\n47\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Distributed Deep Learning Benchmarking\nMethodology\n\u2022 Speedup\n\u2022 Scaling efficiency\n.\nAccuracy and end-to-end training time\n\u2022 Neural network\n.\nDeep learning framework\nGPU type\nCommunication overhead\nSpeedup (throughput) with n machines = n x Scaling efficiency with n machines\n48\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Scaling efficiency\n\u2022\nScaling efficiency: ratio between the run time of one iteration on a\nsingle GPU and the run time of one iteration when distributed over N\nGPUs. Why is this ratio a measure of scaling efficiency?\n\u2022 One can satisfy any given scaling efficiency for any neural network by\nincreasing the batch size and reducing communication overhead\nToo big a batch size will result in converging to an unacceptable\naccuracy or no convergence at all\n\u2022 A high scaling efficiency without being backed up by convergence to a\ngood accuracy and end to end training time is meaningless\n49\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Best practices when benchmarking\ndistributed deep learning systems\n\u2022\n\u2022\nSystems under comparison should train to same accuracy\n\u2022 Accuracy should be reported on sufficiently large test set\n\u2022\nCompute to communication ratio can vary widely for different neural\nnetworks. Using a neural network with high compute to communication\nratio can hide the ills of an inferior distributed Deep Learning system.\n\u2022 a sub-optimal communication algorithm or low bandwidth interconnect will not\nmatter that much\nComputation time for one Deep Learning iteration can vary by up to 50%\nwhen different Deep Learning frameworks are being used. This increases\nthe compute to communication ratio and gives the inferior distributed\nDeep Learning system an unfair uplift to the scaling efficiency.\n50\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Best practices when benchmarking\ndistributed deep learning systems\n\u2022 A slower GPU increases the compute to communication ratio and again\ngives the inferior distributed Deep Learning system an unfair uplift to the\nscaling efficiency.\n\u2022\n\u2022\n\u2022\nNvidia P100 GPUs are approximately 3X faster than Nvidia K40 GPUs.\nWhen evaluating the communication algorithm and the interconnect capability of a\nDeep Learning system, it is important to use a high performance GPU.\nCommunication overhead is the run time of one iteration when distributed\nover N GPUs minus the run time of one iteration on a single GPU.\n\u2022 Includes the communication latency and the time it takes to send the message\n(gradients) among the GPUs.\nCommunication overhead gives an indication of the quality of the communication\nalgorithm and the interconnect bandwidth.\n51\n52\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Imagenet1K/ResNet50 Training at Scale\nWork\nBatch size\nProcessor\nHe et al\n256\nDL Library Interconnect\nTesla P100 x8\nCaffe\nTraining\nTime\nTop-1\nAccuracy\n29 hrs\n75.3%\nScaling\nEfficiency\nGoyal et al\n8K\nTesla P100 x256\nCaffe2\n50 Gbit Ethernet\n60 mins\n76.3%\n~90%\n(Facebook)\nCho et al\n8K\nTesla P100 x256\nCaffe\nInfiniband\n50 mins\n75.01%\n95%\n(IBM)\nSmith et al\n8K 16K\nFull TPU Pod\nTensorflow\n30 mins\n76.1%\nAkiba et al\n32K\nTesla P100 x1024\nChainer\nInfiniband FDR\n15 mins\n74.9%\n80%\nJia et al\n64K\nTesla P40 x2048\nTensorflow\n100 Gbit Ethernet\n6.6 mins\n75.8%\n87.9%\nYing et al\n32K\nTPU v3 x1024\nTensorflow\nYing et al\n64K\nTPU v3 x1024\nTensorflow\nMikami et al\n54K\nTesla V100 x3456\nNNL\n2.2 mins\n76.3%\n1.8 mins\n75.2%\nInfiniband EDR X2\n2.0 mins\n75.29%\n84.75%\nCho et al achieved highest scaling efficiency; Goyal et al and Ying et al achieved highest accuracy\n52\n52\n",
        "links": []
    },
    "./chunks/chunk_53_Lecture-2-columbia-Fall2024.pdf": {
        "text": "2-D Torus Topology for inter-gpu\ncommunication\nmultiple rings in horizontal and vertical orientations.\nRING VO\nRING VX-1\nRING V1\nRING HO\nGPU(0,0)+\n(1,0)\n(X-1,0)\nRING H1\n(0,1)\n(1,1)\n(X-1,1)\nRING HY-1\n(0,Y-1)\n(X-1,Y-1)\n53\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-2-columbia-Fall2024.pdf": {
        "text": "2-D Torus all-reduce\n2D-Torus all-reduce steps of a 4-GPU cluster, arranged in 2x2 grid\nI. Reduce-Scatter in the horizontal direction\nII. All-Reduce in the vertical direction\nGPUO\nGPUT\nAdd\n1 2 3 4\n5 6 7 8\nGPUO\n6834\nGPUI\n5 6 10 12\nGPU2\nGPU\nAdd\nGPU2\nAdd\nGPU3\nAdd\n9 10 11 12\n13 14 15 16\n22 24 11 12\n13\n14 26 28\nIII. All-Gather in the horizontal direction\nIV. Completed\nGPUO\nCopy\n28 32 3 4\nGPUI\n56 36 40\nGPUO\nGPU1\n28 32 36 40\n28 32 36 40\nGPU2\nCopy\nGPU3\nGPU2\n28 32 11 12\n13 14 36 40\n28 32 36 40\n28\nGPU3\n32 36 40\nhttps://arxiv.org/pdf/1811.05233.pdf\n",
        "links": [
            "https://arxiv.org/pdf/1811.05233.pdf"
        ]
    },
    "./chunks/chunk_55_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Tensor Processing Units (TPUs)\n\u2022 TPU: application specific integrated circuits to\naccelerate machine learning workload\n\u2022 TPU pod: multiple TPU chips connected to each\nother over a dedicated high-speed network\nconnection\n\u2022\nhttps://cloud.google.com/tpu/docs/system-\narchitecture\nTPU v2 pod for ResNet-50: linearly scalable\nImages per second\n160000\n140000\n120000\n100000\n80000\n60000\n40000\n20000\n0\n0\n20\n40\n60\nNumber of TPU v2 chips\nObserved\n-- Perfect\nTPU v5e chip\nTensorCore\nScalar Unit\nVector Unit\nHigh\nBandwidth\nMemory\nMatrix\nMultiplication\nUnit\nMatrix\nMultiplication\nUnit\nMatrix\nMultiplication\nUnit\nMatrix\nMultiplication\nUnit\nA TensorCore has four matrix-multiply units\n(MXUS), a vector unit, and a scalar unit\n55\n",
        "links": [
            "https://cloud.google.com/tpu/docs/systemarchitecture"
        ]
    },
    "./chunks/chunk_56_Lecture-2-columbia-Fall2024.pdf": {
        "text": "TPUs vs GPUs Performance\n$100.00\nResNet-50 Training Cost Comparison\n$75.00\n8 V100 GPUs\ntraining duration:\n216 minutes for\n$50.00\n90 epochs\n$25.00\n$0.00\nGoogle Cloud VM with 8 V100 GPUs\n27 times faster training\nat 38% lower cost with TPUs\n1 full Cloud TPU v2\nPod training duration:\n7.9 minutes for\n256 TPUS v2 chips\n90 epochs\nFull Cloud TPU v2 Pod\n56\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-2-columbia-Fall2024.pdf": {
        "text": "TPU VM on Google Cloud\n\u2022 A TPU host is a VM that runs on\na physical computer connected\nto TPU hardware. TPU\nworkloads can use one or more\nhost.\n\u2022 Tutorial on using Google Cloud\nTPU VM: Google Cloud\nQuickstart\n1-Chip VM\nICI network\nCPUO\nChipo\nChip1\nChip2\nChip3\n8-Chip VM\nNUMA O\nTPU host in a v5e\nCPU1\n4-Chip VM\nPCle\nICI network\nChip4\nChip5\nChip6\nChip7\nNUMA 1\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-2-columbia-Fall2024.pdf": {
        "text": "NCCL\n\u2022 Nvidia Common Communications Library\n\u2022 NCCL implements optimized multi-GPU and multi-node\ncommunication primitives for NVIDIA gpus and networking\n.\nNCCL provides routines such as all-gather, all-reduce, broadcast,\nreduce, reduce-scatter as well as point-to-point send and receive\nCommunication primitive are optimized to achieve high bandwidth\nand low latency over PCle and NVLink high-speed interconnects\nwithin a node and over NVIDIA Mellanox Network across nodes\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Imagenet Large Scale Visual Recognition\n(ILSVRC) Challenge Winners\n30 28.2\n25.8\nFirst CNN-based winner\n152 layers 152 layers 152 layers\n25\n20\n20\n15\n16.4\nNetworks have gone deeper\n11.7\n19 layers\n22 layers\n10\n7.3\n6.7\n5.1\n5\nshallow\n8 layers\n8 layers\n3.6\n3\n2.3\n0\n2010\nLin et al\n2011\nSanchez &\nPerronnin\n2012\nKrizhevsky et al\n(AlexNet)\n2013\nZeiler &\nFergus\n2014\n2014\nSimonyan & Szegedy et al\nZisserman (VGG) (GoogleNet)\n2015\n2016\n2017\nHe et al\nShao et al\n(ResNet)\nHu et al\n(SENet)\nHuman\nRussakovsky et al\n59\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Receptive field\n\u2022\nA convolutional layer operates over a\nlocal region of the input to that layer\n\u2022 The effective receptive field of a\nconvolutional layer is the size of the\ninput region to the network that\ncontributes to a layers' activations\n\u2022 For example:\n\u2022\n\u2022\nif the first convolutional layer has a\nreceptive field of 3x3 then it's effective\nreceptive field is also 3x3\nHowever if the second layer also has a\n3x3 filter, then it's (local) receptive field is\n3x3, but it's effective receptive field is\n5x5\nEffective Receptive Field\nContributing input units to a convolutional filter.\nInput Features\n7 // 2 Convolution\nEach filter sees 7 input units\nConvolutional Features\n2 // 2 Max Pool\nEach filter sees 9 input units\nMax Pool Features\n3 // 1 Convolution\nEach filter sees 17 input units\nConvolutional Features\nHPML\n@jimmfleming // fomoro.com\nstrides continue...\nFeatures\nConv1D Filter\nPadding or Stride\nReceptive Field\n60\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Receptive Field in a 2-d CNN\nConvolution with kernel size k = 3x3, padding size p = 1x1,\nstrides = 2x2\nThe receptive field is defined as the\nregion in the input space that a\nparticular CNN's feature is looking\nat (i.e. be affected by)\nThe common way to visualize a CNN feature\nmap. Only looking at the feature map, we do\nnot know where a feature is looking at (the\ncenter location of its receptive field) and how\nbig is that region (its receptive field size). It\nwill be impossible to keep track of the\nreceptive field information in a deep CNN.\nApplying the\nconvolution on a 5x5\ninput map to produce\nthe 3x3 green feature\nmap\nApplying the same\nconvolution on top of the\ngreen feature map to\nproduce the 2x2 orange\nfeature map.\nhttps://medium.com/mlreview/a-guide-to-receptive-field-\narithmetic-for-convolutional-neural-networks-e0f514068807\n61\n52\n",
        "links": [
            "https://medium.com/mlreview/a-guide-to-receptive-fieldarithmetic-for-convolutional-neural-networks-e0f514068807"
        ]
    },
    "./chunks/chunk_62_Lecture-2-columbia-Fall2024.pdf": {
        "text": "VGG Architecture\nSoftmax\nFC 1000\nFC 4096\nSoftmax\nFC 1000\nFC 4096\nPool\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\nPool\n3x3 conv, 512\n3x3 conv, 512\nFC 4096\nFC 4096\nPool\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\nPool\nSoftmax\n3x3 conv, 512\n3x3 conv, 512\nFC 1000\n3x3 conv, 512\n3x3 conv, 512\nFC 4096\n3x3 conv, 512\nFC 4096\nPool\nPool\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 384\nPool\nPool\n3x3 conv, 128\n3x3 conv, 384\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\nPool\nPool\n5x5 conv, 256\n3x3 conv, 64\n11x11 conv, 96\n3x3 conv, 64\nInput\nInput\nAlexNet\nVGG16\nPool\n3x3 conv, 256\n3x3 conv, 256\nPool\nPool\n3x3 conv, 64\n3x3 conv, 64\nInput\nVGG19\nSmaller filters (3x3, stride 1) compared\nto 11x11 and 5x5 in Alexnet\nDeeper nets: more layers compared to\nAlexnet (8 vs 16 or 19)\nWhy smaller filters ?\n\u2022\nReceptive field of 3 3x3 stacked conv.\nlayers is same as a single 7x7 conv layer\n\u2022 More non-linearities with stack of smaller\nconv layers => makes decision function\nmore discriminative\nLesser number of parameters: 3 * (3\u00b2C\u00b2)\nvs. 72 C\u00b2 for C channels per layer (55% less)\n62\n",
        "links": []
    },
    "./chunks/chunk_63_Lecture-2-columbia-Fall2024.pdf": {
        "text": "GoogleNet (2014)\nFull GoogLeNet\narchitecture\n1-1-1-1-1-1\nDeeper network (22 layers)\nComputationally efficient \"Inception\" module\nAvoids expensive FC layers \u2192 uses average pooling\n12x less params than AlexNet\n1-1-1-1-1-1\n63\n",
        "links": []
    },
    "./chunks/chunk_64_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Inception module\n28x28x480\nFilter\nconcatenation\n28x28x128 - 28x28x192\n1x1 conv,\n128\n28x28x96\n3x3 conv,\n192\n5x5 conv,\n96\n28x28x64\n28x28x64\nI\n1x1 conv,\n64\nI\n1x1 conv,\n64\nModule input:\n28x28x256\nPrevious Layer\n28x28x64\n1x1 conv,\n64\n28x28x256\n3x3 pool\n\u2022\nUsing same parallel layers as naive example,\nand adding 1x1 conv with 64 filter bottlenecks:\n\u2022 Conv Ops:\n\u25cf\n\u2022\n\u2022\n\u2022\n\u2022 [1x1 conv, 64] 28x28x64x1x1x256\n[1x1 conv, 64] 28x28x64x1x1x256\n[1x1 conv, 128] 28x28x128x1x1x256\n[3x3 conv, 192] 28x28x192x3x3x64\n[5x5 conv, 96] 28x28x96x5x5x64\n[1x1 conv, 64] 28x28x64x1x1x256\nTotal: 358M ops\n\u2022\nCompared to 854M ops for naive version.\n\u2022 Bottleneck can also reduce depth after pooling\nlayer (480 compared to 672)\n64\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-2-columbia-Fall2024.pdf": {
        "text": "ResNet\nF(x) + x +\nrelu\nF(x)\nconv\n28x28x256\noutput\nSoftmax\nFC 1000\nPool\n3x3 conv. 64\n3x3 conv, 64\n3x3 conv. 64\n3x3 conv. 64\n1x1 conv, 256\n3x3 conv. 64\n3x3 conv. 64\nrelu\n\u2717\nidentity\n3x3 conv, 64\nconv\nResidual block\n1x1 conv, 64\n28x28x256\ninput\n3x3 conv. 128\n3x3 conv. 128\n3x3 conv. 128\n3x3 conv. 128\n3x3 conv. 128\n3x3 conv. 128/2\n3x3 conv. 64\n3x3 conv. 64\n\u2022\nStack of residual blocks: each residual block has 2 3x3 conv layers; number of filters is doubled\nperiodically\n3x3 conv, 64\n3x3 conv. 64\n\u2022\nGlobal average pooling layer after last conv layer\n3x3 conv. 64\n\u2022\nDifferent depths: 34, 50, 101, 152\n\u2022\nDeeper network (ResNet50+) add 1x1 conv for computational efficiency\n3x3 conv. 64\nPool\n7x7 conv. 64/2\nInput\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Residual connections prevents vanishing\ngradient\n\u2717\nweight layer\nF(x)\nrelu\nX\nweight layer\nidentity\nF(x) + x\n+\nrelu\n\u13a5\u13de\n\u018fL \u018fH\n\u018fL\u018fF\n(\n+1):\n=\ndx\n\u04d9\u043d \u04d8\u0445\n\u10dbH \u00b7 \u10db\u10d4\nOL OF aL\n+\n\u04d9\u043d \u04d8\u0445 \u04d9\u043d\n66\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-2-columbia-Fall2024.pdf": {
        "text": "The power of small filters\n\u2022\nSuppose input is H x W x C and we use convolutions with C filters to\npreserve depth (stride 1, padding to preserve H, W)\none CONV with 7 x 7 filters\nNumber of weights:\n= Cx (7 x 7 x C) = 49 C\u00b2\nthree CONV with 3 x 3 filters\nNumber of weights:\n= 3xCx (3 x 3 x C) = 27 C\u00b2\nFewer parameters, more nonlinearity\nNumber of multiply-adds:\n= (H x W x C) x (7 x 7 x C)\n= 49 HWC\u00b2\nNumber of multiply-adds:\n= 3x (H x W xC) x (3 x 3 x C)\n= 27 HWC\u00b2\nLess compute, more nonlinearity\nHPML\n67\n",
        "links": []
    },
    "./chunks/chunk_68_Lecture-2-columbia-Fall2024.pdf": {
        "text": "The power of small filters\nWhy stop at 3 x 3 filters? \u2192 use 1 x 1!\nH x W x C\nConv 1x1, C/2 filters\nH x W x (C/2)\nConv 3x3, C/2 filters\nH x W x (C/2)\nConv 1x1, C filters\n1. \"bottleneck\" 1 x 1 conv\nto reduce dimension\n2. 3 x 3 conv at reduced\ndimension\n3. Restore dimension\nwith another 1 x 1 conv\n[Seen in Lin et al, \"Network in\nNetwork\", GoogleNet, ResNet]\nHPML\n68\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-2-columbia-Fall2024.pdf": {
        "text": "The power of small filters\n\u2022\nStill using 3 x 3 filters ... can we break it up?\nHPML\nConv 3x3, C filters\n9 C\u00b2\nparameters\nH x W x C\nH x W x C\nConv 3x1, C filters\nH x W x C\nHE\nparameters\n6 C\u00b2\nH x W x C\nH x W x C\nConv 1x3, C filters\n69\n",
        "links": []
    },
    "./chunks/chunk_70_Lecture-2-columbia-Fall2024.pdf": {
        "text": "How to stack convolutions - Recap\n\u2022\nReplace large convolutions (5 x 5, 7 x 7) with stacks of 3 x 3\nconvolutions\n1 x 1 \"bottleneck\" convolutions are very efficient\nReplace N x N convolutions into 1 x N and N x 1\nAll of the above give fewer parameters, less compute, more\nnonlinearity\nSzegedy et al, \"Rethinking the Inception Architecture for Computer\nVision\"\nHPML\n10\n70\n",
        "links": []
    },
    "./chunks/chunk_71_Lecture-2-columbia-Fall2024.pdf": {
        "text": "-1 accuracy [%]\nTop-1\n80\n75\n70\n65\n60\n55\nTop1 Accuracy Comparison\nInception-v4: Resnet + Inception!\n50\nAlexNet\nBN\n-\nAlexNet\nBN-NIN\nENet\nGoogLeNet\nResNet-18\nVGG-16\nVGG\n-19\nResNet-34\nTop1 vs. network.\nTop-1\nacy [%]\n-1 accuracy\n80\n75\n70\n65\nInception-v3\nResNet-50\nResNet-101\nResNet-34\nResNet-18\nGoogLeNet\nENet\nInception-v4\nResNet-152\nVGG-16\nVGG-19\nBN-NIN\n60\n5M\n35M\n65M -95M\n125M -155M\n55\nBN-AlexNet\nAlexNet\n50\n0\n5\n10\n15\n20\n25\nOperations [G-Ops]\n30\n35\n40\nTop1 vs. operations, size x parameters\n71\n",
        "links": []
    },
    "./chunks/chunk_72_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Maximum net memory utilisation [MB]\nMemory Consumption\nMemory vs. batch size.\nBN-NIN\nVGG-19\n2000\nGoogLeNet\nResNet-18\nInception-v3\nResNet-34\nAlexNet\nResNet-50\nBN-AlexNet\nResNet-101\nVGG-16\n1000\n500\n300\n200\n1\n2\n4\n8\n16\n32\n64\nBatch size [/ ]\nMaximum net memory utilisation [MB]\n800\n700\n600\n500\n400\n300\n200\nMemory vs. parameters count\nBatch of 1 image\n1.30\n100\n0\n100\n200\n300\n400\n500\nParameters [MB]\n72\n12\n",
        "links": []
    },
    "./chunks/chunk_73_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Recurrent Neural Networks\nht\n100 dims\nYt\n0/1?\n\u2022\nRecurrent Neural Networks are networks with loops,\nallowing information to persist.\n(ht\nA\nOutput is to predict a vector ht,\nwhere output y\u2081 = p(ht) at some\ntime steps (t)\nXt\nA\nFully\nRecurrent cell\nconnected\nlayer\nWord embeddings\n32 dims\nReview\nmovie fantastic, awesome...\nXt\nRecurrent Neural Networks have loops.\nIn the above diagram, a chunk of neural\nnetwork, A = fw, looks at some\ninput Xt and outputs a value ht. A loop\nallows information to be passed from\none step of the network to the next.\nnew state\nold state\nht= fw (ht-1xt\nfunction with\nparameter W\nInput vector at\nsome time step\n73\n",
        "links": []
    },
    "./chunks/chunk_74_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Recurrent Neural Networks Features\n\u2022 The recurrent structure of RNNs enables the following characteristics:\nSpecialized for processing a sequence of values x (1), ..., x(t)\n\u2022\nEach value x(i) is processed with the same network A that preserves past information\n\u2022 Can scale to much longer sequences than would be practical for networks\nwithout a recurrent structure\n\u2022\nReusing network A reduces the required amount of parameters in the network\n\u2022 Can process variable-length sequences\n\u2022\n\u2022\nThe network complexity does not vary when the input length change\nHowever, vanilla RNNs suffer from the training difficulty due to exploding and\nvanishing gradients.\n74\n",
        "links": []
    },
    "./chunks/chunk_75_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Pros and Cons of using RNNs\n\u2022 Pros:\n\u2022\n.\n\u2022\n\u2022\nPossibility of processing input of any length\nModel size not increasing with size of input\nComputation takes into account historical information\nWeights are shared across time\n\u2022 Cons:\n\u2022\n\u2022\nComputation being slow\nDifficulty of accessing information from a long time ago.\n\u2022 Cannot consider any future input for the current state\n\u2022\nSuffers from vanishing and exploding gradient problem and hence training is\nunstable\n",
        "links": []
    },
    "./chunks/chunk_76_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Networks with Memory\n\u2022 Vanilla RNN operates in a \"multiplicative\" way (repeated\ntanh).\nTwo recurrent cell designs were proposed and widely\nadopted:\nLong Short-Term Memory (LSTM) (Hochreiter and\nSchmidhuber, 1997)\n\u2022\nGated Recurrent Unit (GRU) (Cho et al. 2014)\nCt-1\nOt\ntanh\nht\nJari\nht-1\n\u03a7\u03b5\u03b9\nht-1\ntanh\nStandard LSTM Cell\nht\n\u03c3\ntanh\nXt\nGRU Cell\nCt\nht\n\u2022\nBoth designs process information in an \"additive\" way\nwith gates to control information flow.\n\u2022\nSigmoid gate outputs numbers between 0 and 1, describing\nhow much of each component should be let through.\nE.g. ft=\u03c3(W[ht\u22121,xt] + bf) = Sigmoid (W\u2081x, + U\u2081h\u2081-1 + bf)\nNeural Network\nLayer\n\u03c3\nA Sigmoid Gate\nPointwise\nOperation\nVector\nTransfer\nConcatenate\nCopy\n",
        "links": []
    },
    "./chunks/chunk_77_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Long Short-Term Memory Networks (LSTM)\n(ht)\n(nt-1)\nA\ntanh\n0\n\u03c3\ntanh\n\u039f\ntanh\n\u03c3\nA\nt+1)\n4 gates\n4 fully connected layers\n4 activation functions\n4 math operations\n(Xt-1)\nXt\n(Xt+1)\nNeural Network\nLayer\nPointwise\nOperation\nVector\nTransfer\nConcatenate\nCopy\n",
        "links": []
    },
    "./chunks/chunk_78_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Parameter calculation\n\u2022\nInput to all FC networks is a concatenated vector of current input and\nhidden state vector\n[ht\u22121,xt]\nTo calculate number of parameters. Each FC network has a parameter weight\nmatrix of [(m+n),n] and a bias values of 'n'. So total parameters at each FC\nnetwork (m*n + sqr(n) + n) and for four FC networks it will be 4*(m*n + sqr(n)\n+n).\n",
        "links": []
    },
    "./chunks/chunk_79_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Sequence Learning with Multiple RNN\nLayers\nBidirectional RNN\n\u2022 Connects two recurrent units (synced\nmany-to-many model) of opposite\n\u2022\ndirections to the same output.\nCaptures forward and backward\ninformation from the input sequence\nApply to data whose current state (e.g.,\nho) can be better determined when given\nfuture information (e.g., X1, X2, ..., xt)\n\u2022\nE.g., in the sentence \u201cthe bank is robbed,\" the\nsemantics of \"bank\" can be determined given\nthe verb \"robbed.\"\nXt\nX1)\nX2\nBBB B\n(ho)\n(h\u2081)\n(h\u2082)\n(ht\nA A A\n(X)\nX1\nX2)\nB\nA\nXt\n79\n12\n",
        "links": []
    },
    "./chunks/chunk_80_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Sequence Learning with Multiple RNN\nLayers\n\u2022 Sequence-to-Sequence (Seq2Seq) model\n\u2022 Developed by Google in 2018 for use in machine translation.\n\u2022\n\u2022\nSeq2seq turns one sequence into another sequence. It does so by use of a recurrent\nneural network (RNN) or more often LSTM or GRU to avoid the problem of vanishing\ngradient.\nThe primary components are one Encoder and one Decoder network. The encoder\nturns each item into a corresponding hidden vector containing the item and its\ncontext. The decoder reverses the process, turning the vector into an output item,\nusing the previous output as the input context.\n\u2022 Encoder RNN: extract and compress the\nsemantics from the input sequence\nDecoder RNN: generate a sequence based\n\u2022\n\u2022\non the input semantics\nApply to tasks such as machine\ntranslation\n\u2022\n\u2022\nSimilar underlying semantics\n\u2022E.g., \"I love you.\" to \"Je t'aime.\"\nDecoded sequence Yo\nEncoded semantics\nAn RNN as the encoder\nA A A\n\u04231\nY2\nYm\nBBB B\n\u2191\nAn RNN as the decoder\nA\n\u03a7\u03bf\nX1\nX2\nXt\n\u2190 Input sequence 80\n",
        "links": []
    },
    "./chunks/chunk_81_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Generative vs Discriminative Models\n\u2022 Generative models can generate new data\ninstances.\n\u2022 Discriminative models discriminate between\ndifferent kinds of data instances.\n\u2022 Generative models capture the joint\nprobability p(X, Y), or just p(X) if there are no\nlabels.\n\u2022 Discriminative models capture the\nconditional probability p(Y | X).\n\u2022 Discriminative models try to draw\nboundaries in the data space, while\ngenerative models try to model how data is\nplaced throughout the space.\n\u2022\nDiscriminative Model\np(y|x)\nI\n0\ny=0\ny=1\nI\nD\n0\nGenerative Model\np(x, y)\n- y = 0\ny = 1\n",
        "links": []
    },
    "./chunks/chunk_82_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Discriminator\nloss\nloss\nGenerator\nGenerative Adversarial Networks (GANs)\n\u2022 Generator learns to\ngenerate plausible data\n\u2022 Generated instances are\nnegative training examples\nfor the discriminator\n\u2022 Discriminator learns to\ndistinguish the generator's\nfake data from real data.\nRandom input\nReal images\nSample\nGenerator\nSample\nDiscriminator\n",
        "links": []
    },
    "./chunks/chunk_83_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Discriminator Training\nDiscriminator is a classifier\n\u2022\nDistinguishes real data from data created by the\ngenerator\nDiscriminator training:\n\u2022\n\u2022\n\u2022\nClassifies both real data and fake data from the\ngenerator\nDiscriminator loss penalizes the discriminator for\nmisclassifying a real instance as fake or a fake\ninstance as real.\nDiscriminator updates its weights\nthrough backpropagation from the discriminator\nloss through the discriminator network.\nRandom input\nReal images\nSample\nDiscriminator\nGenerator\nSample\nBackpropagation\nDiscriminator\nGenerator\nloss\nloss\n",
        "links": []
    },
    "./chunks/chunk_84_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Generator Training\n\u2022\nGenerator is not directly connected to the loss\nGenerator feeds into the discriminator which\nthen produces the output\n\u2022 Generator loss penalizes the generator for\nproducing a sample that the discriminator\nnetwork classifies as fake\n.\nWe don't want the discriminator to change\nduring generator training (moving target)\nRandom input\nReal images\nSample\nGenerator\nSample\nDiscriminator\nloss\nDiscriminator\nloss\nGenerator\nBackpropagation\n",
        "links": []
    },
    "./chunks/chunk_85_Lecture-2-columbia-Fall2024.pdf": {
        "text": "GAN Training\nAlternating training of discriminator and generator\n1. The discriminator trains for one or more epochs.\n2.\nThe generator trains for one or more epochs.\n3. Repeat steps 1 and 2 to continue to train the generator and discriminator\nnetworks.\nConvergence\n\u2022 As training progresses discriminator performance worsens\n\u2022\nEventually discriminator starts making random guesses\n\u2022 Generator get junk feedback\n",
        "links": []
    },
    "./chunks/chunk_86_Lecture-2-columbia-Fall2024.pdf": {
        "text": "GAN Loss functions\n\u2022 Minimax loss\nEx[log(D(x))] + Ez[log(1 \u2212 D(G(2)))]\n\u2022 D(x) is the discriminator's estimate of the probability that real data instance x is real.\n\u2022\n\u2022\nEx is the expected value over all real data instances.\nG(z) is the generator's output when given noise z.\n\u2022\nD(G(z)) is the discriminator's estimate of the probability that a fake instance is real.\n\u2022\nEz is the expected value over all random inputs to the generator (in effect, the expected value over all generated\nfake instances G(z)).\n\u2022 The formula derives from the cross-entropy between the real and generated distributions.\n",
        "links": []
    },
    "./chunks/chunk_87_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Min-max game\nD and G play the following two-player minimax game with value function V(G, D):\nX-\nmin max V (D, G) = Ex~pdata (x) [log D(x)] + Ez~pz(z) [log(1 \u2013 D(G(z)))].\nG D\nA\nA\n2.\n(a)\n(b)\n(c)\n(d)\n",
        "links": []
    },
    "./chunks/chunk_88_Lecture-2-columbia-Fall2024.pdf": {
        "text": "GAN Training as Stochastic Gradient Descent\nAlgorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets. The number of\nsteps to apply to the discriminator, k, is a hyperparameter. We used k\nexperiments.\nfor number of training iterations do\nfor k steps do\n=\n1, the least expensive option, in our\n\u2022 Sample minibatch of m noise samples {z (\u00b9),..., 2 (m)} from noise prior pg(z).\n\u2022 Sample minibatch of m examples {x(1), x (m)} from data generating distribution\nPdata (x).\n90009\n\u2022 Update the discriminator by ascending its stochastic gradient:\nm\nm\n\u03a3 log D (x(\")) + log (1 - D (G (\u2082\")))] .\ni=1\nend for\n\u2022 Sample minibatch of m noise samples {z(\u00b9),\n\u2022 Update the generator by descending its stochastic gradient:\nz(m)} from noise prior p,(z).\nend for\nm\n097\n\u03a3 log (1-D (G (2()))).\nM\ni=1\nThe gradient-based updates can use any standard gradient-based learning rule. We used momen-\ntum in our experiments.\n",
        "links": []
    },
    "./chunks/chunk_89_Lecture-2-columbia-Fall2024.pdf": {
        "text": "Prepare for Lecture 3\n.\nGet ready with your cloud setup\nHW1 (based on material in Lecture 1 and 2) will be released soon\n\u2022 Start working on your project proposal\nLecture 3 will cover ML on cloud platforms, TorchX, Ray, and\nscheduling on DL clusters\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-9-Columbia.pdf": {
        "text": "+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\nLecture 9\nParijat Dube, Chen Wang\n+\n1\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-9-Columbia.pdf": {
        "text": "Agenda\n\u2022 Pretrained Model Selection and\nModel Pretraining\nQuantization\nEfficient Mult-GPU Compute\nStrategies\nScaling Law\nBloombergGPT\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-9-Columbia.pdf": {
        "text": "Generative Al project lifecycle\nScope\nSelect\nAdapt and align model\nApplication integration\nPrompt\nengineering\nChoose an\nexisting\nDefine the\nuse case\nmodel or\npretrain\nFine-tuning\nEvaluate\nyour own\nOptimize\nand deploy\nmodel for\ninference\nAugment\nmodel and\nbuild LLM-\npowered\napplications\nAlign with\nhuman\nfeedback\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-9-Columbia.pdf": {
        "text": "Generative Al project lifecycle\nScope\nSelect\nAdapt and align model\nApplication integration\nPrompt\nengineering\nDefine the\nuse case\nChoose an\nexisting\nmodel or\npretrain\nyour own\nFine-tuning\nEvaluate\nAlign with\nhuman\nfeedback\nOptimize\nand deploy\nmodel for\ninference\nAugment\nmodel and\nbuild LLM-\npowered\napplications\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-9-Columbia.pdf": {
        "text": "Considerations for choosing a model\nFoundation model\nTrain your own model\nPretrained\nLLM\nCustom\nLLM\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-9-Columbia.pdf": {
        "text": "Model Hubs\nModel Card for T5 Large\n\"translate English to German: That is good.\"\n\"cola sentence: The\ncourse is jumping well.\"\n\"stsb sentence1: The rhino grazed\non the grass. sentence2: A rhino\nis grazing in a field.\"\n\"summarize: state authorities\ndispatched emergency crews tuesday to\nsurvey the damage after an onslaught\nof severe weather in mississippi..\"\nT5\n\"Das ist gut.\"\n\"not acceptable\"\n\"3.8\"\n\"six people hospitalized after\na storm in attala county.\"\nTable of Contents\n1. Model Details\n2. Uses\n3. Bias, Risks, and Limitations\n4. Training Details\n5. Evaluation\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-9-Columbia.pdf": {
        "text": "How Large Language Models are trained?\nFoundation model\nTrain your own model\nPretrained\nLLM\nCustom\nLLM\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-9-Columbia.pdf": {
        "text": "Model architectures and pre-\ntraining objectives\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-9-Columbia.pdf": {
        "text": "LLM pre-training at a high level\nModel\nLLM\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-9-Columbia.pdf": {
        "text": "LLM pre-training at a high level\nTEXT [...]\nTEXT [...]\nTEXT [...]\nTEXT [...]\nTEXT [...]\nTEXT [...]\nTEXT [...]\nTEXT [...]\nTEXT [. ]\nTEXT [...]\nToken String\nToken\nID\nEmbedding/\nVector Representation\nModel\n'The'\n37\n[-0.0513, -0.0584,\n0.0230, ...]\n'_teacher' 3145\n[-0.0335, 0.0167,\n0.0484, ...]\nLLM\n'teaches' 11749\n[-0.0151, -0.0516,\n0.0309,\n'the'\n8\n[-0.0498, -0.0428,\n0.0275, ...]\n'_student'\n1236\n[-0.0460, 0.0031,\nGPU\nGPU\n0.0545, ...]\nIIIII\nGB-TB-PB\nof unstructured data\nThe model internalizes the patterns and\nstructures present in the language.\nVocabulary\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-9-Columbia.pdf": {
        "text": "LLM pre-training at high level\nTEXT [...]\nTEXT[...]\nTEXT[...]\nTEXT[...]\nTEXT [...]\nTEXT[...]\nTEXT [...]\nTEXT[...]\nTEXT[...]\nTEXT[...]\nData Quality Filter\nToken String\nToken\nID\nModel\nEmbedding/\nVector Representation\nThe'\n37\n[-0.0513, -0.0584,\n0.0230, ...]\n'_teacher' 3145\n[-0.0335, 0.0167,\n0.0484, ...]\nTEXT [...]\nLLM\n'teaches' 11749\n[-0.0151, -0.0516,\n0.0309, ...]\n'the'\n8\n[-0.0498, -0.0428,\n0.0275, ...]\nIIIII\nGPU\nGPU\nIIIII\n'_student'\n1236\n[-0.0460, 0.0031,\n0.0545, ...]\nGB - TB - PB\nof unstructured data\nVocabulary\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-9-Columbia.pdf": {
        "text": "LLM pre-training at high level\nTEXT [...]\nTEXT [...]\nTEXT[...]\nTEXT[...]\nTEXT [...]\nTEXT [...]\nTEXT [.\nTEXT [...]\nTEXT [...]\nTEXT [...]\n1-3% of\noriginal\ntokens\nToken String\nToken\nEmbedding/\nID\nVector Representation\nModel\n'The'\n37\n[-0.0513, -0.0584,\n0.0230, ...]\n'_teacher'\n3145\n[-0.0335, 0.0167,\nTEXT [...]\n0.0484, ...]\nLLM\nteaches'\n11749\n[-0.0151, -0.0516,\n0.0309, ...]\n'the'\n8\n[-0.0498, -0.0428,\n0.0275, ...]\nIIIII\nGPU\nIIIII\nIIIII\nGPU\n'_student'\n1236\n[-0.0460, 0.0031,\n0.0545, ...]\nData Quality Filter\nGB - TB - PB\nof unstructured data\nVocabulary\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-9-Columbia.pdf": {
        "text": "Transformers\nOutput\nOutput\nOutput\nEncoder Only\nModels\nEncoder Decoder\nDecoder Only\nModels\nModels\nInput\nInputs\nInput\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-9-Columbia.pdf": {
        "text": "Autoencoding models\nMasked Language Modeling (MLM)\nOriginal text\nThe teacher\nteaches the\nstudent.\n[...]\nThe teacher <MASK> the\nstudent\nEncoder-only\nLLM\nObjective: Reconstruct text (\"denoising\")\nThe\nteacher teaches\nthe\nstudent\nBidirectional context\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-9-Columbia.pdf": {
        "text": "Autoencoding models\n\u2022 Good use cases\n.\nSentiment Analysis\n\u2022 Named entity recognition\n\u2022 Word classification\nExample models:\n. BERT\n\u25cf\nROBERTA\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-9-Columbia.pdf": {
        "text": "Autoregressive Models\nCausal Language Modeling (CLM)\nOriginal text\nThe teacher\nteaches the\nstudent.\n[...]\nThe\n?\nDecoder-only\nLLM\nObjective: Predict Next Token\nThe teacher teaches\nFull Language Modeling\nUnidirectional context\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-9-Columbia.pdf": {
        "text": "Autoregressive models\n\u25cf\nGood use cases:\n\u00b7\nText generation\n\u25cf\nOther emergent behavior\n\u2022\nDepends on model size\nExample models:\n\u2022 GPT\n.\n\u2022 BLOOM\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-9-Columbia.pdf": {
        "text": "Sequence-to-sequence models\nSpan Corruption\nOriginal text\nThe teacher\nteaches the\nstudent.\n[...]\nThe teacher\n<MASK>\n<MASK>\nstudent\nThe teacher\nstudent\nSentinel Token\nEncoder-Decoder\nLLM\nObjective: Reconstruct span\n<x> teaches\nthe\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-9-Columbia.pdf": {
        "text": "Sequence-to-sequence models\n.\nGood use cases:\n.\n\u2022\nTranslation\nText Summarization\n\u2022 Question answering\nExample models\n\u25cf\nT5\n. BART\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-9-Columbia.pdf": {
        "text": "Model architectures and pre-training objectives\nOriginal text\nThe teacher\nteaches the\nstudent\n[...]\nTarget\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-9-Columbia.pdf": {
        "text": "Model architectures and pre-training objectives\nAutoencoding: MLM\nEncoder-only\nTarget\nThe teacher\nLLM\nteaches the\nOriginal text\nThe teacher\nteaches the\nstudent\n[...]\nThe teacher\n<MASK> the\nstudent\nstudent\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-9-Columbia.pdf": {
        "text": "Model architectures and pre-training objectives\nOriginal text\nThe teacher\nteaches the\nstudent\n[...]\nAutoencoding: MLM\nThe teacher\n<MASK> the\nstudent\nEncoder-only\nLLM\nTarget\nThe teacher\nteaches the\nstudent\nDecoder-only\nAutoregressive: CLM\nThe teacher\nThe teacher ?\nLLM\nteaches\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-9-Columbia.pdf": {
        "text": "Model architectures and pre-training objectives\nAutoencoding: MLM\nEncoder-only\nOriginal text\nThe teacher\nteaches the\nstudent\n[...]\nThe teacher\n<MASK> the\nstudent\nTarget\nLLM\nThe teacher\nteaches the\nstudent\nDecoder-only\nAutoregressive: CLM\nThe teacher ?\nLLM\nThe teacher\nteaches\nSeq-to-Seq: Span corruption Encoder-Decoder\nThe teacher\n<X> student\nLLM\n<X> teaches the\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-9-Columbia.pdf": {
        "text": "Question?\nLarge Language Models (LLMs) are capable of performing multiple tasks\nsupporting a variety of use cases. Which of the following tasks supports\nthe use case of converting code comments into executable code?\no Translation\no Invoke actions from text\no Information Retrieval\no Text summarization\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-9-Columbia.pdf": {
        "text": "Question?\nWhich transformer-based model architecture is well-suited to the task\nof text translation?\nAutoencoder\nSequence-to-sequence\nAutoregressive\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-9-Columbia.pdf": {
        "text": "The significance of scale: task ability\nBERT*\n110M\nBLOOM\n176B\n*Bert-base\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-9-Columbia.pdf": {
        "text": "Model size vs. time\nBERT-L\n340M\nGPT-2\n1.5B\nGPT-3\n175B\nPALM\n2018\n540B\nGrowth powered by:\nIntroduction of transformer\n\u2022 Access to massive\ndatasets\nMore powerful compute\nresources\n2022\n2023\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-9-Columbia.pdf": {
        "text": "Model size vs. time\nBERT-L\n340M\nGPT-2\n1.5B\nGPT-3\n175B\nPALM\n540B\n2018\nincrease?\nTrillion(s)\n2022\n2023\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-9-Columbia.pdf": {
        "text": "Computational Challenges\nOutOfMemoryError: CUDA out of memory.\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-9-Columbia.pdf": {
        "text": "Approximate GPU RAM needed to store 1B parameters\n1 parameter = 4 bytes (32-bit float)\n1B parameters = 4 \u00d7 109 bytes =4 GB\n4GB @ 32-bit\nFull precision\nReferences: https://github.com/bitsandbytes-foundation/bitsandbytes\nhttps://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n",
        "links": [
            "https://github.com/bitsandbytes-foundation/bitsandbytes",
            "https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory"
        ]
    },
    "./chunks/chunk_31_Lecture-9-Columbia.pdf": {
        "text": "Additional GPU RAM needed to train 1B parameters\n~\n20 extra bytes\nper parameter\nReferences: https://github.com/bitsandbytes-foundation/bitsandbytes\nhttps://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory\n",
        "links": [
            "https://github.com/bitsandbytes-foundation/bitsandbytes",
            "https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#anatomy-of-models-memory"
        ]
    },
    "./chunks/chunk_32_Lecture-9-Columbia.pdf": {
        "text": "Approximate GPU RAM needed to train 1B-params\nMemory needed to store model\nMemory needed to train model\n4GB@32-bit\nfull precision\n24 GB @ 32-bit\nfull precision\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-9-Columbia.pdf": {
        "text": "Quantization\nMIN\n-3e38\n?\n0.0\n?\nMAX\n+3e38 FP32\nRange:\n32-bit floating point\nFrom -3e38 to +3e38\nFP16 | BFLOAT16 | INT8\n16-bit floating point | 8-bit integer\n0\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-9-Columbia.pdf": {
        "text": "MIN\n-3e38\nQuantization: FP32\n0.0\n*\n3.1415920257568359375\nReal value of Pi:\n3.14159265358979323384\n|\n|\n|\n|\n0\nLet's store Pi: 3.141592\nMAX\n+3e38\n0\n10000000\nFP32 4 bytes memory\n10010010000111111011000\n\ub098\nSign\nExponent\nFraction\n1 bit\n8 bits\n23 bits\nMantissa/Significand\n= = Precision\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-9-Columbia.pdf": {
        "text": "MIN\n-3e38\nQuantization: FP16\n0.0\nLet's store Pi: 3.141592\nMAX\nFP32 4 bytes memory\n+3e38\n*\n0\n10000000\n10010010000111111011000\n3.1415920257568359375\nMIN\n-65504\n|\nI\n|\nI 3.140625\n|\n0\nSign\n\ub098\nExponent\n1 bit\n8 bits\nFP16 2 bytes memory\n1001001000\nFraction\n23 bits\n0\n10000\nMAX\n65504\nSign Exponent\nFraction\n1 bit 5 bits\n10 bits\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-9-Columbia.pdf": {
        "text": "Quantization: BFLOAT16\nLet's store Pi: 3.141592\nMIN\n0.0\nMAX\n-3e38\n+3038\nFP32 4 bytes memory\n0 10000000\n3.1415920257568359375\n3.140625\n0\nSign\n1 bit\nExponent\n8 bits\n10010010000111111011000\nFraction\n23 bits\nBFLOAT 16 | BF16\n2 bytes memory\n0\n10000000\n1001001\nSign\n1 bit\nExponent\n8 bits\nFraction\n7 bits\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-9-Columbia.pdf": {
        "text": "Quantization: BFLOAT16\nLet's store Pi: 3.141592\nMIN\n-3e38\n0.0\nMAX\n+3e38\nFP32 4 bytes memory\nX\n0\n10000000\n10010010000111111011000\n3.1415920257568359375\nSign\nExponent\n1 bit\n8 bits\nFraction\n23 bits\n3.140625\nBFLOAT 16 | BF16\n2 bytes memory\n0\n10000000\n1001001\n0\nMIN\n~ -3e38\nMAX\n\"Truncated FP32\"\n~ +3e38\nSign\nExponent\nFraction\n1 bit\n8 bits\n7 bits\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-9-Columbia.pdf": {
        "text": "Quantization: BFLOAT16\nLet's store Pi: 3.141592\nMIN\n0.0\nMAX\n-3038\n+3e38\nFP32 4 bytes memory\n0 10000000\n10010010000111111011000\n3.1415920257568359375\n3.140625\nMIN\n~ -3e38\n0\n\"Truncated FP32\"\nSign\n1 bit\nExponent\n8 bits\nMAX\n~ +3e38\n2\nBFLOAT16 | BF16\n0 10000000\nSign\nExponent\n1 bit\n8 bits\nFraction\n23 bits\n2 bytes memory\n1001001\nFraction\n7 bits\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-9-Columbia.pdf": {
        "text": "Quantization: INT8\nLet's store Pi: 3.141592\nMIN\n-3e38\n0.0\nMAX\n+3e38\nFP32\n4 bytes memory\n0\n10000000\n10010010000111111011000\nMIN\n-128\n3.1415920257568359375\n3\nSign\nExponent\n1 bit\n8 bits\nINT8 1 byte memory\n0\nMAX\n127\nSign\n1 bit\n0000011\nFraction\n23 bits\nFraction\n7 bits\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-9-Columbia.pdf": {
        "text": "Quantization: Summary\nBits\nExponent\nFraction\nMemory needed\nto store one value\nFP32\n32\n8\n23\n4 bytes\nFP16\n16\n5\n10\n2 bytes\nBFLOAT16\n16\n8\n7\n2 bytes\nFLAN\nT5\nINT8\n8\n-/-\n7\n1 byte\n\u2022 Reduce required memory to store and train models.\n\u25cf\n\u25cf\n\u25cf\nStatistically projects 32-bit floating point numbers into lower precision spaces.\nQuantization-aware training (QAT) learns the quantization scaling factors during\ntraining.\nBFLOAT16 is a popular choice.\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-9-Columbia.pdf": {
        "text": "Approximate GPU RAM needed to store 1B\nparameters\nFull-\nprecision\nmodel\n4GB @ 32-bit\nfull precision\n16-bit\nquantized\nmodel\n2GB @ 16-bit\nhalf precision\n8-bit\nquantized\nmodel\n1GB @ 8-bit\nprecision\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-9-Columbia.pdf": {
        "text": "GPU RAM needed to train larger models\n1B param\nmodel\n175B param\nmodel\n4,200 GB @ 32-bit\nfull precision\n500B param\nmodel\n12,000 GB @ 32-bit\nfull precision\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-9-Columbia.pdf": {
        "text": "GPU RAM needed to train larger models\nAs model sizes get larger, you will\nneed to split your model across\nmultiple GPUs for training\n500B param\nmodel\n1B param\nmodel\n4,200 GB @ 32-bit\nfull precision\n175B param\nmodel\n12,000 GB @ 32-bit\nfull precision\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-9-Columbia.pdf": {
        "text": "Efficient Multi-GPU\nCompute Strategies\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-9-Columbia.pdf": {
        "text": "When to use distributed compute\nLLM\nGPU\nLLM\nGPU\nModel too big for single GPU\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nDataloader\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nDataloader\nGPU 3\nGPU 2\nGPU 1\nGPU O\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nDataloader\nLLM\nGPU 3\nLLM\nGPU 2\nLLM\nGPU 1\n(LLM\nGPU O\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nLLM\nDataloader\nGPU 3\nLLM\nGPU 2\nLLM\nGPU 1\nLLM\nGPU O\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nLLM\nForward/\nDataloader\nGPU 3\nBackward pass\nLLM\nForward/\nGPU 2\nBackward pass\nLLM\nForward/\nGPU 1\nBackward pass\n(LLM)\nForward/\nGPU O\nBackward pass\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nSynchronize\nLLM\nForward/\nDataloader\nGPU 3\nBackward pass\nLLM\nGPU 2\nForward/\nBackward pass\nSynchronize\ngradients\nLLM\nForward/\nGPU 1\nBackward pass\nLLM\nForward/\nGPU O\nBackward pass\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nSynchronize\nLLM\nForward/\nUpdate\nDataloader\nGPU 3\nBackward pass\nModel\nLLM\nForward/\nUpdate\nGPU 2\nBackward pass\nModel\nSynchronize\ngradients\n(LLM)\nForward/\nUpdate\nGPU 1\nBackward pass\nModel\nLLM\nForward/\nGPU O\nBackward pass\nUpdate\nModel\n",
        "links": []
    },
    "./chunks/chunk_53_Lecture-9-Columbia.pdf": {
        "text": "Fully Sharded Data Parallel (FSDP)\nMotivated by the \"ZERO\" paper - zero data overlap between GPUs\nZeRO: Memory Optimizations Toward Training\nTrillion Parameter Models\nSamyam Rajbhandari*, Jeff Rasley*, Olatunji Ruwase, Yuxiong He\n{samyamr, jerasley, olruwase, yuxhe}@microsoft.com\nABSTRACT\nLarge deep learning models offer significant accuracy gains,\nbut training billions to trillions of parameters is challenging.\nExisting solutions such as data and model parallelisms exhibit\nfundamental limitations to fit these models into limited device\nmemory, while obtaining computation, communication and\ndevelopment efficiency. We develop a novel solution, Zero\nRedundancy Optimizer (ZERO), to optimize memory, vastly\nimproving training speed while increasing the model size that\ncommon settings like mixed precision and ADAM optimizer\n[6]. Other existing solutions such as Pipeline Parallelism (PP),\nModel Parallelism (MP), CPU-Offloading, etc, make trade-\noffs between functionality, usability, as well as memory and\ncompute/communication efficiency, all of which are crucial to\ntraining with speed and scale.\nAmong different existing solution for training large models,\nMP is perhaps the most promising one. The largest models in\nthe current literature, the 11B T5 model [5], and Megatron-\nS. Rajbhandari, J. Rasley, O. Ruwase and Y. He, \"ZeRO: Memory optimizations Toward Training Trillion\nParameter Models,\" SC20: International Conference for High Performance Computing, Networking,\nStorage and Analysis, Atlanta, GA, USA, 2020, pp. 1-16, doi: 10.1109/SC41405.2020.00024.\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-9-Columbia.pdf": {
        "text": "Recap: Additional GPU RAM needed for training\nModel Parameters (Weights)\nBytes per parameter\n4 bytes per parameter\nAdam optimizer (2 states)\n+8 bytes per parameter\nGradients\n+4 bytes per parameter\nActivations and\ntemp memory (variable size)\n+8 bytes per parameter (high-end estimate)\nTOTAL =4 bytes per parameter\n+20 extra bytes per parameter\n",
        "links": []
    },
    "./chunks/chunk_55_Lecture-9-Columbia.pdf": {
        "text": "Memory usage in DDP\nOne full copy of model and training parameters on each GPU\nSources:\nRajbhandari et al. 2019: \"ZERO: Memory Optimizations Toward Training Trillion Parameter Models\"\nZhao et al. 2023: \"PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\"\nParameters\nGradients\nOptimizer States\n",
        "links": []
    },
    "./chunks/chunk_56_Lecture-9-Columbia.pdf": {
        "text": "Zero Redundancy Optimizer (ZeRO)\nReduces memory by distributing (sharding) the model\ngpuo\ngpu\ngpuN-1\nMemory\nConsumed\nK=12\n4=7.58\nN=64\nBaseline\n(2+2+K) 120GB\nPos\nPos+g\nPos+g+p\nParameters\nGradients\n***\n24+24+\nOptimizer States\n31.4GB\n(2+K)-P\n24+\n16.6GB\n(2+2+K)\n1.9GB\nParameters\nGradients\nOptimizer States\nSources:\nRajbhandari et al. 2019: \"ZERO: Memory Optimizations Toward Training Trillion Parameter Models\"\nZhao et al. 2023: \"PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\"\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-9-Columbia.pdf": {
        "text": "Zero Redundancy Optimizer (ZeRO)\nReduces memory by distributing (sharding) the model\nparameters, gradients\ngpuo\ngpu\ngpuN-1\nBaseline\nZeRO Stage 1\nPos -\u2014\nZeRO Stage 2\n***\n\"\nPos+g\nZeRO Stage 3\nSources:\nParameters\nGradients\nOptimizer States\nRajbhandari et al. 2019: \"ZERO: Memory Optimizations Toward Training Trillion Parameter Models\"\nZhao et al. 2023: \"PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\"\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nLLM\nForward/\nDataloader\nGPU 3\nBackward pass\nUpdate\nModel\nLLM\nForward/\nUpdate\nGPU 2\nBackward pass\nModel\nSynchronize\ngradients\nLLM\nForward/\nUpdate\nGPU 1\nBackward pass\nModel\nLLM\nForward/\nGPU O\nBackward pass\nUpdate\nModel\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-9-Columbia.pdf": {
        "text": "Distributed Data Parallel (DDP)\nDataloader\nGPU 3\nForward/\nBackward pass\nUpdate\nModel\nForward/\nUpdate\nGPU 2\nBackward pass\nModel\nSynchronize\ngradients\nForward/\nUpdate\nGPU 1\nBackward pass\nModel\nForward/\nGPU O\nBackward pass\nUpdate\nModel\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-9-Columbia.pdf": {
        "text": "Fully Sharded Data Parallel (FSDP)\nDataloader\nGPU 3\nForward/\nBackward pass\nUpdate\nModel\nForward/\nUpdate\nGPU 2\nBackward pass\nModel\nSynchronize\ngradients\nForward/\nUpdate\nGPU 1\nBackward pass\nModel\nForward/\nGPU O\nBackward pass\nUpdate\nModel\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-9-Columbia.pdf": {
        "text": "Fully Sharded Data Parallel (FSDP)\nDataloader\nGPU 3\nForward/\nBackward pass\nUpdate\nModel\nForward/\nUpdate\nGPU 2\nBackward pass\nModel\nSynchronize\ngradients\nForward/\nUpdate\nGPU 1\nBackward pass\nModel\nForward/\nGPU O\nBackward pass\nUpdate\nModel\n",
        "links": []
    },
    "./chunks/chunk_62_Lecture-9-Columbia.pdf": {
        "text": "Fully Sharded Data Parallel (FSDP)\nDataloader\nGPU 3\nForward\npass\nBackward\npass\nUpdate\nmodel\nGPU 2\nForward\npass\nBackward\npass\nUpdate\nmodel\nGet\nweights\nGet\nweights\nSynchronize\ngradients\nForward\nBackward\nUpdate\nGPU 1\npass\npass\nmodel\nForward\nBackward\nGPU O\npass\npass\nUpdate\nmodel\n",
        "links": []
    },
    "./chunks/chunk_63_Lecture-9-Columbia.pdf": {
        "text": "Fully Sharded Data Parallel (FSDP)\nDataloader\nGPU 3\nForward\npass\nBackward\npass\nUpdate\nmodel\nGPU 2\nForward\npass\nBackward\npass\nUpdate\nmodel\nGet\nweights\nGet\nweights\nSynchronize\ngradients\nForward\nBackward\nUpdate\nGPU 1\npass\npass\nmodel\nForward\nBackward\nGPU O\npass\npass\nUpdate\nmodel\n",
        "links": []
    },
    "./chunks/chunk_64_Lecture-9-Columbia.pdf": {
        "text": ".\nFully Sharded Data Parallel (FSDP)\nHelps to reduce overall GPU memory utilization\nSupports offloading to CPU if needed.\nConfigure level of sharding via sharding factor\nFull replication (no sharding)\n1 GPU\nmax. number of GPUs\nFull sharding\n1 GPU\nHybrid sharding\n1 GPU\nmax. number of GPUs\nmax. number of GPUs\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-9-Columbia.pdf": {
        "text": "TFLOPS/GPU\n60\nImpact of using FSDP\n140\n120\nFull Sharding\nHybrid Sharding\nFull Replication\nDDP\n148.48\n145.81\nNote: 1 teraflop/s = 1,000,000,000,000\n(One trillion) floating point operations per second\n154\n100\n40\n20\n20\n15-18.-29.01.65\n27.49.70\n25.76.04\nTFLOPS / GPU\n152\n150\n148\n146\n144\n0\n611M\n2.28B\n11.3B\nModel Size (Numel)\n(a) Model Scale\n142\n- B=16\n8\n16\n32\n64\n128\n256\n512\nNumber of 80 GB A100s\n(c) T5-11B TFLOPS\nZhao et al. 2023: \"PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\"\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-9-Columbia.pdf": {
        "text": "Scaling laws and compute-optimal models\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-9-Columbia.pdf": {
        "text": "Scaling choices for pre-training\nGoal: maximize model\nperformance\nCONSTRAINT:\nCompute budget\n(GPUs, training time, cost)\nSCALING CHOICE:\nDataset size\n(number of tokens)\nModel\nperformance\n(minimize loss)\nSCALING CHOICE:\nModel size\n(number of parameters)\n|\n",
        "links": []
    },
    "./chunks/chunk_68_Lecture-9-Columbia.pdf": {
        "text": "Compute budget for training LLMs\n1 \"petaflop/s-day\" =\n#floating point operations performed at rate of 1 peta FLOP per second for one day\nNVIDIA V100s\nGPU\nGPU\n|||||\nGPU\nGPU\nIIIII\n|||||\nGPU\nGPU\nGPU\nGPU\nNote: 1 petaFLOP/s = 1,000,000,000,000,000\n(one quadrillion) floating point operations per second\n1 petaflops/s-day is these chips running at\nfull efficiency for 24 hours\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-9-Columbia.pdf": {
        "text": "Training Petaflop/s-days\nBERT-Base\n10\n100\nBERT-Large\n10000\n1000\nBERT/\nROBERTA\nT5\nNumber of petaflop/s-days to pretrain various LLMs\nTotal Compute Used During Training\nROBERTS\nTS-Small\nTS-Base\nTS-Large\nT5-38\nT5-11B\nGPT-3 Medium\nGPT-3 XL\nGPT-3678\nGPT-3\nBrown et al. 2020, \"Language Models are Few-Shot Learners\", https://arxiv.org/pdf/2005.14165\nGPT.3 138\nGPT.3 1758\n",
        "links": [
            "https://arxiv.org/pdf/2005.14165"
        ]
    },
    "./chunks/chunk_70_Lecture-9-Columbia.pdf": {
        "text": "Compute budget vs. model performance\nDATASET\nSIZE\nCOMPUTE\nBUDGET\nMODEL\nSIZE\n7\n9\nTest Loss\n3\n\u0441\u043b\n2\n10-9 10-7\n10-5 10-3 10-1 101\nCompute\nSource: Kaplan et al. 2020, \"Scaling Laws for Neural Language Models\"\n",
        "links": []
    },
    "./chunks/chunk_71_Lecture-9-Columbia.pdf": {
        "text": "Dataset size and model size vs. performance\nCOMPUTE\nBUDGET\nCompute resource constraints\n\u25cf\nHardware\n\u2022\nProject timeline\n\u25cf\nFinancial budget\nDATASET\nSIZE\nMODEL\nSIZE\nSource: Kaplan et al. 2020, \"Scaling Laws for Neural Language Models\"\n",
        "links": []
    },
    "./chunks/chunk_72_Lecture-9-Columbia.pdf": {
        "text": "Data size and model size vs. performance\nDATASET\nSIZE\nCOMPUTE\nBUDGET\nMODEL\nSIZE\nTest Loss\n108\nDataset Size\n109\n105\n107\nParameters\n109\nSource: Kaplan et al. 2020, \"Scaling Laws for Neural Language Models\"\n",
        "links": []
    },
    "./chunks/chunk_73_Lecture-9-Columbia.pdf": {
        "text": "Chinchilla paper\nDeepMind\nTraining Compute-Optimal Large Language Models\nJordan Hoffmann*, Sebastian Borgeaud*, Arthur Mensch*, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\nErich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre*\n*Equal contributions\nWe investigate the optimal model size and number of tokens for training a transformer language model\nunder a given compute budget. We find that current large language models are significantly under-\ntrained, a consequence of the recent focus on scaling language models whilst keeping the amount of\ntraining data constant. By training over 400 language models ranging from 70 million to over 16 billion\nparameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and\nthe number of training tokens should be scaled equally: for every doubling of model size the number\nof training tokens should also be doubled. We test this hypothesis by training a predicted compute-\noptimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and\n4x more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B),\nJurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.\nThis also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly\nfacilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of\n67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.\nJordan et al. 2022\n",
        "links": []
    },
    "./chunks/chunk_74_Lecture-9-Columbia.pdf": {
        "text": "Compute optimal models\n\u2022 Very large models may be over-parameterized and under-trained\nDATASET\nSIZE\nFixed\nCOMPUTE\nBUDGET\nMODEL\nSIZE\nDATASET\nSIZE\nFixed\nCOMPUTE\nBUDGET\nMODEL\nSIZE\n",
        "links": []
    },
    "./chunks/chunk_75_Lecture-9-Columbia.pdf": {
        "text": "Compute optimal models\n\u2022 Very large models may be over-parameterized and under-trained\nDATASET\nSIZE\nFixed\nCOMPUTE\nBUDGET\nOver-parameterized\nMODEL\nSIZE\nDATASET\nSIZE\nFixed\nCOMPUTE\nBUDGET\nMODEL\nSIZE\n",
        "links": []
    },
    "./chunks/chunk_76_Lecture-9-Columbia.pdf": {
        "text": "Compute optimal models\n\u2022 Very large models may be over-parameterized and under-trained\nSmaller models trained on more data could perform as well as large models\nOver-parameterized\nUnder-trained\nDATASET\nSIZE\nFixed\nCOMPUTE\nBUDGET\nMODEL\nDATASET\nSIZE\nSIZE\nFixed\nCOMPUTE\nBUDGET\nMODEL\nSIZE\n",
        "links": []
    },
    "./chunks/chunk_77_Lecture-9-Columbia.pdf": {
        "text": "Chinchilla scaling laws for model and dataset size\nModel # of parameters\nCompute-optimal*\n# of tokens (~20x)\nActual\n# tokens\nChinchilla\n70B\n~1.4T\n1.4T\nLLAMA-65B\n65B\n~1.3T\n1.4T\nGPT-3\n175B\n~3.5T\n300B\nOPT-175B\n175B\n~3.5T\n180B\nBLOOM\n176B\n~3.5T\n350B\nCompute optimal training datasize\nis ~20x number of parameters\nSources: Hoffmann et al. 2022, \"Training Compute-Optimal Large Language Models\"\nTouvron et al. 2023, \"LLAMA: Open and Efficient Foundation Language Models\"\nassuming models are trained to be\ncompute-optimal per Chinchilla paper\n",
        "links": []
    },
    "./chunks/chunk_78_Lecture-9-Columbia.pdf": {
        "text": "Question\nScaling laws for pre-training large language models consider several\naspects to maximize performance of a model within a set of constraints\nand available scaling choices. Select all alternatives that should be\nconsidered for scaling when performing model pre-training?\nA. Batch size: Number of samples per iteration\nB. Model size: Number of parameters\nC. Compute budget: Compute constraints\nD. Dataset size: Number of tokens\n",
        "links": []
    },
    "./chunks/chunk_79_Lecture-9-Columbia.pdf": {
        "text": "Model size vs. time\nBERT-L\n340M\nGPT-2\n1.5B\nGPT-3\n175B\nPaLM\n540B\n2018\nincrease?\nTrillion(s)\n2022\n2023\n",
        "links": []
    },
    "./chunks/chunk_80_Lecture-9-Columbia.pdf": {
        "text": "Model size vs. time\nincrease?\nBERT-L\n340M\nGPT-2\n1.5B\nGPT-3\n175B\nPaLM\n540B\nLLaMa\n65B\n2018\n2022\nTrillion(s)\ndecrease?\nBloombergGPT\n50B\n2023\n",
        "links": []
    },
    "./chunks/chunk_81_Lecture-9-Columbia.pdf": {
        "text": "Pre-training for domain adaptation\n",
        "links": []
    },
    "./chunks/chunk_82_Lecture-9-Columbia.pdf": {
        "text": "Pre-training for domain adaptation\nLegal language\nThe prosecutor had difficulty proving mens\nrea, as the defendant seemed unaware that\nhis actions were illegal.\nThe judge dismissed the case, citing the\nprinciple of res judicata as the issue had\nalready been decided in a previous trial.\nDespite the signed agreement, the contract\nwas invalid as there was no consideration\nexchanged between the parties.\nMedical language\nAfter a strenuous workout, the patient\nexperienced severe myalgia that\nlasted for several days.\nAfter the biopsy, the doctor confirmed\nthat the tumor was malignant and\nrecommended immediate treatment.\nSig: 1 tab po qid pc & hs.\nTake one tablet by mouth four times a day, after\nmeals, and at bedtime.\n",
        "links": []
    },
    "./chunks/chunk_83_Lecture-9-Columbia.pdf": {
        "text": "BloombergGPT: domain adaptation for finance\nBloombergGPT: A Large Language Model for Finance\nShijie Wu\u00b9*, Ozan \u0130rsoy\u00b9,*, Steven Lu\u00b9*, Vadim Dabravolski\u00b9, Mark Dredze\u00b9\u00b3,\nSebastian Gehrmann\u00b9, Prabhanjan Kambadur\u00b9, David Rosenberg\u00b2, Gideon Mann\u00b9\n1 Bloomberg, New York, NY USA\n2 Bloomberg, Toronto, ON Canada\n3 Computer Science, Johns Hopkins University, Baltimore, MD USA\n~51%\nFinancial\n(Public & Private)\nAbstract\nThe use of NLP in the realm of financial technology is broad and complex, with applications\nranging from sentiment analysis and named entity recognition to question answering. Large\nLanguage Models (LLMs) have been shown to be effective on a variety of tasks; however, no\nLLM specialized for the financial domain has been reported in literature. In this work, we\npresent BLOOMBERGGPT, a 50 billion parameter language model that is trained on a wide\nrange of financial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet, augmented with\n345 billion tokens from general purpose datasets. We validate BLOOMBERGGPT on stan-\ndard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks\nthat most accurately reflect our intended usage. Our mixed dataset training leads to a\nmodel that outperforms existing models on financial tasks by significant margins without\nsacrificing performance on general LLM benchmarks. Additionally, we explain our model-\ning choices, training process, and evaluation methodology. We release Training Chronicles\n(Appendix C) detailing our experience in training BLOOMBERGGPT.\n~49%\nOther\n(Public)\n",
        "links": []
    },
    "./chunks/chunk_84_Lecture-9-Columbia.pdf": {
        "text": "BloombergGPT relative to other LLMs\nParamete\neters (B)\nOptimal # Parameters w.r.t. FLOPS\n2000\n1000\n500\n200\n100\nChinchilla-1\nChinchilla-2\nChinchilla-3\nKaplan\n5000\nMT-NLG. PaLM\u26ab\n\u2022Gopher\nGPT-3/Jurassic/OPT\u25cf \u25cfBLOOM\nLaMDA.\nChinchilla\nTokens (B)\nOptimal #Tokens w.r.t. FLOPS\nChinchilla-1\nChinchilla-2\nChinchilla-3\nKaplan\n2000\n1000\n\u26abLLAMA\nLLAMA Chinchilla\nLLAMA\n\u26abPaLM\nBloombergGPT\n\u25cfPaLM\n50\nOPT PALM\nBloombergGPTO\nLaMA\nLLAMA\n500\nNeoX\n\u2022BLOOM\n\u26abOPT\n20\n20\nNeoX\n200\n\u2022LaMDA\n10\n10\nGopher\n\u26abGPT-3/Jurassic/OPT\n\u25cfMT-NLG-\nLLAMA\u26ab\n100\n1e22\n3.2e22\n1e23\n3.2e23\n1e24\n3.2e24\n1e22\n3.2e22\n1e23\n3.2e23\n1e24\n3.2e24\nFLOPS\nFLOPS\nSource: Wu et al. 2023, \"BloombergGPT: A Large Language Model for Finance.\"\n",
        "links": []
    },
    "./chunks/chunk_85_Lecture-9-Columbia.pdf": {
        "text": "BloombergGPT relative to other LLMs\nParameters (B)\nOptimal #Parameters w.r.t. FLOPS\n2000\nChinchilla-1\n1000\n5000\nChinchilla-2\nChinchilla-3\n500\nMT-NLG PALM\nKaplan\nGopher\n2000\n200\nGPT-3/Jurassio/OPT BLOOM\n100\n50\nLaMDA\nOPT PALM\nBloombergGPTO\nLLAMA\n20\n20\nNeox\n10\n10\nChinchilla\nLLAMA\nTokens (B)\n1000\nOptimal #Tokens w.r.t. FLOPS\nChinchilla-1\nChinchilla-2\nChinchilla-3\nKaplan\n\u26abLLAMA\n500\nNeoX\n200\nLLAMA\n\u0394\u0391\u039c\u0391\nChinchilla\n\u25cfPALM\nBloombergGPT\nBLOOM\n\u26abORT\nLaMDA\nMT-NLG\nGopher\nGPT-3/Jurassic/OPT\n\u25cfPALM\nLLAMA\n100\n1e22\n3.2e22\n1e23\n3.2e23\n1e24\n3.2e24\n1e22\n3.2e22\nle23\n3.2e23\n1e24\n3.2e24\nFLOPS\nFLOPS\nSource: Wu et al. 2023, \"BloombergGPT: A Large Language Model for Finance\"\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-11-columbia.pdf": {
        "text": "+\nLecture 11 11/12/24\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\nParijat Dube and Chen Wang\n1\n+\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-11-columbia.pdf": {
        "text": "Agenda\n.\nLLM Evaluation Benchmarks\n\u2022\nGLUE, SuperGLUE, HELM, MMLU, Big-Bench, LLMPerf\nLLM Evaluation Metrics\n2\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-11-columbia.pdf": {
        "text": "Evaluation benchmarks\nGLUE\nSuperGLUE HELM\nMMLU (Massive Multitask\nLanguage Understanding)\nBIG-bench\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-11-columbia.pdf": {
        "text": "GLUE\n\u2022 General Language Understanding Evaluation (GLUE) benchmark\n.\nA collection of NLU tasks including question answering, sentiment\nanalysis, and textual entailment\n\u2022 An associated online platform for model evaluation, comparison,\nand analysis\n\u25cf\nGLUE does not place any constraints on model architecture beyond\nthe ability to process single-sentence and sentence-pair inputs and\nto make corresponding predictions\nEncourage development of models that can generalize across\nseveral linguistic tasks\n\u2022 Centered on 9 English sentence understanding tasks\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-11-columbia.pdf": {
        "text": "GLUE\n* GLUE\nThe tasks included in SuperGLUE benchmark:\nCorpus Train] [Test Task\nMetrics\nDomain\nSingle-Sentence Tasks\nCOLA\n8.5k\n1k\nSST-2\n67k\n1.8k\nacceptability\nsentiment\nMatthews corr.\nmisc.\nacc.\nmovie reviews\nSimilarity and Paraphrase Tasks\nMRPC\n3.7k\n1.7k\nparaphrase\nacc./F1\nnews\nSTS-B\n7k\n1.4k\nsentence similarity\nPearson/Spearman corr.\nmisc.\nQQP\n364k\n391k\nparaphrase\nacc./F1\nsocial QA questions\nInference Tasks\nMNLI\n393k\n20k\nNLI\nmatched acc./mismatched acc.\nmisc.\nQNLI\nRTE\n105k 5.4k\n2.5k\n3k\nQA/NLI\nNLI\nWNLI\n634\n146\ncoreference/NLI\nWikipedia\nnews, Wikipedia\nfiction books\nSource: Wang et al. 2018, \"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural\nLanguage Understanding\"\nacc.\nacc.\nacc.\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-11-columbia.pdf": {
        "text": "GLUE Tasks: Single-Sentence Tasks\n1. COLA: The Corpus of Linguistic Acceptability consists of English\nacceptability judgments drawn from books and journal articles on\nlinguistic theory. Each example is a sequence of words annotated with\nwhether it is a grammatical English sentence.\n2. SST-2: The Stanford Sentiment Treebank consists of sentences from\nmovie reviews and human annotations of their sentiment. The task is to\npredict the sentiment of a given sentence.\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-11-columbia.pdf": {
        "text": "GLUE Tasks - Similarity and Paraphrase Tasks\n3. MRPC: The Microsoft Research Paraphrase Corpus is a corpus of sen-\ntence pairs automatically extracted from online news sources, with\nhuman annotations for whether the sentences in the pair are\nsemantically equivalent.\n4. QQP: The Quora Question Pairs2 dataset is a collection of question\npairs from the community question-answering website Quora. The task\nis to determine whether a pair of questions are semantically equivalent.\n5. STS-B: The Semantic Textual Similarity Benchmark is a collection of\nsentence pairs drawn from news headlines, video and image captions,\nand natural language inference data. Each pair is human-annotated\nwith a similarity score from 1 to 5; the task is to predict these scores.\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-11-columbia.pdf": {
        "text": "GLUE Tasks: Inference Tasks\n6. MNLI: The Multi-Genre Natural Language Inference Corpus is a crowd-\nsourced collection of sentence pairs with textual entailment\nannotations. Given a premise sentence and a hypothesis sentence, the\ntask is to predict whether the premise entails the hypothesis\n(entailment), contradicts the hypothesis (contradiction), or neither\n(neutral).\n7. QNLI: The Stanford Question Answering Dataset is a question-\nanswering dataset consisting of question-paragraph pairs, where one of\nthe sentences in the paragraph (drawn from Wikipedia) contains the\nanswer to the corresponding question (written by an annotator).\n8. RTE: The Recognizing Textual Entailment (RTE) datasets\n9. WNLI: The Winograd Schema Challenge is a reading comprehension\ntask in which a system must read a sentence with a pronoun and select\nthe referent of that pronoun from a list of choices.\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-11-columbia.pdf": {
        "text": "0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nBILSTM+ELMo+Attr\n*\n1.1\n1.2\nOpenAI GPT\n2019 SOTA Performance on GLUE\nBERT + Single-task Adapters\n*\nBERT (Large)\nBERT on STILTS\nBERT + BAM\nSemBERT\nSnorkel MeTal\nGLUE Score\nMRPC\nHuman Performance\nCOLA\nSTS-B\n*\nQQP\nSST-2\nMNLI\nQNLI\nRTE\nWNLI\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-11-columbia.pdf": {
        "text": "GLUE Leaderboard\nhttps://gluebenchmark.com/leaderboard\nGLUE SuperGLUE\nRank Name\nModel\n1 Microsoft Alexander v-team\nTuring ULR v6\n2 JDExplore d-team\nVega v1\n3 Microsoft Alexander v-team\nTuring NLR v5\n4\nDIRL Team\nDeBERTA + CLEVER\n5\nERNIE Team - Baidu\nERNIE\n6\nAliceMind & DIRL\nStructBERT + CLEVER\nPaper </> Code\nTasks Leaderboard i FAQ Diagnostics\nSubmit Login\nURL Score COLA SST-2 MRPC STS-B\nQQP MNLI-m MNLI-mm QNLI RTE WNLI\nA\n91.3 73.3 97.5 94.2/92.3 93.5/93.1 76.4/90.9\n92.5\n92.1 96.7 93.6 97.9 55.\n91.3 73.8 97.9 94.5/92.6 93.5/93.1 76.7/91.1\n92.1\n91.9 96.7 92.4 97.9 51.\n91.2 72.6 97.6 93.8/91.7 93.7/93.3 76.4/91.1\n92.6\n92.4 97.9 94.1 95.9 57.\n91.1 74.7 97.6 93.3/91.1 93.4/93.1 76.5/91.0\n92.1\n91.8 96.7 93.2 96.6 53.\n91.1\n75.5 97.8 93.9/91.8 93.0/92.6 75.2/90.9\n92.3\n91.7 97.3 92.6 95.9 51\n91.0 75.3 97.7 93.9/91.9 93.5/93.1 75.6/90.8\n91.7\n91.5 97.4 92.5 95.2 49\n7\nDeBERTA Team - Microsoft\nDeBERTa / Turing NLRv4\n90.8 71.5 97.5 94.0/92.0 92.9/92.6 76.2/90.8\n91.9\n91.6 99.2 93.2 94.5 53.\n8\nHFL iFLYTEK\n9\nPING-AN Omni-Sinitic\nMacALBERT + DKM\n90.7\n74.8 97.0 94.5/92.6 92.8/92.6 74.7/90.6\n91.3\n91.1 97.8 92.0 94.5 52.\nALBERT + DAAF + NAS\n90.6 73.5 97.2 94.0/92.0 93.0/92.4 76.1/91.0\n91.6\n91.3 97.5 91.7 94.5 51.\n10\nT5 Team - Google\nT5\n90.3 71.6 97.5 92.8/90.4 93.1/92.8 75.1/90.6\n92.2\n91.9 96.9 92.8 94.5 53.\n",
        "links": [
            "https://gluebenchmark.com/leaderboard"
        ]
    },
    "./chunks/chunk_11_Lecture-11-columbia.pdf": {
        "text": "SuperGLUE\nEight language understanding tasks\nFour categories of tasks\n1. QA: question answering\n2. NLI: Natural Language Inference\n3. WSD: Word Sense Disambiguation\n4. coref.: coreference resolution\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-11-columbia.pdf": {
        "text": "SuperGLUE Tasks\nCorpus |Train| |Dev Test Task\nMetrics\nBoolQ\n9427 3270\n3245\nQA\nacc.\nCB\n250\n57\n250 NLI\nCOPA\n400\n100\n500\nQA\nacc.\nMultiRC\n5100\n953\n1800\nQA\nF1a/EM\nRECORD\n101k\n10k\n10k \u041e\u0410\nF1/EM\nRTE\n2500\n278\n300\nNLI\nacc.\nWiC\n6000\n638\n1400\nWSD\nacc.\nacc./F1\nText Sources\nGoogle queries, Wikipedia\nvarious\nblogs, photography encyclopedia\nvarious\nnews (CNN, Daily Mail)\nnews, Wikipedia\nWordNet, VerbNet, Wiktionary\nWSC\n554\n104\n146\ncoref.\nacc.\nfiction books\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-11-columbia.pdf": {
        "text": "Example of different SuperGLUE tasks\nBoolQ (Boolean Questions): QA task where each example consists of a\nshort passage and a yes/no question about the passage\nBoolQ\nPassage: Barq's - Barq's is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq's, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq's Famous Olde Tyme Root Beer\nuntil 2012.\nQuestion: is barq's root beer a pepsi product\nAnswer: No\n\u2022 COPA (Choice of Plausible): system is given a premise sentence and must\ndetermine either the cause or effect of the premise from two possible\nchoices\nCOPA\nPremise: My body cast a shadow over the grass. Question: What's the CAUSE for this?\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\nCorrect Alternative: 1\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-11-columbia.pdf": {
        "text": "Example of different SuperGLUE tasks\n\u2022\nMultiRC (Multi-Sentence Reading Comprehension): each example\nconsists of a context paragraph, a question about that paragraph, and a list of\npossible answers. The system must predict which answers are true and\nwhich are false.\nMultiRC\nParagraph: Susan wanted to have a birthday party. She called all of her friends. She has five friends.\nHer mom said that Susan can invite them all to the party. Her first friend could not go to the party\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\nparents would let her. The fourth friend said maybe. The fifth friend could go to the party for sure. Susan\nwas a little sad. On the day of the party, all five friends showed up. Each friend had a present for Susan.\nSusan was happy and sent each friend a thank you card the next week\nQuestion: Did Susan's sick friend recover? Candidate answers: Yes, she recovered (T), No (F), Yes\n(T), No, she didn't recover (F), Yes, she was at Susan's party (T)\nWiC (Word-in-Context): is a word sense disambiguation task cast as binary\nclassification of sentence pairs. Given two text snippets and a polysemous\nword that appears in both sentences, the task is to determine whether the\nword is used with the same sense in both sentences.\nText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident.\nEntailment: False\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-11-columbia.pdf": {
        "text": "Example of different SuperGLUE tasks\n\u2022 RECORD (Reading Comprehension with Commonsense Reasoning\nDataset): is a multiple-choice QA task. Each example consists of a news.\narticle and a Cloze-style question about the article in which one entity is\nmasked out. The system must predict the masked out entity from a list of\npossible entities in the provided passage\nRECORD\nParagraph: (CNN) Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\nbody that can approve new states, will ultimately decide whether the status of the US commonwealth\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, official results from the State Electorcal Commission show. It\nwas the fifth such vote on statehood. \"Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, \"Don't blame me, I didn't vote for them,\" when discussing the\n<placeholder> presidency Correct Entities: US\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-11-columbia.pdf": {
        "text": "SuperGLUE\nSuperGLUE\nThe tasks included in SuperGLUE benchmark:\n250\nCOPA\n400\nCorpus Train |Dev |Test Task\nBoolQ\n9427 3270 3245 QA\nCB\n57 250 NLI\n500 QA\nMetrics\nText Sources\nacc.\nacc./F1\n100\nacc.\nMultiRC\n5100\n953\n1800 QA\nF1a/EM\nRECORD\n101k\n10k\n10k QA\nF1/EM\nRTE\n2500\n278\n300\nNLI\nacc.\nGoogle queries, Wikipedia\nvarious\nblogs, photography encyclopedia\nvarious\nnews (CNN, Daily Mail)\nnews, Wikipedia\nWiC\n6000\n638\nWSC\n554\n104\n1400 WSD\n146 coref.\nacc.\nWordNet, VerbNet, Wiktionary\nacc.\nfiction books\nSource: Wang et al. 2019, \"SuperGLUE: A Stickier Benchmark for General-Purpose Language\nUnderstanding Systems\"\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-11-columbia.pdf": {
        "text": "SuperGLUE Leaderboard\nhttps://super.gluebenchmark.com/leaderboard/\nSuperGLUE GLUE\nPaper </> Code\nTasks Leaderboard i FAQ Diagnostics\nSubmit Login\nRank Name\n1\nInspur Cloud\nModel\nHairuo\n2\nJDExplore d-team\nVega v2\n3\nLiam Fedus\nST-MOE-32B\n4\nMicrosoft Alexander v-team\nTuring NLR v5\n5\nERNIE Team - Baidu\n6\nYi Tay\n7\nZirui Wang\nERNIE 3.0\nPALM 540B\nT5+UDG, Single Model (Google Brain)\n8\nDeBERTA Team - Microsoft\nDeBERTA / Turing NLRv4\n9\nSuperGLUE Human Baselines\nSuperGLUE Human Baselines\n+\n10 T5 Team - Google\nT5\nLeaderboard Version: 2.0\nURL Score BoolQ\nCB COPA MultiRC ReCORD\nRTE\nWic\nWSC AX-b\nAX-g\n| | \u0f46\n\u0f46 \u0f46\n91.4\n92.5 96.5/97.6 100.0 90.5/67.9 94.1/93.2\n92.8\n76.1\n100.0\n64.6 96.1/94.7\n91.3\n90.5 98.6/99.2\n99.4 88.2/62.4 94.4/93.9\n96.0\n77.4\n98.6\n-0.4 100.0/50.0\n91.2\n92.4 96.9/98.0\n99.2 89.6/65.8 95.1/94.4\n93.5\n77.7\n96.6\n72.3 96.1/94.1\n90.9\n92.0 95.9/97.6\n98.2 88.4/63.0 96.4/95.9\n94.1\n77.1\n97.3\n67.8 93.3/95.5\n90.6\n91.0 98.6/99.2\n97.4 88.6/63.2 94.7/94.2\n92.6\n77.4\n97.3\n68.6 92.7/94.7\n \u0f7a\u0f0b\u0f5a\u0f7c\n90.4\n91.9 94.4/96.0\n99.0 88.7/63.6 94.2/93.3\n94.1\n77.4\n95.9\n72.9 95.5/90.4\n90.4\n91.4 95.8/97.6\n98.0 88.3/63.0 94.2/93.5\n93.0\n77.9\n96.6\n69.1 92.7/91.9\n90.3\n90.4 95.7/97.6\n98.4 88.2/63.7 94.5/94.1\n93.2\n77.5\n95.9\n66.7 93.3/93.8\n89.8\n89.0 95.8/98.9\n100.0 81.8/51.9 91.7/91.3\n93.6\n80.0\n100.0\n76.6 99.3/99.7\n89.3\n91.2 93.9/96.8\n94.8 88.1/63.3 94.1/93.4\n92.5\n76.9\n93.8\n65.6 92.7/91.9\n",
        "links": [
            "https://super.gluebenchmark.com/leaderboard/"
        ]
    },
    "./chunks/chunk_18_Lecture-11-columbia.pdf": {
        "text": "Benchmarks for LLMs\nMassive\nMultitask\nLanguage\nUnderstanding\n(MMLU)\nBIG-bench Hard\nBIG-bench\nLite\n2021\nSource: Hendrycks, 2021. \"Measuring Massive\nMultitask Language Understanding\"\n2022\nSource: Suzgun et al. 2022. \"Challenging BIG-Bench\ntasks and whether chain-of-thought can solve them\"\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-11-columbia.pdf": {
        "text": "Holistic Evaluation of Language Models (HELM)\n\u2022 A holistic framework for evaluating language\nmodels\n\u2022 Leaderboards with many scenarios, metrics,\nmodels\nCore scenarios\n\u2022 NarrativeQA\n\u2022 NaturalQuestions (open-book)\n\u2022 NaturalQuestions (closed-book)\n\u2022\nOpenbookQA\n\u2022 MMLU (Massive Multitask Language Understanding)\n\u2022 MATH\n\u2022 GSM8K (Grade School Math)\nIN\nScenarios\nM\nHELM\n1\n123\nModels\n\u2022\nLegalBench\n\u2022 MedQA\n\u2022 WMT 2014\nhttps://crfm.stanford.edu/helm/\n",
        "links": [
            "https://crfm.stanford.edu/helm/"
        ]
    },
    "./chunks/chunk_20_Lecture-11-columbia.pdf": {
        "text": "Holistic Evaluation of Language Models (HELM)\nXHELM\nModels\nMetrics:\n1. Accuracy\n2. Calibration\n3.\nRobustness\n4.\nFairness\n5.\nBias\n6.\nToxicity\n7.\nEfficiency\nhttps://crfm.stanford.edu/helm/\nScenarios\nAnthropic-\nCohere Cohere\nCohere\nCohere\nGPT-\nJ1-Jumbo J1-Grande\nJ1-Large\nLM\nBLOOM\nTOpp\nXL\nLarge\nMedium\nSmall\nNeoX\nNaturalQuestions (open)\nNaturalQuestions (closed) \u2713 \u2713\n\u2713\n\u2713\n\u2713\n\u2713\nBoolQ\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u03bd\n\u2713\n\u2713\n\u2713\nNarrativeQA\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nQUAC\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nHellaSwag\n\u2713\n\u2713\n\u2713\n\u2713 \u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nOpenBookQA\n\u2713\n\u2713\n\u2713 \u2713 \u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTruthfulQA\nVVVVVVVVVVV\nMMLU\nMS MARCO\nTREC\n\u2713\n\u2713 \u2713 \u2713 \u2713 \u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713 \u2713\n\u2713\n\u2713 \u2713 \u2713\n\u2713 \u2713\n\u2713\n\u2713\n\u2713\n\u2713\nXSUM\nCNN/DM\nIMDB\nCivilComments\nRAFT\n|\u09e7\u09e7\u09e7\u09e7\u09e7\n| \u09e7\u09e7\u09e7\u09e7\u09e7\n| \u09e7\u09e7\u09e7\u09e7\u09e7\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n",
        "links": [
            "https://crfm.stanford.edu/helm/"
        ]
    },
    "./chunks/chunk_21_Lecture-11-columbia.pdf": {
        "text": "LLM Evaluation - Challenges\n\"Mike really loves drinking tea.\"\n\"Mike adores sipping tea.\"\n\"Mike does not drink coffee.\"\n#\n\"Mike does drink coffee.\"\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-11-columbia.pdf": {
        "text": "LLM Evaluation Metrics\nROUGE\nBLEU\nSCORE\nUsed for text summarization\n\u2022 Compares a summary to one\nor more reference summaries\nUsed for text translation\n\u2022 Compares to human-generated\ntranslations\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-11-columbia.pdf": {
        "text": "LLM Evaluation - Metrics - Terminology\nn-gram\nThe dog lay on the rug as I sipped a cup of tea.\nbigram\n\u0633\u0647\u0627\nunigram\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-11-columbia.pdf": {
        "text": "LLM Evaluation - Metrics - ROUGE-1\nReference (human):\nIt is cold outside.\nROUGE-1\nRecall\n=\nGenerated output:\nIt is very cold outside.\nROUGE-1\n=\nPrecision:\nunigram matches\nunigrams in reference\nGenerated output:\nROUGE-1\n=\n2\nIt is not cold outside.\nF1:\nunigram matches\nunigrams in output\nprecision x recall\nprecision + recall\n=\n=\n44\n4\n= 1.0\n4\n45\n=\n= 0.8\n0.8\n=\n2\n= 0.89\n1.8\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-11-columbia.pdf": {
        "text": "LLM Evaluation - Metrics - ROUGE-2\nReference (human):\nIt is cold outside.\nROUGE-2\nRecall:\n=\nIt is\nis cold\ncold outside\nGenerated output:\nIt is very cold outside.\nbigram matches\nbigrams in reference\n=\n23\n= 0.67\nROUGE-2\nPrecision:\nbigram matches\n2\n=\n= 0.5\nbigrams in output\n4\nIt is\nis very\nROUGE-2\nprecision x recall\n0.335\n=\n2\n=\n2\nvery cold\ncold outside\nF1:\nprecision + recall\n= 0.57\n1.17\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-11-columbia.pdf": {
        "text": "LLM Evaluation - Metrics - ROUGE-L\nReference (human):\nIt is cold outside.\nReference (human):\nIt is cold outside.\nROUGE-L\nRecall:\n=\nLCS(Gen, Ref)\nunigrams in reference\n=\n24\n= 0.5\nGenerated output:\nIt is very cold outside.\nGenerated output:\nIt is very cold outside.\nROUGE-L\nPrecision:\nLCS(Gen, Ref)\nunigrams in output\n=\n25\n= 0.4\nLongest common subsequence (LCS):\nIt is\ncold outside\n2\nROUGE-L\nF1:\n= 2\nprecision x recall\nprecision + recall\n0.2\n=\n2\n= 0.44\n0.9\n\u2022\nROUGE is a collection of ROUGE-1, ROUGE-2, ROUGE-L metrics\n\u2022\nROUGE scores are only comparable for the same task\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-11-columbia.pdf": {
        "text": "Problems with ROUGE\nReference (human):\nIt is cold outside.\nROUGE-1\nPrecision\nunigram matches\n4\n=\n= 1.0\nunigrams in output\n4\nGenerated output:\ncold cold cold cold\nModified\nprecision\nclip(unigram matches)\nunigrams in output\n1\n= 0.25\n4\nGenerated output:\noutside cold it is\nModified\nprecision\nclip(unigram matches)\n4\n=\n=\n= 1.0\nunigrams in output\n4\nD:\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-11-columbia.pdf": {
        "text": "LLM Evaluation - Metrics - BLEU\nBLEU metric = Avg(precision across range of n-gram sizes)\nReference (human):\nI am very happy to say that I am drinking a warm cup of tea.\nGenerated output:\nI am very happy that I am drinking a cup of tea. - BLEU 0.495\nI am very happy that I am drinking a warm cup of tea. - BLEU 0.730\nI am very happy to say that I am drinking a warm tea. - BLEU 0.798\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-11-columbia.pdf": {
        "text": "Quantitative performance metrics for LLM\nInference\n\u2022 Performance metrics for LLM inference\n\u2022\n\u2022\nCompleted requests per minute\nTime to first token (TTFT): how long before the LLM returns the first token\n\u2022 Inter-token latency (ITL): average time between consecutive tokens\n\u2022 End-to-end Latency: approximately the same as the average output length of\ntokens multiplied by the inter-token latency\n\u2022 Cost per typical request: API providers can usually trade off one of the other\nmetrics for cost. For example, you can reduce latency by running the same\nmodel on more GPUs or using higher-end GPUs.\nhttps://www.anyscale.com/blog/reproducible-performance-metrics-for-llm-inference\n",
        "links": [
            "https://www.anyscale.com/blog/reproducible-performance-metrics-for-llm-inference"
        ]
    },
    "./chunks/chunk_30_Lecture-11-columbia.pdf": {
        "text": "Metrics for LLM Reasoning\n\u2022\n.\nLarge language models show improved downstream task\nperformance when prompted to generate step-by-step reasoning\nto justify their final answers (Chain of Thought, Tree of Thought,\nReAct)\nReasoning steps greatly improve model interpretability and\nverification,\n\u2022 Objectively studying the correctness of reasoning is difficult\nHow often the stated reasoning steps actually support the final\nend task predictions?\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-11-columbia.pdf": {
        "text": "Example of 2-shot reasoning\n2-shot examples\nI will explain how to make logical inferences:\nContext: John wakes to the sound of his children giggling outside the door. They bust in and say\n\"Happy Father's Day!\" with a plate full of food. 2022 has been a great year.\nQuestion: Can we infer that John was born before the inauguration of Barack Obama?\nExplanation: We can infer that John is a father because of \"his children\". We can infer that the\ncurrent year is 2022. Barack Obama was inaugurated in 2009, so it has been about 13 years since\nObama was inaugurated. John's children are old enough to speak, so if John were born after Obama's\ninauguration, he would have been younger than 10 when he had children. So, we can infer that John\nwas born before Obama's inauguration. The answer is \"yes\".\nContext: Sam sees a piano in the subway stations. Laughing, his friends prod him to play, saying\n\"Remember how amazing your rendition of Chopsticks was at the Christmas party?\"\nQuestion: Can we infer that John majored in piano at Juilliard?\nExplanation: Juilliard is a top school for music, so someone who majored in piano at Juilliard would\nbe an expert pianist. \"Chopsticks\" is a very simple song to play on the piano. His friends were\ntherefore likely being sarcastic when they told him to play, which would imply that Sam was not an\nexpert piano player. So it is unlikely that Sam majored in piano at Juilliard. The answer is \"no\".\nStep-by-step reasoning for logical inference\ntask generated from 2-shot GPT-3\nContext: Shelley is from Virginia, but is visiting that\ncity with that famous market where they throw the fish.\nGoing home next Tuesday!\nQuestion: Is it likely that Shelly will be near the Pacific\nOcean this weekend?\nGPT-3 Step-by-step reasoning:\n\u25cf No, it is not likely that Shelley will be near the Pacific\nOcean this weekend.\n\u2022 Shelley is from Virginia, which is on the East Coast\nof the United States.\n\u2022 The Pacific Ocean is on the West Coast.\n\u2022 Shelley is visiting a city with a famous market, which\ncould be anywhere in the world.\n\u2022 Tuesday is her travel day, so she will likely not be\ntraveling this weekend.\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-11-columbia.pdf": {
        "text": "Taxonomy of Step-by-Step Reasoning Errors\nError Type\nGrammar\nFactuality\nHallucination\nRedundancy\nRepetition\nDescription\nFaulty, unconventional, or contro-\nversial grammar usage\nInformation about object (i.e. quan-\ntity, characteristics) or a personal\nnamed entity does not match infor-\nmation provided in the question\nInformation is not provided in the\nproblem statement and is irrelevant\nor wrong\nExplanation contains redundant in-\nformation, which even though\nmight be factual, is not required to\nanswer the question\nStep paraphrases information al-\nready mentioned in previous reason-\ning steps\nExample\nReference: He chews 4 pieces of gum a day so over 30 days\nhe will chew 4*30 = \u00ab4*30=120\u00bb120 pieces of gum.\nModel Expl: He eats 4 pieces of gum a day and lasts 30 days\nso he eats 4*30= \u00ab4*30=120\u00bb120 pieces of gum.\nContext: Anakin caught 10 fish.\nModel Expl: Locsin caught 10 fish.\nContext: The basketball team went to the steakhouse to eat\ndinner. The first player ate a 6-ounce steak. The second\nplayer ate beef tips, containing 8 beef tips, each an ounce\nin size. The third player ate a one-pound steak. And the\nfourth and fifth players ordered vegetarian meals. In total,\nhow many ounces of meat were consumed by the team?\nModel Expl: The fourth and fifth players ordered vegetarian\nmeals, for a total of 2 ounces of meat.\nContext: Bob is cold. If something is quiet then it is blue.\nAll blue things are round. If something is round and smart\nthen it is cold. Question: Bob is cold.\nModel Expl: If something is round and not smart then it is\nblue. All blue things are round. Answer: true.\nContext: Parker chews 4 pieces of gum a day. A pack of gum\nhas 15 pieces of chewing gum per pack. How many packs of\ngum will he need to last him 30 days?\nModel Expl.: He chews 4 pieces of gum a day and there are\n30 days in a month so he chews 4*30 = 120 pieces of gum a\nmonth. Parker chews 120 pieces of gum a month. He needs\n120 pieces of gum a month and a pack of gum has 15 pieces\nof gum so he will need 120/15 = 8 packs of gum.\nROSCOE: A SUITE OF METRICS FOR SCORING STEP-BYSTEP REASONING\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-11-columbia.pdf": {
        "text": "Taxonomy of Step-by-Step Reasoning Errors\nMissing step\nCoherency\nCommonsense\nThe content of the generated rea-\nsoning is incomplete and lacks re-\nquired information to produce the\ncorrect answer. If this missing steps\nare added, the model would produce\nthe correct answer\nContext: A farm has 10 2-legged animals and 15 4-legged\nanimals. If all animals are healthy, how many pairs of animal\nlegs in total are on the farm? Answer: 40\nModel Expl.: 10 2-legged animals have 10*2=20 legs. 15\n4-legged animals have 15*4=60 legs. In total on the farm,\nthere are 20+60=80 legs.Answer: 80 missing step: Total\nnumber of pairs is 80/2=40\nSteps contradict each other or do Context: If the reactor is intact, then it is safe. If the reactor\nnot follow a cohesive story\nis safe, then no graphite is outside it.\nModel lacks relations that should\nbe known from general knowledge\nabout the world (i.e. how to com-\npute velocity, how many inches in\none foot, all ducks are birds, etc.)\nArithmetic\nError in math calculations\nModel Expl.: The reactor is intact. All graphite is inside of\nit. There are some graphite outside of the the reactor.\nQuestion: Jordan has 2 children who wear diapers. Each\nchild requires 5 diaper changes per day. Jordan's wife\nchanges half of the diapers. How many diapers does Jor-\ndan change per day?\nModel Expl.: Jordan has 2*5-10 diapers to change per day.\nJordan's wife changes 10/2=5.0 diapers per day. For both\nchildren, Jordan changes 10+5=15 diapers per day.\nReference: If a truck was carrying 20 tons of fertiliser packed\nin bags, the total number of bags in a truck is 20*20 = 400\nbags\nModel Expl: If a truck was carrying 20 tons of fertiliser\npacked in bags, the total number of bags in a truck is 20*20\n= 40 bags\nROSCOE: A SUITE OF METRICS FOR SCORING STEP-BYSTEP REASONING\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-11-columbia.pdf": {
        "text": "Reasoning Scorer: ROSCOE\n\u25cf\n\u2022\nSuite of metrics for scoring Step-by-Step reasoning\nSource context of T sentences indicating a problem statement\nfollowed by a question and is prompted to generate step-by-step\nreasoning\nsource context s = {81,\u2026,ST}\nHypothesis of N-steps, including a final answer as the last step\nhypothesis h = {h\u2081,...,hN}\nGold step-by-step reasoning of K steps (may or may not be available)\nreferences r = {r\u2081,\u00b7\u00b7\u00b7,rk}\n...\nROSCOE: A SUITE OF METRICS FOR SCORING STEP-BYSTEP REASONING\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-11-columbia.pdf": {
        "text": "Reasoning Scorer: ROSCOE\n\u25cf\nFine-grained metrics under 4 categories\n1. Semantic-Alignment Metrics\n2. Semantic-Similarity Metrics\n3. Local Inference Metrics\n4. Language Coherence Metrics\nROSCOE: A SUITE OF METRICS FOR SCORING STEP-BYSTEP REASONING\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-11-columbia.pdf": {
        "text": "Reasoning Alignment Vector\nReasoning alignment vector from the N-step hypothesis h to the\nsources of length T\nr-align(h \u2192 s) = {a1, a2,\u00b7\u00b7\u00b7, \u03b1N}\n\u03b12,\n\u03b1i =\nr-align(h; \u2192 s) = [1 + max 1 (cos(hi, s\u2081)]/2 \u20ac [0, 1]\nT\nj=1\nnormalized cosine similarity between hypothesis step and most similar\nsentence in a context\n\u2022 Measures the grounding of the step-wise reasoning with respect\nto the source text\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-11-columbia.pdf": {
        "text": "Semantic Alignment Metrics\nTable 3: Semantic alignment metrics (ROSCOE-SA).\nScore\nFaithfulness-Step\n(h\u2192 s)\nInformativeness-Step\n(Info-Step) (hs)\nHallucination\n(h \u2192 (s,r))\nRedundancy (h \u2192 r)\nDescription\nThis step-level score is based on the alignment from the hypothesis steps to the source sentences, and is calcu-\nlated as the mean reasoning alignment score over the steps of reasoning (see illustration in Appendix D, Figure 3):\n(1/N)r-align(h; \u2192 s). Faithfulness measures if the model misinterpreted the problem statement, or the reasoning\nchain is too vague, irrelevant, or misuses information.\nMeasures how well information present in the source is used in the reasoning steps: [(1/T) \u03a3\u2081 r-align(st \u2192 h) +\n(1/N) r-align(h; \u2192 s)]/2. Info-step gives a higher score to reasoning steps that are well-grounded with respect\nto the source, and identifies the degree of information from source that is covered by the generated hypothesis. A lower\nInfo-Step score corresponds to the reasoning steps that are not related to the source sentences or have missed information\nprovided in the context.\nTo find irrelevant reasoning steps, we use alignment score to identify steps that are both not related to the context and not in\nthe reference chain (to avoid punishing for possibly relevant commonsense knowledge): 1 maxi=1..N ([1 \u2014 r-align(h \u2192\n.\ns)] \u00b7 [1 \u2014 r-align(h \u2192 r)]). Here, 1 is an all-ones vector, and (\u2022) is the element-wise product.\n-\nTo find chains that contain information that is not required to solve the problem (i.e., redundant steps), we identify those\nhypothesis steps that are least aligned with the the reference steps: min\u00bf=1..N r-align(hi \u2192r). This score punishes chains\nwith steps that are not required for the correct solution.\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-11-columbia.pdf": {
        "text": "\u25cf\nSemantic Similarity Metrics\nQuantify the degree of semantic equivalence between pieces of text\nConsiders text as a whole, rather than relying on text units\ncomparisons\nScore\nInformativeness-Chain\n(Info-Chain) (h\u2192s)\nRepetition-Step\n(b+hj)\nSemantic Coverage-\nChain (rh)\nDescription\nTable 4: Semantic similarity metrics (ROSCOE-SS).\nSimilar to Info-Step, this metric quantifies the degree of agreement between the hypothesis chain and the source and is\ncalculated as [1 + cos(h, s)]/2. We embed reasoning chain and source context as a whole, as opposed to using step-wise\nembeddings in *-Step types of metrics introduced in Table 3.\nMeasures repetition-related errors on the step level by checking if it paraphrases information already mentioned in the\nprevious steps: (1 \u2013 maxi=2..N maxj=1...i-1[cos(hi, hj)])/2. Unlike Repetition-Token, which is orderless and compares\nindividual tokens in pairs of steps, Repetition-Step considers step embeddings similarity and is more robust to changing\ncontexts.\n-\nReflects the overall degree of similarity between the reference and hypothesis chains, comparing reference and hypothesis\nembeddings as a whole: [1 + cos(r, h)]/2.\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-11-columbia.pdf": {
        "text": "Logical Inference Metrics\n\u2022 Measure logical errors between pieces of text\nUse a Natural Language Inference (NLI) model trained to classify\nhypothesis-context pairs into entailment, neutral, and\ncontradiction classes to infer the contradiction probability Pcontr\nScore\nSelf-Consistency\n(hihj)\nSource-Consistency\n(hs)\nDescription\nTable 5: Logical inference metrics (ROSCOE-LI).\n-\nMeasures logical entailment errors within the reasoning steps: 1 maxi=2..N maxj<i Pcontr (hi, hj). This metric will\npunish chains where there is a pair of steps that are likely to contradict each other.\nMeasures logical entailment errors between any generated reasoning hand the source context s: 1\nmax\u00bf=1..N maxj=1..T Pcontr (hi, Sj). Specifically, for each reasoning step we measure the probability that it contra-\ndicts any sentence in the context. We take the maximum probability of contradiction over all steps, following the logic that\na contradiction anywhere in the reasoning chain signals a failure of the overall argument.\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-11-columbia.pdf": {
        "text": "Language Coherence Metrics\n\u2022 Use perplexity PPL as scored by the GPT2-Large model and\nEnglish grammatical acceptability p_gram scores\nTable 6: Language coherence metrics (ROSCOE-LC).\nScore\nPerplexity-Chain (h)\nDescription\nAverage perplexity of all tokens in the generated reasoning steps: 1/PPL(h). The context used to score each token is\nthe previous tokens in the current and all previous steps. Steps are joined with a space character. To keep the range and\norientation consistent with the other scores we invert the perplexity.\nPerplexity-Step (hi)\nN\ni=0\nAverage perplexity of all tokens in the generated reasoning steps, where the context used to score each token is only the\nprevious tokens within the current step: 1/[(1/N) \u03a3 PPL(h;)]. To keep the range and orientation consistent with the\nother scores we invert the perplexity.\nGrammar (hi)\nProbability of grammatical acceptability of each step, averaged over all steps: (1/N) \u03a30 Pgram (hi).\nN\n=0\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-11-columbia.pdf": {
        "text": "LLMPerf\n\u2022 Tool for evaluating performance of LLM APIs\nTwo tests for evaluating LLMs:\n1. Load test - check for performance\n2. Correctness test \u2013 check for correctness\nhttps://github.com/ray-project/LLMPerf\n",
        "links": [
            "https://github.com/ray-project/LLMPerf"
        ]
    },
    "./chunks/chunk_42_Lecture-11-columbia.pdf": {
        "text": "\u2022\n.\nLLMPerf Load Test\nSpawns a number of concurrent requests to the LLM API and\nmeasures the inter-token latency and generation throughput per\nrequest and across concurrent requests\nPrompt send with each request\nRandomly stream lines from the following text. Don't generate eos tokens:\nLINE 1,\nLINE 2,\nLINE 3,\n...\n\u2022\n\u2022\nThe lines are randomly sampled from a collection of lines from Shakespeare\nsonnets.\nTokens are counted using the LlamaTokenizer regardless of which LLM API is\nbeing tested. This is to ensure that the prompts are consistent across\ndifferent LLM APIs.\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-11-columbia.pdf": {
        "text": "LLMPerf Correctness Test\n\u2022\nSpawns a number of concurrent requests to the LLM API with the\nfollowing format:\nConvert the following sequence of words into a number: {random_number_in_word_format}. Output just your final answer.\n\u2022\n\u2022\nrandom_number_in_word_format could be for example \"one\nhundred and twenty three\". The test then checks that the response\ncontains that number in digit format which in this case would be\n123.\nThe test does this for a number of randomly generated numbers and\nreports the number of responses that contain a mismatch.\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-11-columbia.pdf": {
        "text": "LLMPerf Leaderboard\nOutput tokens throughput (tokens/s)\n70B Models\nAnyscale \u54c1\n21\nBedrock\naws\nFireworks.ai\n40\n66\nFramework Model\nanyscale\nbedrock\nmeta-llama/Llama-2-70b-chat-hf\nmeta.llama2-70b-chat-v1\naccounts/fireworks/models/llama-\nfireworks\nv2-70b-chat\nGroq\ngroq\nLepton.ai\n33\n185\ngroq\nllama2-70b-4096\nH lepton llama2-70b\n\u0434\u0438\nPerplexity.ai \u2611\n10\nReplicate\nPeplicate\nTogether.ai together.al\nllama-2-70b-chat\nmeta/llama-2-70b-chat\nperplexity\nreplicate\ntogether\n2-70b-chat\ntogether_ai/togethercomputer/Ilama-\n0 10\n20\n30\n40\n40\n50\n60\n60\n70\n80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230\nhttps://github.com/ray-project/llmperf-leaderboard\n",
        "links": [
            "https://github.com/ray-project/llmperf-leaderboard"
        ]
    },
    "./chunks/chunk_45_Lecture-11-columbia.pdf": {
        "text": "LLMPerf Leaderboard\nTime to first token (seconds)\n70B Models\n0.21s\nAnyscale\n8b\nBedrock\naws\nFireworks.ai\nLepton.ai\n0.22s\nGroq\ngroq\nPerplexity.ai\nReplicate\nFeplicate\nTogether.ai together.al\n0.39s\n+\n0.51 s\n0.93s\nFramework\nanyscale\nbedrock\nfireworks\nModel\nmeta-llama/Llama-2-70b-chat-hf\nmeta.llama2-70b-chat-v1\naccounts/fireworks/models/llama-\nv2-70b-chat\nllama2-70b-4096\ngroq\n0.37s\nlepton\nllama2-70b\n1.19s\nperplexity\n0.63s\nreplicate\nllama-2-70b-chat\nmeta/llama-2-70b-chat\ntogether_ai/togethercomputer/llama-\n0.1s\n0.2s 0.3s 0.4s 0.5s 0.6s 0.7s 0.8s\n0.9s\n1s\n1.1s 1.2s 1.3s 1.4s 1.5s\n1.6s\ntogether\n2-70b-chat\nhttps://github.com/ray-project/llmperf-leaderboard\n",
        "links": [
            "https://github.com/ray-project/llmperf-leaderboard"
        ]
    },
    "./chunks/chunk_1_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Lecture 6 10/01/24\n+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\nParijat Dube and Chen Wang\n1\n+\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Operationalizing Machine Learning\n2\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Production Grade Machine Learning\nSystems\nData\nVerification\nConfiguration\nData Collection\nMachine\nResource\nManagement\nMonitoring\nServing\nInfrastructure\nML\nCode\nAnalysis Tools\nFeature\nExtraction\nProcess\nManagement Tools\nThe portion of ML training code in a production-grade ML\nsystem is a lot smaller than the technologies and processes\nneeded for supporting it.\n3\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Operationalizing Al\nML Models\nML Systems\nDeployed ML Service\nBusiness Value\nData Science & Al Lifecycle - a General View\nScope\nUnderstand\nBuild\n(Dev)\nML/DL assets\nConnect to Needed Data\n1. Analyze Data\n2. Prepare Data\nI\n3. Build & Train Model\nCode, Canvas\nRequirements\n1. Explore multiple\nbusiness ideas\n2. Assess Feasibility\n3. Prioritize and Select\nUse Cases...\n4. Establish Business\nKPIs for use cases\n5. Establish swim\nlanes for each use case\nData Assets\n1. All: Use Heterogeneous\nSources, Projects, General\nPolicies, etc.\n2. Provider: Discover, Track\nLineage, Catalog, Curate,\nClassify, Grant Access\n3. Steward: Create Policies\nfor Data Assets, Grant\nAccess\n4. Consumer: Find,\nUnderstand, Add, Explore,\nReview\n5. Make Explored Data\nAvailable for Collaboration\n\u0c17\u0c3f\u0c02\nTag for Review\n6. Publish to Catalog\n(models, notebooks)\n4. Evaluate Model\n5. Collaborate;\nDeploy and Run\n(QA and Prod)\nBusiness KPIs\n1. Review for Deploy\nafter code review, third\nparty oversight, unit tests\nTag Deployable Version\n2. Deploy Model to ML\nRuntime Engine\n3. Monitor & Evaluate\nmodel execution\n4. Manage\nView vs. thresholds\nModel versioning\nMonitor and Manage\n(QA and Prod)\nBusiness KPIs\nConfigure Model for\nMonitoring & Integrations\n(new and updates)\nObtain Model Insights\n1. Quality\n2. Perf. (throughput)\n3. Custom Metrics\n4. Fairness\n5. Explain Transactions\n(on-demand)\n\u0647\u0645\n3=\nData Steward\nData Engineer\nBusiness User\nData Scientist\nData Provider\nData Scientist\nSoftware\nEngineer\nData\nScientist\nBusiness\nUser\n\u0391\u0399\nOperations\nData Consumer\nCI/CD Pipeline\nBusiness\nUser\nIBM\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Devops principles in Software Engineering\nEnd-to-end traceability\nPLAN\nDEPLOY\nCODE\nTEST\nCONTINUOUS\nFEEDBACK\nMONITOR\nOPERATE\nReal-time collaboration\n5\nCONTINOUS\nINTEGRATION\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Software Engineering in ML Systems\nMachine learning applications run as pipelines that\ningest data, compute features, identify model(s),\ndiscover hyperparameters, train model(s), validate\nand deploy model(s).\nMaking a model as a production-capable web\nservice\n\u2022 Containerization (docker), cluster deployment (K8s)\nAPIs exposed as web service (Tensorflow\nserving/ONNX runtime)\n\u2022\nWorkflow engines (e.g., Kubeflow) to automate ML\npipeline\nDeployment monitoring and operational analytics\nDevops principles applicable to ML Systems:\n\u2022\n\u2022\n\u2022\nContinuous Integration, Continuous delivery (CI/CD)\nPredictability\n\"A model may be unexplainable\u2014but an API cannot be\nunpredictable\"\nReproducibility and Traceability\n\u2022\nProvenance for Machine Learning Artifacts\nEnd-to-end\nCODE\ntraceability\nPLAN\nCONTINOUS\nINTEGRATION\nTEST\nCONTINUOUS\nFEEDBACK\nDEPLOY\nMONITOR\nOPERATE\nReal-time collaboration\nML Specific testing and monitoring apart\nfrom traditional software testing\n\u2022\n\u2022\n\u2022\nData testing\nInfrastructure testing\nModel testing\nProduction testing\n6\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-6-columbia-Fall2024.pdf": {
        "text": "MLOps: The Assembly Line for ML\nML System\nDevelopment\n(Dev)\nML System\nOperations\n(Ops)\nMLOps is an ML engineering culture and practice that aims at unifying ML system development (Dev)\nand ML system operation (Ops)\nGoal: Accelerate model life cycle ( from development to deployment)\nMaintain high quality model in production\nApproach: automation and monitoring through development of tool-chain covering all steps of ML system\nconstruction, including development, integration, testing, releasing, deployment and infrastructure management.\nIs MLOps same as DevOps ?\nAn introduction to MLOps on Google Cloud\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-6-columbia-Fall2024.pdf": {
        "text": "\u2022\nML specific challenges to DevOps\nContinuous Integration (CI) is not only about testing and validating code\nand components, but also testing and validating data, data schemas,\nand models.\nContinuous Delivery (CD) is not only about a single software package or\na service, but a system (an ML training pipeline) that should\nautomatically deploy another service (model prediction service).\n\u2022 Continuous Training (CT) is a new property, unique to ML systems, that's\nconcerned with automatically retraining candidate models for testing\nand serving.\nContinuous Monitoring (CM) is not only about catching errors in\nproduction systems, but also about monitoring production inference\ndata and model performance metrics tied to business outcomes.\n80\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-6-columbia-Fall2024.pdf": {
        "text": "MLOps Principles\n(P2)\n(P3)\nWorkflow\n(P4)\n(P3\nOrchestration\nComponent\nModel\nRegistry\nP1\nP6\nSource Code\nRepository\nP6 CI/CD\n\u2117 Component\nModel Training\nInfrastructure\nMonitoring\nComponent\nModel Serving\nComponent\nPRINCIPLES\nP1 CI/CD automation\nP2 Workflow orchestration\nP3 Reproducibility\nP4 Versioning of data, code, model\nP5 Collaboration\nP6 Continuous ML training & evaluation\nP7 ML metadata tracking\nP8 Continuous monitoring\nP9 Feedback loops\nCOMPONENT\nP4\nFeature\nStores\nML Metadata\nStores\n9\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-6-columbia-Fall2024.pdf": {
        "text": "MLOps with CI/CD\nExperimentation/\nDevelopment\nCode\nRepository\nCode and\nconfigurations\nTraining\nPipeline CI/CD\nPipeline\nartifacts\nArtifact\nRepository\nPutting it all together\nEnd-to-end view\nContinuous\nTraining\nTrained\nmodel\nModel\nRegistry\nML Metadata\nModel\nDeployment CI/CD\nServing\nInfrastructure\nRepeatable and reliable pipelines\nLineage tracking of trained models\nModel\ndeployment\nServing and\nMonitoring\nServing\nlogs\nLogs\n10\n10\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-6-columbia-Fall2024.pdf": {
        "text": "MLOps Roles\nDS\nData Scientist\n(ML model development)\nIDE\nML\nBE\nBackend Engineer\n(ML infrastructure management)\nML Engineer/\nMLOps Engineer\n(cross-functional management\nof ML environment and assets:\nData Engineer\n(data management,\ndata pipeline management)\nML infrastructure,\nML models,\nML workflow pipelines,\ndata Ingestion,\nmonitoring)\nIDO\nDevOps Engineer\n(Software engineer with DevOps skills,\nML workflow pipeline orchestration,\nCI/CD pipeline management,\nmonitoring)\nSE\nSoftware Engineer\n(applies design patterns and\ncoding guidelines)\n11\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Kubeflow\n\u2022\nEcosystem of Kubernetes based components for each stage inthe AI/ML\nLifecycle\n\u2022 Kubeflow is an effort to standardize deployment of ML apps and managing\nthe entire lifecycle from development to production\n\u2022 Goal is to making deployments of machine learning (ML) workflows on\nKubernetes simple, portable and scalable\n\u2022 Multi-architecture, multi-cloud framework for running entire machine\nlearning pipelines\nOpen source; built on top of K8S\nhttps://www.kubeflow.org/docs/about/kubeflow/\n12\n",
        "links": [
            "https://www.kubeflow.org/docs/about/kubeflow/"
        ]
    },
    "./chunks/chunk_13_Lecture-6-columbia-Fall2024.pdf": {
        "text": "ML workflow using Kubefow \u2013 experimental phase\n-\nExperimental phase with Kubeflow\nIdentify problem\nand collect and\nanalyse data\nChoose an ML\nalgorithm and\ncode your model\nIterate tuning\nand training\nExperiment with\ndata and model\ntraining\nTune the model\nhyperparameters\nPyTorch\nJupyter Notebook\nscikit-learn\nFairing\nTensorFlow\nPipelines\nXGBoost\nKatib\n13\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-6-columbia-Fall2024.pdf": {
        "text": "ML workflow with Kubeflow-production phase\nProduction phase with Kubeflow\nIterate tuning\nand training\nTransform data\nTrain model\nChainer\nServe the model\nfor online/batch\nprediction\nMonitor the\nmodel's\nperformance\nKFServing\nMetadata\nMPI\nNVIDIA TensorRT\nTensorBoard\nMXNet\nPyTorch\nPyTorch\nTFServing\nTFJob\nSeldon\nPipelines\n14\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Kubeflow Logical Components\n\u2022 Central Dashboard\nKubeflow Notebooks\nKubeflow Pipelines\nKatib (for hyperparameter tuning and NAS)\n\u2022 Training Operators\nSpark Operator\n\u2022 Model Registry\n15\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Kubeflow Pipelines\nA platform for building, deploying, and managing multi-step ML\nworkflows based on Docker containers.\n\u2022 The Kubeflow Pipelines platform consists of:\n\u2022\n\u00b7\n\u2022\n\u2022\nA user interface (UI) for managing and tracking experiments and runs.\nAn engine for scheduling multi-step ML workflows.\nAn SDK for defining and manipulating pipelines and components.\n\u2022 Notebooks for interacting with the system using the SDK.\nhttps://www.kubeflow.org/docs/pipelines/pipelines-quickstart/\n16\n",
        "links": [
            "https://www.kubeflow.org/docs/pipelines/pipelines-quickstart/"
        ]
    },
    "./chunks/chunk_17_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Experiments >My XGBoost experiment\nMy first XGBoost run\nExperiments\nGraph\nRun output\nConfig\ndataproc-create-cl...\nPipeline\n\u2022\nDescription of ML workflow\nPipeline is formed using pipeline\ncomponents\nPipeline includes\n\u2022\nComponents and their inputs and\noutputs\n\u2022 Definition of the inputs (parameters)\nrequired to run the pipeline\nhttps://www.kubeflow.org/docs/pi\npelines/overview/pipelines-\noverview/\n\u4e09\u4fca Kubeflow\nPipelines\nArtifacts\nExecutions\nArchive\nBuild commit: ee207f2\ndataproc-transform\ndataproc-train-xgb...\nroc-curve\ndataproc-predict-w...\ndataproc-analyze\nconfusion-matrix\nonExit - dataproc-d...\nRuntime execution graph. Only steps that are currently running or have already completed are shown.\n17\n",
        "links": [
            "https://www.kubeflow.org/docs/pi"
        ]
    },
    "./chunks/chunk_18_Lecture-6-columbia-Fall2024.pdf": {
        "text": "\u00b7\nPipeline Components\nhttps://www.kubeflow.org/docs/pipelines/overview/concepts/comp\nonent/\n\u2022 A pipeline component is a self-contained set of user code,\npackaged as a Docker image, that performs one step in the pipeline.\n\u2022 A component for data preprocessing, data transformation, model\ntraining, data visualization...\n\u2022 A component is analogous to a function, in that it has a name,\nparameters, return values, and a body.\n\u2022 Each component in a pipeline executes independently.\n\u2022 The components do not run in the same process and cannot directly\nshare in-memory data.\n18\n",
        "links": [
            "https://www.kubeflow.org/docs/pipelines/overview/concepts/comp"
        ]
    },
    "./chunks/chunk_19_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Component Specification\nExample of a component specification\nA component specification takes the form of a YAML file, component.yaml. Below is an\nexample:\nname: xgboost4j Train classifier\ndescription: Trains a boosted tree ensemble classifier using xgboost4j\ninputs:\n- {name: Training data}\n- {name: Rounds, type: Integer, default: '30', help: Number of training rounds\noutputs:\n-\n{name: Trained model, type: XGBoost model, help: Trained XGBoost model}\nimplementation:\ncontainer:\nimage: gcr.io/ml-pipeline/xgboost-classifier-train@sha256:b3a64d57\ncommand: [\n/ml/train.py,\n--train-set, {inputPath: Training data},\n--rounds, {inputValue: Rounds},\n--out-model, {outputPath: Trained model},\nhttps://www.kubeflow.org/docs/pipelines/refe\nrence/component-spec/\nMetadata (name, description), input and\noutput interfaces, implementation (docker\nimage url)\nReal world component specifications\nLink here\n]\n19\n",
        "links": [
            "https://www.kubeflow.org/docs/pipelines/refe"
        ]
    },
    "./chunks/chunk_20_Lecture-6-columbia-Fall2024.pdf": {
        "text": "ML Commons\nMLCommons builds and measures the following benchmark suites:\nAl Safety Benchmarks\nThe MLCommons Al Safety benchmarks aim to assess the safety\nof Al systems.\nLearn more \u2192\nMLPerf Training\nThe MLPerf Training benchmark suite measures how fast systems\ncan train models to a target quality metric.\nLearn more \u2192\nML\nCommons\nMLPerf Inference: Mobile\nThe MLPerf Mobile benchmark suite measures how fast systems\ncan process inputs and produce results using\nLearn more \u2192\ntrained model.\nMLPerf Training: HPC\nThe MLPerf HPC benchmark suite measures how fast systems\ncan train models to a target quality metric.\nLearn more \u2192\nhttps://mlcommons.org\nMLPerf Inference: Tiny\nThe MLPerf Tiny benchmark suite measures how fast systems.\ncan process inputs and produce results using a trained model.\nLearn more \u2192\nMLPerf Inference: Datacenter\nThe MLPerf Inference: Datacenter benchmark suite measures\nhow fast systems can process inputs and produce results using a\ntrained model.\nLearn more \u2192\nMLPerf Storage\nThe MLPerf Storage benchmark suite measures how fast storage\nsystems can supply training data when a model is being trained.\nLearn more \u2192\nMLPerf Inference: Edge\nThe MLPerf Edge benchmark suite measures how fast systems\ncan process inputs and produce results using a trained model.\nLearn more \u2192\nAlgoPerf: Training Algorithms\nBenchmark Results\nThe AlgoPerf: Training Algorithms benchmark measures how\nmuch faster we can train neural network models to a given target\nperformance by changing the underlying training algorithm.\nLearn more \u2192\n125+\n6\nMLCommons Members and Affiliates\nBenchmark Suites\n56,000+\nMLPerf Performance Results to-date\n",
        "links": [
            "https://mlcommons.org"
        ]
    },
    "./chunks/chunk_21_Lecture-6-columbia-Fall2024.pdf": {
        "text": "\u2022\nBenchmark ML Training\nAfter an ML practitioner selects a data set, optimizer, and DNN model,\nthe system trains the model to its state-of-the-art quality (e.g., Top-1\naccuracy for image classification)\n\u2022 Provided the system meets this requirement, the practitioner can make\ndifferent operation, implementation, and numerical-representation\nchoices to maximize system performance\u2014that is, how fast the\ntraining executes.\n\u2022 An ML performance benchmark must ensure that systems under test\nachieve state-of-the-art quality while providing sufficient flexibility to\naccommodate different implementations.\n\u2022 Tradeoff between quality and performance is challenging because\nmultiple factors affect both the final quality and the time to achieve it.\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-6-columbia-Fall2024.pdf": {
        "text": "MLPerf Training\nBenchmark suite that measures how fast systems can train\nmodels to a target quality metric\nBenchmark\nImage classification\nObject detection\n(lightweight)\nInstance segmentation and\nobject detection (heavyweight)\nTranslation\n(recurrent)\nTranslation\n(nonrecurrent)\nRecommendation\nReinforcement learning\nData set\nImageNet\n(Deng et al., 2009)\nCOCO 2017\n(Lin et al., 2014)\nCOCO 2017\n(Lin et al., 2014)\nWMT16 EN-DE\n(WMT, 2016)\nWMT17 EN-DE\n(WMT, 2017)\nMovieLens-20M\n(GroupLens, 2016)\nGo\n(9x9 Board)\nModel\nResNet-50 v1.5\n(MLPerf, 2019b)\nSSD-ResNet-34\n(Liu et al., 2016)\nMask R-CNN\n(He et al., 2017a)\nGNMT\n(Wu et al., 2016)\nTransformer\n(Vaswani et al., 2017)\nNCF\n(He et al., 2017b)\nMiniGo\n(MLPerf, 2019a)\nQuality Threshold\n74.9% Top-1 accuracy\n21.2 mAP\n37.7 Box min AP,\n33.9 Mask min AP\n21.8 Sacre BLEU\n25.0 BLEU\n0.635 HR@10\n40.0% Professional move prediction\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-6-columbia-Fall2024.pdf": {
        "text": "MLPerf Inference\n\u2022\nBenchmark suite that measures how fast systems can\nprocess inputs and produce results using a trained model\nAREA\nTASK\nVISION\nIMAGE CLASSIFICATION (HEAVY)\nVISION\nIMAGE CLASSIFICATION (LIGHT)\nVISION\nOBJECT DETECTION (HEAVY)\nVISION\nOBJECT DETECTION (LIGHT)\nLANGUAGE\nMACHINE TRANSLATION\nREFERENCE MODEL\nRESNET-50 v1.5\n25.6M PARAMETERS\n8.2 GOPS INPUT\nMOBILENET-V1 224\n4.2M PARAMETERS\n1.138 GOPS / INPUT\nSSD-RESNET-34\n36.3M PARAMETERS\n433 GOPS/INPUT\nSSD-MOBILENET-V1\n6.91M PARAMETERS\n2.47 GOPS / INPUT\nGNMT\n210M PARAMETERS\nDATA SET\nIMAGENET (224x224)\nIMAGENET (224x224)\nCOCO (1,200x1,200)\nCOCO (300x300)\nWMT16 EN-DE\nQUALITY TARGET\n99% OF FP32 (76.456%) TOP-1 ACCURACY\n98% OF FP32 (71.676%) TOP-1 ACCURACY\n99% OF FP32 (0.20 MAP)\n99% OF FP32 (0.22 MAP)\n99% OF FP32 (23.9 SACREBLEU)\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-6-columbia-Fall2024.pdf": {
        "text": "MLPerf Storage\n\u2022\nBenchmark suite measures how fast storage systems can\nsupply training data when a model is being trained.\nhttps://mlcommons.org/benchmarks/storage/\n",
        "links": [
            "https://mlcommons.org/benchmarks/storage/"
        ]
    },
    "./chunks/chunk_25_Lecture-6-columbia-Fall2024.pdf": {
        "text": "\u2022\nTime to Accuracy (TTA) Metric\nTTA measures time for a system to train to a target, near-state-of-the-art\naccuracy level on a held-out dataset\n\u2022 TTA combines both generalization and speed\n\u2022 Dawnbench was the first multi-entrant benchmark competition to use the\nTTA metric\n\u25cf\nMLPerf benchmark also uses TTA as its primary metric\n\u2022 Entries compete to achieve target accuracy in the fastest time\n\u2022\n\u2022\n\u2022\nImagenet training from 30 mins to less than 2 mins\nVery large-scale distributed training, with large batch sizes, GPUs, CPUs, TPUs\nMajor companies Google, Intel, NVIDIA compete with optimized solutions with the\ngoal to reduce TTA\n\u2022 Entries provide an opportunity to study ML systems optimized heavily for training\nperformance\n25\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Identifying Optimal DL Architecture\nValidation Acc. (%)\n96\n94\n96\nResNet20\nResNet56\nResNet164 (S)\nResNet164 (B)\nAccuracy\nthreshold\nTime to\nthreshold\nFastest model\n91.8%\n1h 43m\nResNet20\nN\n\u09b2\u09a8\n93%\n4h 42m\nResNet56\nResNet164\n94%\n10h 6m\n(simple)\nResNet164\n2\n4\n6\n8\n10\n12\n14\n94.4%\n11h 42m\nTraining time (in hours)\n(bottleneck)\nFigure 6: Validation accuracy vs. training time for different ResNet architectures on CIFAR10. Horizontal lines\nindicate accuracy thresholds of 91.8%, 93%, 94%, and 94.4%. ResNet20, ResNet56, ResNet 164 (with simple build-\ning blocks), and ResNet 164 (with bottleneck building blocks) are fastest to the corresponding accuracy thresholds.\nFor lower accuracy thresholds, shallower architectures reach the threshold faster.\nColeman et al. DAWNBench: An End-to-End Deep Learning\nBenchmark and Competition. NIPS 2017\n26\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-6-columbia-Fall2024.pdf": {
        "text": "Training Cost vs Training Time\n86420\nTraining cost ($)\nGPUS 1 (BS = 128)\nGPUS 2 (BS = 256)\nGPUS 4 (BS = 512)\nGPUS 8 (BS=1024)\n=\nGPUS 8 (BS=512)\n1\n2\n3\n4\nTraining time (in hours)\n\u2022\n\u2022\nFigure 7: Training cost vs. training time for ResNet56 on the CIFAR10 dataset, using different numbers of GPUs,\nwith an accuracy threshold of 92.5%. The cost of training stays roughly the same, regardless of the number of\nGPUs used, until 8 GPUs. Training time scales almost linearly with the inverse of the number of GPUs.\nScaling from 1 to 4 GPUs\n\u2022\nTraining time scales perfectly linearly with the inverse of the number of GPUs used\nCost remains constant despite training time going down\nScaling to 8 GPUs\n\u2022\nIncrease in the cost of training\nTraining time does not decrease enough to counter the doubling in instance cost per unit time\n(related to scaling efficiency)\nColeman et al. DAWNBench: An End-to-End Deep Learning\nBenchmark and Competition. NIPS 2017\n27\n27\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-10-Columbia.pdf": {
        "text": "Lecture 11\nParijat Dube, Chen Wang\n+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\n+\n1\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-10-Columbia.pdf": {
        "text": "Agenda\nInstruction Finetuning\nSingle Task Fine Tuning\nMulti-task, Instruction Finetuning\nParameter Efficient Finetuning (PEFT)\nLow-Rank Adaptation of Large Language Models (LORA)\nSoft Prompt Tuning\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-10-Columbia.pdf": {
        "text": "Finetuning an LLM with\ninstruction prompts\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-10-Columbia.pdf": {
        "text": "In-context learning (ICL) - zero shot inference\nPrompt\nClassify this review:\nI loved this DVD!\nSentiment:\nModel\nLLM\nCompletion\nClassify this review:\nI loved this DVD!\nSentiment: eived a\nvery nice book review\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-10-Columbia.pdf": {
        "text": "In-context learning (ICL) - one/few shot inference\nPrompt\nModel\nClassify this review:\nI loved this DVD!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment:\nOne-shot or Few-shot Inference\nCompletion\nClassify this review:\nLLM\nI loved this DVD!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair...\nSentiment: Negative\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-10-Columbia.pdf": {
        "text": "Limitations of in-context learning\nClassify this review:\nI loved this movie!\nSentiment: Positive\nClassify this review:\nI don't like this chair.\nSentiment: Negative\nClassify this review:\nEven with\nmultiple\nexamples\n\u2022 In-context learning may\nnot work for smaller\nmodels LLM\n\u2022 Examples take up space\nin the context window\nThis sofa is so ugly.\nSentiment: Negative\nClassify this review:\nWho would use this product?\nSentiment:\nInstead, try fine-tuning\nthe model\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-10-Columbia.pdf": {
        "text": "LLM fine-tuning at a high level\nLLM pre-training\nTEXT [...]\nTEXT [...]\nTEXT [...]\nTEXT[...]\nTEXT [...]\nTEXT [...]\nTEXT[...]\nTEXT [...]\nTEXT [...]\nTEXT[...]\nModel\nPre-trained\nLLM\nGB - TB - PB\nof unstructured textual data\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-10-Columbia.pdf": {
        "text": "LLM fine-tuning at a high level\nLLM fine-tuning\nModel\nTask-specific examples\nModel\nPre-trained\nLLM\nTEXT [...], LABEL[...]\nTEXT[...], LABEL [...]\n\u2022 .\nTEXT[ ],\nTEXT [...],\n\u00b7\n\u2022\n\u22c5\nLABEL [ ]\nLABEL [...]\nTEXT [...], LABEL [...]\nFine-tuned\nLLM\nGB - TB\nof labeled examples for a specific\ntask or set of tasks\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-10-Columbia.pdf": {
        "text": "Using prompts to fine-tune LLMs with instruction\nLLM fine-tuning\nModel\nPre-trained\nLLM\nTask-specific examples\nPROMPT [...], COMPLETION [...]\nPROMPT [...], COMPLETION [...]\nPROMPT [...], COMPLETION [...]\nPROMPT [. ], COMPLETION [.\nPROMPT [...], COMPLETION [...]\n..\n\u2022\n]\nModel\nFine-tuned\nLLM\nSummarize the following text:\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\nTranslate this sentence to.\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-10-Columbia.pdf": {
        "text": "Using prompts to fine-tune LLMS with instruction\nLLM fine-tuning\nModel\nPre-trained\nLLM\nClassify this review:\nI loved this DVD!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment: Negative\nModel\nFine-tuned\nLLM\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-10-Columbia.pdf": {
        "text": "Using prompts to fine-tune LLMs with instruction\nLLM fine-tuning\nModel\nPre-trained\nLLM\nClassify this review:\nI loved this DVD!\nSentiment:\nPositive\nClassify this review:\nI don't like this\nchair.\nSentiment: Negative\nEach prompt/completion pair includes a\nspecific \u201cinstruction\" to the LLM\nModel\nFine-tuned\nLLM\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-10-Columbia.pdf": {
        "text": "Using prompts to fine-tune LLMS with instruction\nLLM fine-tuning\nModel\nTask-specific examples\nModel\nPre-trained\nLLM\n.. .\nPROMPT [...], COMPLETION [...]\nPROMPT [...], COMPLETION[...]\nPROMPT [...], COMPLETION [...]\nPROMPT [. ], COMPLETION[...]\nPROMPT [...], COMPLETION [...]\nFine-tuned\nLLM\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-10-Columbia.pdf": {
        "text": "Using prompts to fine-tune LLMs with instruction\nLLM fine-tuning\nModel\nPre-trained\nLLM\nTask-specific examples\nPROMPT [...], COMPLETION [...]\n\u2022\n. .\nPROMPT [. ], COMPLETION [...]\nPROMPT [. . .], COMPLETION [...]\nPROMPT [...], COMPLETION[...]\nPROMPT [...], COMPLETION [...]\nModel\nFine-tuned\nLLM\nSummarize the following text:\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-10-Columbia.pdf": {
        "text": "Using prompts to fine-tune LLMS with instruction\nLLM fine-tuning\nModel\nTask-specific examples\nPre-trained\nLLM\n.\n.\nPROMPT [...], COMPLETION [...]\nPROMPT [. ], COMPLETION [...]\nPROMPT [. . .], COMPLETION [...]\nPROMPT [...], COMPLETION[...]\nPROMPT [...], COMPLETION [...]\n\u2022\nModel\nFine-tuned\nLLM\nSummarize the following text:\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\nTranslate this sentence to.\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-10-Columbia.pdf": {
        "text": "Using prompts to fine-tune LLMS with instruction\nLLM fine-tuning\nModel\nTask-specific examples\nModel\n.\n]\nFine-tuned\nPre-trained\nLLM\nPROMPT [...], COMPLETION [...]\n.\n.\nPROMPT [. ], COMPLETION[.\nPROMPT [ .), COMPLETION[...]\nPROMPT [...], COMPLETION[...]\nPROMPT [...], COMPLETION [...]\nFull fine-tuning\nupdates all parameters\nLLM\nImproved\nperformance\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-10-Columbia.pdf": {
        "text": "Sample prompt instruction templates\nClassification / sentiment analysis\njinja: \"Given the following review: \\n{{review_body}}\\npredict the associated rating\\\u00a6\n\\ from the following choices (1 being lowest and 5 being highest)\\n-\u00af {{ answer_choices\\\nhighest)\\n-\u00af{{\u00afanswer_choices\\\n\\ | join('\\\\n- ') }} \\n|||\\n{{answer_choices [star_rating-1]}}\"\nText generation\njinja: Generate a {{star_rating}}-star review (1 being lowest and 5 being highest)\nabout this product {{product_title}}.\n{{review_body}}\n|||\nText summarization\njinja: \"Give a short sentence describing the following product review: \\n{{review_body}}\\\n\\ \\n\\n{{review_headline}}\"\nSource: https://github.com/bigscience-workshop/promptsource/blob/main/promptsource/templates/amazon_polarity/templates.yaml\n",
        "links": [
            "https://github.com/bigscience-workshop/promptsource/blob/main/promptsource/templates/amazon_polarity/templates.yaml"
        ]
    },
    "./chunks/chunk_17_Lecture-10-Columbia.pdf": {
        "text": "LLM fine-tuning process\nLLM fine-tuning\nPrepared instruction dataset\nTraining splits\nPROMPT [...], COMPLETION [...]\nPROMPT [...], COMPLETION [...]\nPROMPT [...], COMPLETION[...]\nPROMPT [...], COMPLETION [...]\nPROMPT [...], COMPLETION [...]\nTraining\nPROMPT [...], COMPLETION [...]\nValidation\nPROMPT [...], COMPLETION [...]\nTest\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-10-Columbia.pdf": {
        "text": "LLM fine-tuning process\nLLM fine-tuning\nPrepared instruction dataset\nPrompt:\nClassify this review:\nI loved this DVD!\nSentiment:\nModel\nLLM completion:\nClassify this review:\nI loved this DVD!\nSentiment: Neutral\nPre-trained\nLLM\nLabel:\nClassify this review:\nI loved this DVD!\nSentiment: Positive\nLoss: Cross-Entropy\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-10-Columbia.pdf": {
        "text": "LLM fine-tuning process\nLLM fine-tuning\nPrepared instruction dataset\nTraining splits\n. .\nPROMPT [...], COMPLETION [...]\nPROMPT [. ], COMPLETION [...]\nPROMPT [...], COMPLETION[...]\nPROMPT [. . .], COMPLETION [...]\nPROMPT [...], COMPLETION [...]\nPROMPT [\n\u22c5 .\n.], COMPLETION[...]\nTraining\nValidation\nvalidation_accuracy\nPROMPT [. . .], COMPLETION[...]\nTest\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-10-Columbia.pdf": {
        "text": "LLM fine-tuning process\nLLM fine-tuning\nPrepared instruction dataset\nTraining splits\nPROMPT [...], COMPLETION[...]\nPROMPT [...], COMPLETION [...]\nPROMPT [...], COMPLETION[...]\nPROMPT [...], COMPLETION[...]\nPROMPT [...], COMPLETION[...]\nTraining\nPROMPT [...], COMPLETION [...]\nValidation\n.\nPROMPT [. .), COMPLETION[...]\nTest\ntest_accuracy\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-10-Columbia.pdf": {
        "text": "LLM fine-tuning process\nModel\nModel\nPre-trained\nFine-tuned\nLLM\nLLM\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-10-Columbia.pdf": {
        "text": "LLM fine-tuning process\nModel\nModel\nPre-trained\nInstruct\nLLM\nLLM\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-10-Columbia.pdf": {
        "text": "Fine-tuning on a single task\nModel\nPre-trained\nSingle-task training dataset,\ne.g. summarization\nLLM\nSummarize the following text:\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\nModel\nInstruct\nLLM\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-10-Columbia.pdf": {
        "text": "Fine-tuning on a single task\nModel\nSingle-task training dataset,\nPre-trained\ne.g. summarization\nLLM\nSummarize the following text:\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\nOften, only 500-1000 examples\nneeded to fine-tune a single task\nModel\nInstruct\nLLM\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-10-Columbia.pdf": {
        "text": "Catastrophic forgetting\n\u2022 Fine-tuning can significantly increase the performance of a model on a\nspecific task...\nAfter fine-tuning\nPrompt\nClassify this review:\nI loved this DVD!\nSentiment:\nModel\nLLM\nCompletion\nClassify this review:\nI loved this DVD!\nSentiment: POSITIVE\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-10-Columbia.pdf": {
        "text": "Catastrophic forgetting\n\u2022 ...but can lead to reduction in ability on other tasks\nAfter fine-tuning\nPrompt\nWhat is the name of\nthe cat?\nCharlie the cat roamed\nthe garden at night.\nModel\nLLM\nCompletion\nWhat is the name of\nthe cat?\nCharlie the cat roamed\nthe garden at night.\nThe garden was\npositive.\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-10-Columbia.pdf": {
        "text": "How to avoid Catastrophic forgetting?\nFirst note that you may not have to!\n\u2022 Fine tune on multiple-tasks at the same time.\nConsider Parameter Efficient Fine Tuning (PEFT)\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-10-Columbia.pdf": {
        "text": "Multi-task, Instruction\nFinetuning\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-10-Columbia.pdf": {
        "text": "Multi-task, instruction fine-tuning\nModel\nInstruction fine-tune on many tasks\nSummarize the following text: 1\nPre-trained\nLLM\nRate this review:\nTranslate into Python code:\nIdentify the places:\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-10-Columbia.pdf": {
        "text": "Multi-task, instruction fine-tuning\nModel\nInstruction fine-tune on many tasks\nModel\nSummarize the following text:\nPre-trained\nLLM\nRate this review:\nInstruct\nLLM\nTranslate into Python code:\nIdentify the places:\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-10-Columbia.pdf": {
        "text": "Multi-task, instruction fine-tuning\nModel\nInstruction fine-tune on many tasks\nModel\nSummarize the following text:\nPre-trained\nLLM\nRate this review:\nInstruct\nLLM\nMany examples of each\nneeded for training\nTranslate into Python code:\nIdentify the places:\n[EXAMPLE TEXT]\n[EXAMPLE COMPLETION]\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-10-Columbia.pdf": {
        "text": "Instruction fine-tuning with FLAN\n\u2022 FLAN models refer to a specific set of instructions used to perform\ninstruction fine-tuning\nFLAN\n\"The metaphorical dessert to the main course of\npretraining\"\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-10-Columbia.pdf": {
        "text": "Instruction fine-tuning with FLAN\n\u2022 FLAN models refer to a specific set of instructions used to perform\ninstruction fine-tuning\nC\nFLAN\n(Fine-tuned\nLAnguage Net)\nT5\nFLAN-T5\nT5\nPALM\nFLAN-PALM\nPALM\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-10-Columbia.pdf": {
        "text": "FLAN-T5: Fine-tuned version of pre-trained T5 model\n\u2022 FLAN-T5 is a great, general purpose, instruct model\nSource: Chung et al. 2022, \"Scaling Instruction-Finetuned Language Models\"\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-10-Columbia.pdf": {
        "text": "FLAN-T5: Fine-tuned version of pre-trained T5 model\n\u2022 FLAN-T5 is a great, general purpose, instruct model\nTO-SF\n- Commonsense Reasoning,\n- Question Generation,\n- Closed-book QA,\n- Adversarial QA,\n- Extractive QA\nMuffin\n- Natural language inference,\n-Code instruction gen,\n-Code repair\n- Dialog context generation.\n- Summarization (SAMSum)\nCoT (reasoning)\n-Arithmetic reasoning,\n- Commonsense reasoning\n- Explanation generation,\n- Sentence composition,\n- Implicit reasoning,\nNatural Instructions\n- Cause effect classification,\n- Commonsense reasoning,\n- Named Entity Recognition,\n-Toxic Language Detection,\n- Question answering\n55 Datasets\n14 Categories\n193 Tasks\n69 Datasets\n27 Categories\n80 Tasks\n9 Datasets\n1 Category\n9 Tasks\nSource: Chung et al. 2022, \"Scaling Instruction-Finetuned Language Models\"\n372 Datasets\n108 Categories\n1554 Tasks\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-10-Columbia.pdf": {
        "text": "SAMSum: A dialogue dataset\nSample prompt training dataset (samsum) to fine-tune FLAN-T5 from pretrained T5\nDatasets: samsum Tasks:\ndialogue (string)\n\"Amanda: I baked cookies. Do you want some? Jerry: Sure!\nAmanda: I'll bring you tomorrow :-)\"\n\"Olivia: Who are you voting for in this election? Oliver:\nLiberals as always. Olivia: Me too!! Oliver: Great\"\n\"Tim: Hi, what's up? Kim: Bad mood tbh, I was going to do\nlots of stuff but ended up procrastinating Tim: What did...\nSummarization\nsummary (string)\nLanguages:\nEnglish\n\"Amanda baked cookies and will bring Jerry some tomorrow.\"\n\"Olivia and Olivier are voting for liberals in this\nelection. \"\n\"Kim may try the pomodoro technique recommended by Tim to\nget more stuff done.\"\nSource: https://huggingface.co/datasets/samsum, https://github.com/google-research/FLAN/blob/2c79a31/flan/v2/templates.py #L3285\n",
        "links": [
            "https://huggingface.co/datasets/samsum,",
            "https://github.com/google-research/FLAN/blob/2c79a31/flan/v2/templates.py"
        ]
    },
    "./chunks/chunk_37_Lecture-10-Columbia.pdf": {
        "text": "Sample FLAN-T5 prompt templates\n\"samsum\": [\n]\nL\n(\"{dialogue}\\n\\Briefly summarize that dialogue.\", \"{summary}\"\n(\"Here is a dialogue: \\n{dialogue}\\n\\nWrite a short summary!\n\"{summary}\"),\n(\"Dialogue: \\n{dialogue}\\n\\nWhat is a summary of this dialogue?'\n\"{summary}\"),\n(\"{dialogue}\\n\\nWhat was that dialogue about, in two sentences or less?\",\n\"{summary}\"),\n(\"Here is a dialogue: \\n{dialogue}\\n\\nWhat were they talking about?\",\n\"{summary}\"),\n(\"Dialogue: \\n{dialogue}\\nWhat were the main points in that \"\n\"conversation?\", \"{summary}\"),\n(\"Dialogue: \\n{dialogue}\\nWhat was going on in that conversation?\",\n\"{summary}\"),\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-10-Columbia.pdf": {
        "text": "Improving FLAN-T5's summarization capabilities\nShopBot\nHi there! How can I help\nyou today?\nOf course, I'd be happy\nto help you. When did\nyou buy your item?\nI need to return a pair of\njeans that I purchased\nT\nTwo weeks ago.\nT\nGoal: Summarize\nconversations to\nidentify actions to\ntake\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-10-Columbia.pdf": {
        "text": "Improving FLAN-T5's summarization capabilities\nFurther fine-tune FLAN-T5 with a domain-specific instruction dataset (dialogsum)\nDatasets: knkarthick dialogsum like 13\nTasks: Summarization Text2Text Generation\nText Generation Languages:\nEnglish Multilinguality: monolingual Size Categorie\nLanguage Creators: expert-generated Annotations Creators: expert-generated Source Datasets: original License: mit\nDataset card Files and versions\nCommunity\nDataset Preview\nSplit\ntrain (12.5k rows)\nid (string)\ndialogue (string)\n\"train_0\"\n\"train_1\"\n\"train_2\"\n\"Personl#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are\nyou here today? #Person20: I found it would be a good....\n\"#Person1#: Hello Mrs. Parker, how have you been?\n#Person2: Hello Dr. Peters. Just fine thank you. Ricky..\n\"#Person1: Excuse me, did you see a set of keys?\n#Person20: What kind of keys? #Person10: Five keys and a\nsmall foot ornament. Person20: What a shame! I didn't\nsee them. #Person14: Well, can you help me look for it?\nThat's my first time here. #Person2#: Sure. It's my\npleasure. I'd like to help you look for the missing keys.\n#Person10: It's very kind of you. #Person2#: It's not a\nbig deal. Hey, I found them. #Person1#: Oh, thank God! I\ndon't know how to thank you, guys. #Person2#: You're\nwelcome.\"\nsummary (string)\n\"Mr. Smith's getting a check-up, and Doctor Hawkins\nadvises him to have one every year. Hawkins'11 give some...\n\"Mrs Parker takes Ricky for his vaccines. Dr. Peters\nchecks the record and then gives Ricky a vaccine.\"\n\"#Person1@'s looking for a set of keys and asks for\nPerson20's help to find them.\"\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-10-Columbia.pdf": {
        "text": "Example support-dialog summarization\nPrompt (created from template)\nSummarize the following conversation.\nTommy: Hello. My name is Tommy Sandals, I have a reservati\nMike: May I see some identification, sir, please?\nTommy: Sure. Here you go.\nMike: Thank you so much. Have you got a credit card, Mr.\nSandals?\nTommy: I sure do.\nMike: Thank you, sir. You'll be in room 507, nonsmoking,\nqueen bed.\nTommy: That's great, thank you!\nMike: Enjoy your stay!\nSource: https://huggingface.co/datasets/knkarthick/dialogsum/viewer/knkarthick--dialo\n",
        "links": [
            "https://huggingface.co/datasets/knkarthick/dialogsum/viewer/knkarthick--dialo"
        ]
    },
    "./chunks/chunk_41_Lecture-10-Columbia.pdf": {
        "text": "Summary before fine-tuning FLAN-T5 with our dataset\nPrompt (created from template)\nSummarize the following\nconversation.\nTommy: Hello. My name is\nTommy Sandals, I have a\nreservation.\nMike: May I see some\nModel\nFLAN-T5\nCompletion (Summary)\nTommy Sandals has a reservation\nfor a room at the Venetian\nHotel in Las Vegas.\nTommy: That's great, thank\nyou!\nMike: Enjoy your stay!\nAdequate completion, but does not\nmatch human baseline.\nHuman baseline summary:\nTommy Sandals has got a\nreservation. Mike asks for his\nidentification and credit card\nand helps his check-in.\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-10-Columbia.pdf": {
        "text": "Summary after fine-tuning FLAN-T5 with our dataset\nPrompt (created from template)\nSummarize the following\nconversation.\nTommy: Hello. My name is\nTommy Sandals, I have a\nreservation.\nMike: May I see some\nTommy: That's great, thank\nyou!\nMike: Enjoy your stay!\nModel\nFLAN-T5\n(Fine-tuned)\nCompletion (Summary)\nTommy Sandals has a\nreservation and checks in\nshowing his ID and credit\ncard. Mike helps him to\ncheck in and approves his\nreservation.\nBetter summary,\nmore-closely matches\nhuman baseline.\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-10-Columbia.pdf": {
        "text": "Fine-tuning with your own data\nShopBot\nHi there! How can I help\nyou today?\nI need to return a pair of\njeans that I purchased\nOf course, I'd be happy\nT\nto help you. When did\nyou buy your item?\n0 0\nTwo weeks ago.\nT\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-10-Columbia.pdf": {
        "text": "Full fine-tuning of large LLMs is challenging\nLLM\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-10-Columbia.pdf": {
        "text": "Full fine-tuning of large LLMs is challenging\nLLM\nOptimizer states\nTrainable\nWeights\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-10-Columbia.pdf": {
        "text": "Full fine-tuning of large LLMs is challenging\nLLM\nTemp memory\nForward\nActivations\nGradients\n12-20x\nweights\nOptimizer states\nTrainable\nWeights\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nLLM\n\u2022\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nLLM\nLLM with most layers\nfrozen\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nSmall number of\ntrainable layers\nLLM\nLLM with most layers\nfrozen\n\u2022\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nLLM\n|||\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nLLM\nLLM with additional\nlayers for PEFT\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nNew trainable\nlayers\nLLM\nLLM with additional\nlayers for PEFT\n",
        "links": []
    },
    "./chunks/chunk_53_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nNew trainable\nlayers\nLLM\nLLM with additional\nlayers for PEFT\nFrozen Weights\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nNew trainable\nlayers\nLLM\nLLM with additional\nlayers for PEFT\nFrozen Weights\nTrainable\nweights\n",
        "links": []
    },
    "./chunks/chunk_55_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nNew trainable\nlayers\nLLM\nLLM with additional\nFrozen Weights\nOther\ncomponents\nTrainable\nweights\nlayers for PEFT\n",
        "links": []
    },
    "./chunks/chunk_56_Lecture-10-Columbia.pdf": {
        "text": "Parameter efficient fine-tuning (PEFT)\nNew trainable\nlayers\nLess prone to\ncatastrophic forgetting\nLLM\nLLM with additional\nFrozen Weights\nOther\ncomponents\nTrainable\nweights\nlayers for PEFT\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-10-Columbia.pdf": {
        "text": "Full fine-tuning creates full copy of original LLM per task\nGBs\nLLM\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-10-Columbia.pdf": {
        "text": "Full fine-tuning creates full copy of original LLM per task\nGBs\nLLM\nQA fine tune\nGBs\nQA\nLLM\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-10-Columbia.pdf": {
        "text": "Full fine-tuning creates full copy of original LLM per task\nGBs\nLLM\nQA fine tune\nSummarize\nfine tune\nGBs\nQA\nLLM\nGBs\nSummarize\nLLM\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-10-Columbia.pdf": {
        "text": "Full fine-tuning creates full copy of original LLM per task\nGBs\nLLM\nQA fine tune\nSummarize\nfine tune\nGBs\nQA\nLLM\nGBs\nSummarize\nLLM\nGBs\nGenerate fine\ntune\nGenerate\nLLM\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-10-Columbia.pdf": {
        "text": "PEFT fine-tuning saves space and is flexible\nGBs\nLLM\n",
        "links": []
    },
    "./chunks/chunk_62_Lecture-10-Columbia.pdf": {
        "text": "PEFT fine-tuning saves space and is flexible\nPEFT weights\nGBs\nLLM\nQA PEFT\n| MB\nMBs\n",
        "links": []
    },
    "./chunks/chunk_63_Lecture-10-Columbia.pdf": {
        "text": "PEFT fine-tuning saves space and is flexible\nPEFT weights\nQA PEFT\n|MB\nMBs\nGBs\nSummarize\nLLM\nMBs\nPEFT\n",
        "links": []
    },
    "./chunks/chunk_64_Lecture-10-Columbia.pdf": {
        "text": "PEFT fine-tuning saves space and is flexible\nPEFT weights\nQA PEFT\nMBs\nGBs\nSummarize\nLLM\nMBs\nPEFT\nGenerate\nMBs\nPEFT\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-10-Columbia.pdf": {
        "text": "PEFT fine-tuning saves space and is flexible\nPEFT weights\nQA PEFT\nMBs\nGBs\nSummarize\nGenerate\nLLM\nMBs\nPEFT\nLLM\nGenerate\nMBs\nPEFT\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-10-Columbia.pdf": {
        "text": "PEFT Trade-offs\nParameter Efficiency\nMemory Efficiency\nTraining Speed\nModel Performance\nInference Costs\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-10-Columbia.pdf": {
        "text": "PEFT methods\nSource: Lialin et al. 2023, \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\",\n",
        "links": []
    },
    "./chunks/chunk_68_Lecture-10-Columbia.pdf": {
        "text": "PEFT methods\nSelective\nSelect subset of initial\nLLM parameters to\nfine-tune\nSource: Lialin et al. 2023, \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\",\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-10-Columbia.pdf": {
        "text": "PEFT methods\nSelective\nSelect subset of initial\nLLM parameters to\nfine-tune\nReparameterization\nReparameterize model\nweights using a low-rank\nrepresentation\nLORA\nSource: Lialin et al. 2023, \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\",\n",
        "links": []
    },
    "./chunks/chunk_70_Lecture-10-Columbia.pdf": {
        "text": "PEFT methods\nSelective\nSelect subset of initial\nLLM parameters to\nfine-tune\nReparameterization\nReparameterize model\nweights using a low-rank\nrepresentation\nAdditive\nAdd trainable layers or\nparameters to model\nLORA\nSource: Lialin et al. 2023, \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\",\n",
        "links": []
    },
    "./chunks/chunk_71_Lecture-10-Columbia.pdf": {
        "text": "PEFT methods\nSelective\nSelect subset of initial\nLLM parameters to\nfine-tune\nReparameterization\nReparameterize model\nweights using a low-rank\nrepresentation\nAdditive\nAdd trainable layers or\nparameters to model\nAdapters\nLORA\nSource: Lialin et al. 2023, \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\",\n",
        "links": []
    },
    "./chunks/chunk_72_Lecture-10-Columbia.pdf": {
        "text": "PEFT methods\nSelective\nSelect subset of initial\nLLM parameters to\nfine-tune\nReparameterization\nReparameterize model\nweights using a low-rank\nrepresentation\nAdditive\nAdd trainable layers or\nparameters to model\nAdapters\nLORA\nSoft Prompts\nSource: Lialin et al. 2023, \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\",\n",
        "links": []
    },
    "./chunks/chunk_73_Lecture-10-Columbia.pdf": {
        "text": "PEFT methods\nSelective\nSelect subset of initial\nLLM parameters to\nfine-tune\nReparameterization\nReparameterize model\nweights using a low-rank\nrepresentation\nAdditive\nAdd trainable layers or\nparameters to model\nAdapters\nLORA\nSoft Prompts\nPrompt Tuning\nSource: Lialin et al. 2023, \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\",\n",
        "links": []
    },
    "./chunks/chunk_74_Lecture-10-Columbia.pdf": {
        "text": "Low-Rank Adaptation of Large\nLanguage Models (LORA)\n",
        "links": []
    },
    "./chunks/chunk_75_Lecture-10-Columbia.pdf": {
        "text": "Transformers: recap\nJ'aime l'apprentissage\nautomatique\nEncoder\n2345\n3425\n3853\nOutput\nSoftmax\noutput\nDecoder\nEmbedding\nEmbedding\nInputs\n",
        "links": []
    },
    "./chunks/chunk_76_Lecture-10-Columbia.pdf": {
        "text": "Transformers: recap\nOutput\nSoftmax\noutput\nEncoder\nDecoder\n2345\n3425\n3853\nEmbedding\nEmbedding\nInputs\n",
        "links": []
    },
    "./chunks/chunk_77_Lecture-10-Columbia.pdf": {
        "text": "Transformers: recap\nOutput\nSoftmax\noutput\nSelf-attention\nEncoder\nDecoder\nEmbedding\nEmbedding\nInputs\nSelf-attention\n",
        "links": []
    },
    "./chunks/chunk_78_Lecture-10-Columbia.pdf": {
        "text": "Transformers: recap\nOutput\nFeed forward\nnetwork\nSelf-attention\nEncoder\nSoftmax\noutput\nDecoder\nEmbedding\nEmbedding\nInputs\nFeed forward\nnetwork\nSelf-attention\n",
        "links": []
    },
    "./chunks/chunk_79_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\nEncoder\nEmbedding\n",
        "links": []
    },
    "./chunks/chunk_80_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\nWeights\napplied to\nembedding\nvectors\nEncoder\nEmbedding\n",
        "links": []
    },
    "./chunks/chunk_81_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\n1. Freeze most of the original LLM weights.\nSelf-attention\nWeights\napplied to\nembedding\nvectors\nEncoder\nEmbedding\n",
        "links": []
    },
    "./chunks/chunk_82_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\n1.\nFreeze most of the original LLM weights.\nEncoder\n2.\nInject 2 rank decomposition matrices\nEmbedding\n",
        "links": []
    },
    "./chunks/chunk_83_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\n1.\nEncoder\n2.\nFreeze most of the original LLM weights.\nInject 2 rank decomposition matrices\n3.\nTrain the weights of the smaller matrices\n+\nEmbedding\n",
        "links": []
    },
    "./chunks/chunk_84_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\n1.\nEncoder\n2.\n3.\nFreeze most of the original LLM weights.\nInject 2 rank decomposition matrices\nTrain the weights of the smaller matrices\n+\nEmbedding\nRank r is small\ndimension,\ntypically 4, 8... 64\n",
        "links": []
    },
    "./chunks/chunk_85_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\n1.\nFreeze most of the original LLM weights.\nEncoder\n2.\nInject 2 rank decomposition matrices\n3.\nTrain the weights of the smaller matrices\nSteps to update model for inference\n1. Matrix multiply the low rank matrices\nB\nA\nBxA\nEmbedding\n2. Add to original weights\n* + BxA\n",
        "links": []
    },
    "./chunks/chunk_86_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\n1.\nFreeze most of the original LLM weights.\nEncoder\n2.\nInject 2 rank decomposition matrices\n3.\nTrain the weights of the smaller matrices\nUpdated\nweights\n+\nBxA\nEmbedding\nSteps to update model for inference:\n1. Matrix multiply the low rank matrices\nB\u2b51 A\nBxA\n2. Add to original weights\n+ BxA\n",
        "links": []
    },
    "./chunks/chunk_87_Lecture-10-Columbia.pdf": {
        "text": "Concrete example using base Transformer as reference\nUse the base Transformer model presented by Vaswani et al. 2017:\nTransformer weights have dimensions d x k = 512 \u00d7 64\n\u2022\nSo 512 \u00d7 64 = 32,768 trainable parameters\n512\nIn LoRA with rankr = 8:\nA has dimensions r \u00d7 k =\n\u2022 B has dimensions d xr =\n.\n\u2191\n8 \u00d7 64 = 512 parameters\n512 \u00d7 8 = 4,096 parameters\n86% reduction in parameters to train!\n64\n512\n+\n64\n\u2193\n8\n",
        "links": []
    },
    "./chunks/chunk_88_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\nWeights\napplied to\nembedding\nvectors\nEncoder\n1. Train different rank decomposition\nmatrices for different tasks\n2. Update weights before inference\nTask A\n*\n+\nEmbedding\n",
        "links": []
    },
    "./chunks/chunk_89_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\nEncoder\nUpdated\nweights for\ntask A\n+\n1. Train different rank decomposition\nmatrices for different tasks\n2. Update weights before inference\nTask A\n*\nEmbedding\nTask B\n*\n+\n+\n",
        "links": []
    },
    "./chunks/chunk_90_Lecture-10-Columbia.pdf": {
        "text": "LORA: Low Rank Adaption of LLMs\nSelf-attention\nEncoder\nUpdated\nweights for\ntask B\n+\n1. Train different rank decomposition\nmatrices for different tasks\n2. Update weights before inference\nTask A\n*\nEmbedding\nTask B\n*\n+\n",
        "links": []
    },
    "./chunks/chunk_91_Lecture-10-Columbia.pdf": {
        "text": "Sample ROUGE metrics for full vs. LORA fine-tuning\nFLAN-T5\nDialog\nsummarization\nBase model\nROUGE\nFull fine-tune\n+80.63%\nROUGE\nflan_t5_base\n{'rouge1': 0.2334,\n'rouge2' 0.0760,\nBaseline\nrougeL': 0.2014,\n'rougeLsum': 0.2015}\nscore\n-3.20%\nLORA fine tune\nROUGE\nflan_t5_base_instruct_full\n{'rouge1'\n0.4216,\n'rouge2'\n0.1804,\nrougeL': 0.3384,\nrougeLsum': 0.3384}\nflan_t5_base_instruct_lora\n{'rouge1'\n'rouge2'\n0.4081,\n0.1633,\nrougeL': 0.3251,\n'rougeLsum': 0.3249}\n",
        "links": []
    },
    "./chunks/chunk_92_Lecture-10-Columbia.pdf": {
        "text": "Choosing the LoRA rank\n\u2022\nEffectiveness of higher rank\nRank r\nval loss BLEU NIST\nMETEOR ROUGE L CIDER\n1\n1.23\n68.72 8.7215\n0.4565\n0.7052\n2.4329\nappears to plateau\n2\n1.21\n69.17 8.7413\n0.4590\n0.7052\n2.4639\n4\n1.18\n70.38\n8.8439 0.4689\n0.7186\n2.5349\n8\n1.17\n69.57\n8.7457 0.4636\n0.7196\n2.5196\n16\n1.16\n69.61\n8.7483 0.4629\n0.7177\n2.4985\n\u2022 Relationship between rank\n32\n1.16\n69.33\n8.7736\n0.4642\n0.7105\n2.5255\n64\n1.16\n69.24\n8.7174 0.4651\n0.7180\n2.5070\n128\n1.16\n68.73\n8.6718 0.4628\n0.7127\n2.5030\n256\n1.16\n68.92\n8.6982 0.4629\n0.7128\n2.5012\n512 1.16\n68.78\n8.6857 0.4637\n0.7128\n2.5025\nand dataset size needs more\nempirical data\n1024\n1.17\n69.37\n8.7495 0.4659\n0.7149\n2.5090\nSource: Hu et al. 2021, \"LORA: Low-Rank Adaptation of Large Language Models\"\n",
        "links": []
    },
    "./chunks/chunk_93_Lecture-10-Columbia.pdf": {
        "text": "Prompt tuning with soft prompts\n",
        "links": []
    },
    "./chunks/chunk_94_Lecture-10-Columbia.pdf": {
        "text": "Prompt tuning is not prompt engineering!\nPrompt\nClassify this review:\nI loved this DVD!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair.\nSentiment:\nOne-shot or Few-shot Inference\nModel\nLLM\nCompletion\nClassify this review:\nI loved this DVD!\nSentiment: Positive\nClassify this review:\nI don't like this\nchair...\nSentiment: Negative\n",
        "links": []
    },
    "./chunks/chunk_95_Lecture-10-Columbia.pdf": {
        "text": "Prompt tuning adds trainable \"soft prompt\" to inputs\nSoft prompt\nX,\n\u2717\u2081\n\u2717\u2081\nSame length as\ntoken vectors\nX X X X X X X X\nTypically 20-100\ntokens\nThe teacher teaches the student with the book.\n",
        "links": []
    },
    "./chunks/chunk_96_Lecture-10-Columbia.pdf": {
        "text": "Soft prompts\nbook\nswim\nZ\nO fire\nwhale\njumps\n\u25cb fox\nEmbeddings of each token\nexist at unique point in\nmulti-dimensional space\n",
        "links": []
    },
    "./chunks/chunk_97_Lecture-10-Columbia.pdf": {
        "text": "Soft prompts\nZ\n\u3053\n\u041e\n",
        "links": []
    },
    "./chunks/chunk_98_Lecture-10-Columbia.pdf": {
        "text": "Full Fine-tuning vs prompt tuning\nWeights of model updated\nduring training\n",
        "links": []
    },
    "./chunks/chunk_99_Lecture-10-Columbia.pdf": {
        "text": "Full Fine-tuning vs prompt tuning\nWeights of model updated\nduring training\n",
        "links": []
    },
    "./chunks/chunk_100_Lecture-10-Columbia.pdf": {
        "text": "Full Fine-tuning vs prompt tuning\nWeights of model updated\nduring training\nWeights of model frozen and\nsoft prompt trained\nMillions to Billions of\nparameter updated\n10K 100K of parameters\nupdated\n",
        "links": []
    },
    "./chunks/chunk_101_Lecture-10-Columbia.pdf": {
        "text": "Switch out soft prompt at\ninference time to change task!\nPrompt tuning for multiple tasks\nTask A\nTask B\n----- ----\n",
        "links": []
    },
    "./chunks/chunk_102_Lecture-10-Columbia.pdf": {
        "text": "Performance of prompt tuning\nSuperGLUE Score\n100\n90\n96\n80\n08\n70\n60\nFull Fine-tuning\nMulti-task Fine-tuning\nX Prompt tuning\nPrompt engineering\n50\n108\n109\n1010\nNumber of Model Parameters\nSource: Lester et al. 2021, \"The Power of Scale for Parameter-Efficient Prompt Tuning\"\n1011\nPrompt tuning\ncan be as\neffective as full\nFine-tuning for\nlarger models!\n",
        "links": []
    },
    "./chunks/chunk_103_Lecture-10-Columbia.pdf": {
        "text": "Interpretability of soft prompts\nZ\nO fire\nO book\nswim\n\u25cb jumps\nfox\nTrained soft-prompt\nembedding does not\ncorrespond to a known\ntoken...\nwhale\n",
        "links": []
    },
    "./chunks/chunk_104_Lecture-10-Columbia.pdf": {
        "text": "Interpretability of soft prompts\ncompletely\ntotally\naltogether\nentirely\n100%\n...but nearest neighbors\nform a semantic group\nwith similar meanings.\n",
        "links": []
    },
    "./chunks/chunk_105_Lecture-10-Columbia.pdf": {
        "text": "PEFT methods summary\nSelective\nSelect subset of initial\nLLM parameters to\nfine-tune\nReparameterization\nReparameterize model\nweights using a low-rank\nrepresentation\nAdditive\nAdd trainable layers or\nparameters to model\nAdapters\nLORA\nSoft Prompts\nPrompt Tuning\n",
        "links": []
    },
    "./chunks/chunk_106_Lecture-10-Columbia.pdf": {
        "text": "Key takeaways\nInstruction fine-tuning\n.\nPrompt templates and data sets\nfor FLAN-T5\nPEFT\n. LORA\n\u25cf\nPrompt Tuning\nQuantization + LORA = QLORA\nO\n",
        "links": []
    },
    "./chunks/chunk_107_Lecture-10-Columbia.pdf": {
        "text": "ROUGE-L-sum\nThe ROUGE-L-SUM score is a variant of the ROUGE (Recall-\nOriented Understudy for Gisting Evaluation) metric,\nwhich is widely used to evaluate the quality of generated text,\n\u2022 especially in tasks like summarization,\nwhere the goal is to measure how well the generated summary\nmatches a reference summary.\n",
        "links": []
    },
    "./chunks/chunk_108_Lecture-10-Columbia.pdf": {
        "text": "Key Components\nROUGE-L focuses on the Longest Common Subsequence (LCS):\n\u2022\nInstead of comparing n-grams like ROUGE-N, ROUGE-L evaluates the\nlongest subsequence of words that appears in the same order in both the\ngenerated and reference texts.\nLCS captures the sentence-level structure, rewarding fluency and\ncoherence.\n\u2022 SUM (Summarization-specific variant):\n.\nROUGE-L-SUM is tailored for summarization tasks, ensuring the metric\nemphasizes overlap in content that is crucial for summaries, such as\nmain ideas and key details.\n\u2022 It typically considers sentence-level LCS rather than a document-level\nLCS.\n",
        "links": []
    },
    "./chunks/chunk_109_Lecture-10-Columbia.pdf": {
        "text": "Key Components\n\u2022\nComponents Evaluated:\n\u2022\nPrecision: How much of the LCS in the generated text aligns with the\nreference text.\n\u25cf\nP =\nLCS\nLength of generated summary\nRecall: How much of the reference text's content is captured in the LCS.\nR\n=\nLCS\nLength of reference summary\nF1 Score: The harmonic mean of precision and recall, offering a balanced\nevaluation.\n2 x P x R\nF1\nP+R\n",
        "links": []
    },
    "./chunks/chunk_110_Lecture-10-Columbia.pdf": {
        "text": "Applications\nUsed in research papers and benchmarks for summarization\nmodels (e.g., abstractive or extractive summarization models).\n\u2022 A good ROUGE-L-SUM score indicates that the generated\nsummary captures important content in a structured and\ncoherent way.\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Lecture 4 09/24/24\n+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\nParijat Dube and Chen Wang\n1\n+\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Today's Agenda\nJob Scheduling on DL Clusters\nAutomated ML\n\u2022 Neural Architecture Search\n\u2022\nHyperparameter Optimization\nOpen Neural Network Exchange (ONNX)\n\u2022 Introduction to Transfer learning\n2\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Scheduling Pods in K8s\n\u2022 Pods are matched to Nodes so that\n\u2022\nkubelet can run them\nSteps in Scheduling\n1. Scheduler watches for newly\ncreated Pods that have no Node\nassigned\n2. Scheduler finds the best Node for\nthat Pod to run on\nkube-scheduler - default scheduler\n\u2022\nin K8s\nCONTROL PLANE\ncloud-control-manager\netcd\nkube-api-server\nCLUSTER\nNode 1\nNode 2\nkubelet\nkube-proxy\nkubelet\nkube-proxy\nscheduler\nController Manager\npod\npod\nkube-scheduler\nkube-controller-manager\npod\npod\nCRI\nCRI\n3\nCLOUD PROVIDER API\n",
        "links": []
    },
    "./chunks/chunk_4_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Node selection in kube-scheduler\n\u2022\nkube-scheduler selects a node for the pod in a 2-step operation:\n1. Filtering\n2. Scoring\nFiltering: find the set of feasible Nodes to schedule the Pod\n\u2022\nCheck whether a candidate Node has enough available resources to meet a\nPod's specific resource requests\nScoring: scheduler scores the feasible Nodes for Pod placement\n\u26ab scheduler assigns a score to each Node that survived filtering, basing this\nscore on the active scoring rules\nkube-scheduler assigns the Pod to the Node with the highest ranking\n4\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Job Scheduling: Spread vs Pack\n\u2022 Spread\n\u2022\nDefault placement policy of K8s pods\nSpread distributes pods over the cluster nodes,\nand avoids placing two pods belonging to the same\njob on the same physical machine\nProblems:\n\u2022 Increased communication costs\n# of jobs arriving\n1400-\nJob Arrival by Day\n1200-\n1000-\n800-\n600-\n400-\n200-\n0\n10\n20\n30\n40\n50\n60\nDay #\n(a) Arrival of jobs by day over a 60 day period at a production\ncluster\n\u2022\nIncreased fragmentation\n\u2022\nPack\nPods from a DL job are packed into as few physical\nmachines (pod consolidation) as possible\nImplemented as an extension to K8S scheduler\nPack results in significantly fewer jobs queued for more than 15 minutes\nover 3x fewer queued jobs\n% of jobs queued\n20-\n\u0bae\u0bcd\nwww\nspread\npack\n10\n20\n30\n40\n50\n60\nDay #\n(b) Percentage of arriving jobs that would be queued for over\n15 mins, in SPREAD VS. PACK\n5\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-4-columbia-Fall2024.pdf": {
        "text": ".\u2022\nGang Scheduling\nIdeally a job should either be fully scheduled or\nfully queued\nDL job can have multiple pods\n\u2022 K8S scheduler doesn't consider one whole job\nwhile scheduling, it considers each of the learner\n\u2022\n\u2022\npods individually\nScheduling pods of a job individually can cause\ntemporary deadlocks; learners are waiting and\nholding hardware resources while waiting for\nother pods (learners) of the job to get deployed\nGPU held by the learner is idle because training has not\nstarted\n\u2022\nGang scheduler schedules all pods that belong to\na DL job holistically, as a group/gang\nprobability\n1.0-\n0.8-\n0.6-\n0.4-\n0.2-\n0\n50 jobs, 2 L x 1 GPU/L\n50 jobs, 2 L x 2 GPU/L\n50 jobs, 4 L x 1 GPU/L\ngang scheduling\n10\n20\n30\n40\n50\n60\n# of temporarily deadlocked learners\n(a) CDF of the probability of temporarily deadlocked learners\nwith and without gang scheduling\nprobability\n1.0-\n0.8-\n0.6-\n0.4-\n50 jobs, 2 L x 1 GPU/L\n50 jobs, 2 L x 2 GPU/L\n50 jobs, 4 L x 1 GPU/L\n0.2-\ngang scheduling\n0\n10\n20\n30\n40\n50\n60\n% of idle GPUs\n(b) CDF of the probability of idle GPUs with and without gang\nscheduling\n6\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Problem with current scheduling of Deep\nLearning Training (DLT) jobs on clusters\n\u2022 Cloud operators and large companies that manage clusters of tens of\nthousands of GPUs rely on cluster schedulers to ensure efficient\nutilization of the GPUs.\n\u2022 A typical cluster scheduler, such as Kubernetes or YARN (for Hadoop\nclusters) treats a DL job simply as a yet another big-data job\n\u2022 DLT jobs are assigned a fixed set of GPUs at startup\n\u2022\n\u2022 Job holds exclusive access to its GPUs until completion\nHead-of-line blocking, preventing early feedback and resulting in high queuing\ntimes for incoming jobs\n\u2022\nLow GPU utilization\nTypical cluster schedulers treat DLT job as a black box\n7\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-4-columbia-Fall2024.pdf": {
        "text": "DLT Job Features\nShort jobs vs long jobs on a DLT cluster\nFeedback-driven exploration\n\u2022\nHyperparameter search; manual or automated\nwith identified right hyperparameters\n\u2022 Do not need to run jobs to completion\n\u2022\nRequire early indication of performance\n.\nHead-of-line-blocking, as long-running jobs hold exclusive access to\nthe GPUs until completion, while multi-jobs depending on early\nfeedback wait in queue\nDLT jobs are heterogeneous targeting diverse domains\n\u2022\n\u26ab Jobs widely differ in terms of memory usage, GPU core utilization, sensitivity\nto interconnect bandwidth, and/or interference from other jobs\n80\n",
        "links": []
    },
    "./chunks/chunk_9_Lecture-4-columbia-Fall2024.pdf": {
        "text": "DL Jobs: Sensitivity to Locality\nPerformance of a multi-GPU DLT job depends on the affinity of\nDual Root System\nCPU\nCPU\nLOM\n3 UPI\nPCle\nSwitch\nPCle\nSwitch\nPCle\nSwitch\nPCle\nSwitch\nGPU\nGPU GPU GPU GPU GPU GPU GPU\nthe allocated GPUs\nSingle node setting\nDistributed setting\n\u2022\nSamePCIeSw \u2610 SameSocket \u2610 DiffSocket\nLocal 4-GPU\n2*2-GPU N 4* 1-GPU Z\n800\n700\n\u2022\n0.9\n0.8\n0.7\n0.6\n0.5\nVGG16\n600\n500\n400\n300\n200\n100\n0\nResNet-50\nResNet-50 Inception V3\nDiffSocket: GPUs on\ndifferent CPU sockets\nSameSocket: GPUs in\nthe same CPU socket,\nbut on different PCle\nswitches\nSamePCleSw: GPUs on\nthe same PCle switch\ninterconnected with a\n40G InfiniBand network\n\u2022\nFigure 1: Intra-server locality. Figure 2: Inter-server locality.\nDifferent DLT jobs exhibit different levels of sensitivity to inter-GPU affinity. Even for GPUs on the same\nmachine, we observe different levels of inter-GPU affinity due to asymmetric architecture\n\u2022\nSensitivity is model dependent; VGG16 is a larger neural model than ResNet-50\n\u2022\nModel synchronization in each mini-batch incurs a higher communication load on the underlying PCIe bus.\nDLT scheduler needs to take into account a job's sensitivity to locality when allocating GPUs\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep Learning. OSDI 2018\n9\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-4-columbia-Fall2024.pdf": {
        "text": "DL Jobs: Sensitivity to Interference\nIntra-server interference\nLM Other\nInter-server interference\n1 Job 2 Jobs\n4 Jobs \u2611\n\u2022\n\u2022\n\u2022\n\u2022\n0.9\n0.8\n0.7\n0.6\n0.5\nLM\nGNMT ResNet-50\n0.9\n0.8\n0.7\n0.6\n0.5\n\u2022\n.\n2 4-GPU servers\nconnected by\n40GB InfiniBand\nnetwork\n2-GPU jobs with\neach GPU on a\ndifferent server\nModels co-located with LM\nFigure 3: 1-GPU interference.\nResNet-50\nInception V3\nDeepSpeech\nFigure 4: NIC interference.\nJobs interfere caused by resource contention due the presence of multiple jobs\nDifferent DLT jobs exhibit different degrees of interference\nInterference exists both for single-GPU and multi-GPU jobs\nWhen running multiple 2-GPU jobs, where each GPU is placed on different server, ResNet-50 shows\nup to 47% slowdown, Inception V3 shows 30% slow-down\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep\nLearning. OSDI 2018\n10\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-4-columbia-Fall2024.pdf": {
        "text": "GPU Memory Used (GB)\n25\n20\n15\n10\nDL Jobs: Intra Job Predictability\n0 5 10 15\n20\n77x\nGPU Memory Used (GB)\n55453525\n1.5\n05\n10\n15\n20\nTime (seconds)\n(a) ResNet50/Imagenet\nTime (seconds)\n(b) GNMT/WMT' 14 En-De\nFigure 5: GPU memory usage during training.\n\u2022\n\u2022\n\u2022\n\u2022\nGradient descent algorithm performing\nmany mini-batch iterations\n\u2022\nCyclic pattern in GPU memory used\nEach cycle corresponds to one mini-batch\nprocessing\nLeveraging predictability\n\u2022\n\u2022\nDefine micro-tasks (collection of\ncycles) as scheduling units\nVery efficient suspend/resume and\nmigration by performing them at\nend of cycle when GPU memory\nusage is minimum\nMini-batch progress rate can be profiled\nand used as proxy to evaluate the\neffectiveness of applying performance\noptimization mechanisms\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep\nLearning. OSDI 2018\n11\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Gandiva Scheduler\n\u2022 Goals\nEarly feedback\n.\nGandiva supports over-subscription by allocating GPUs to a new job immediately and using\nthe suspend-resume mechanism to provide early results\nCluster efficiency\n.\nThrough a continuous optimization process that uses profiling and a greedy heuristic that\ntakes advantage of mechanisms such as packing, migration, and grow-shrink\n\u2022 Modes of operations\n\u25cf\n\u2022\n\u26ab Reactive\nIntrospective\nScheduling framework to exploit the unique characteristics of the deep\nlearning workload\n12\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Gandiva Mechanisms\n\u2022 3 ways to remove exclusivity and fixed assignment of GPUs to DLT jobs\n\u2022 Time-sharing GPUs: During overload, instead of waiting for current jobs to depart,\nGandiva allows incoming jobs to time-share GPUs with existing jobs. This is enabled\nusing a custom suspend-resume mechanism tailored for DLT jobs along with\nselective packing.\n\u2022 Job migration: Gandiva supports efficient migration of DLT jobs from one set of\nGPUs to another. Migration allows time-sliced jobs to migrate to other (recently\nvacated) GPUs or for defragmentation of the cluster so that incoming jobs are\nassigned GPUs with good locality.\n\u2022 GPU grow-shrink: Gandiva supports a GPU grow-shrink mechanism so that idle\nGPUs can be used opportunistically. In order to support these mechanisms efficiently\nand enable effective resource management, Gandiva introspects DLT jobs by\ncontinuously profiling their resource usage and estimating their performance.\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep\nLearning. OSDI 2018\n13\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Gandiva Mechanisms\nTraditional GPU allocation\nGandiva: (2) Migration\nJob 1\nJob 1\nJob 2\nJob 2\nGPU O\nGPU 1\nMigration\nJob 2\nGPU 2 GPU 3\nGPU O\nGPU 1\nGPU 2\nGPU 3\nGandiva: (1) Suspend-Resume/Packing\nSuspend\nJob 3\nJob 1\nGPU O GPU 1\nGPU 2\nJob 4\nGPU 3\nJob 2\nResume\nGrow\nGandiva: (3) Grow-Shrink\nJob 1\nJob 2\nJob 1\nJob 3\nShrink T\nJob 1\nGPU 0\nGPU 1\nGPU 2\nGPU 3\nFigure 6: GPU usage options in Gandiva.\nadds custom support for GPU time-slicing.\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep\nLearning. OSDI 2018\n14\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Job Migration\nCo-locate jobs after a departure\nServer0\nServer1\n0000\n8886 8680\nJobo slot\nOtherJob's slot\nDeepSpeech slot\nServer3\nServer4\nServer5\nServer6\nMigration helpful in\ni) Moving time-sliced jobs to vacated GPUs\nanywhere in the cluster;\nii) Migrating interfering jobs away from each\nother;\niii) Defragmentation of the cluster so that\nincoming jobs get GPUs with good locality.\nServer2\nMigrate\nFigure 8: Job migration in a shared cluster.\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep\nLearning. OSDI 2018\n15\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Gandiva Performance: Time Slicing\nEach of the long job\nshare of GPU time drop to 4/6\nMinibatches per 6 min\n4000\n3500\n3000\n2500\n2000\nTime Slicing\n14000\n12000\n10000\n$8000\n6000\n4000\n1500\nlong 1\nshort 1\n1000\nlong 2\nshort 2\nlong 3\nNet throughput\n500\n2000\nlong 4\n0\n0\n0\n25\n50\n75\n100\n125\n150\n175\nTime (in min)\nTotal minibatches per 6 min\nFigure 9: Time slicing six 1-GPU jobs on 4 GPUs.\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep\nLearning. OSDI 2018\n16\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-4-columbia-Fall2024.pdf": {
        "text": "GPU Memory Utilization Minibatches per 30s\n2000\n0\nGandiva Performance: Packing\nPacking\njob 1\nNet throughput\n<job 2\n0\n10\n20\n30\n40\nTime (in min)\nGPU Stats\n4000\nMemory\nCore utilization\n2000\n0\n10\n20\ni\n30\n40\nTime (in min)\n50\n50\n1.0\n0.5\n0.0\n10\n50\n50\nGPU Core Utilization\nJobs are initially being time- sliced on the\nsame GPU. After some time, the scheduler\nconcludes that their memory and GPU\ncore utilization is small enough that\npacking them is feasible and schedules\nthem together on the GPU. The scheduler\ncontinues to profile their performance.\nBecause their aggregate performance\nimproves, packing is retained; otherwise\n(not shown), packing is undone and the\njobs continue to use time-slicing.\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep\nLearning. OSDI 2018\n17\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Gandiva Performance: Grow-Shrink\nMinibatches per 6 min\nGrow-Shrink\n9000\nJob shrinks and grows\nelastic job\n8000\nshort 1\nshort 2\nshort 3\nshort 4\n7000\n6000-\n5000\n4000\n3000\n2000\n1000\n0\n0\n20\n40\n60\n80\n100\n120\nTime (in min)\nFigure 11: Grow from 1 to 4 GPUs, Shrink to 1-GPU.\n4-P100 server\nXiao et al. Gandiva: Introspective Cluster Scheduling for Deep\nLearning. OSDI 2018\n18\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Automated Machine Learning\nNeural network identification\nHyperparameter optimization\nInput\nImage\nPre-\nProcessing\nNeural Network\nOutputs\nData Preprocessing: normalization, data-augmentation\n\u2022 Neural architecture search (NAS)\n\u2022\n\u2022 Standard, off the shelf\n\u2022\nSynthesize a new\n.\n\u2022\nTypes of layers (conv, maxpool), number of different layers, how to stack the layers\nConvolution layer parameters (filter dim, stride dim)\nHyperparameter optimization\n\u2022\nBatch size, learning rate, momentum\n\u2022 NAS and hyperparameter optimization can be done jointly or sequentially 19\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Neural Architecture Search\n20\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Neural Architecture Search\nDefines the neural architectures a NAS\napproach may discover\narchitecture\n\u0410\u0415\u0410\nSearch Space\nA\nSearch Strategy\nPerformance\nEstimation\nStrategy\nperformance\nestimate of A\nFigure 1: Abstract illustration of Neural Architecture Search methods. A search strategy\nselects an architecture A from a predefined search space A. The architecture is\npassed to a performance estimation strategy, which returns the estimated perfor-\nmance of A to the search strategy.\nElsken et al. Neural Architecture Search: A Survey. JMLR 2019\n21\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-4-columbia-Fall2024.pdf": {
        "text": "NAS Search Space: Layer Based\nMutli branch networks with skip connections\nChain structured networks\ninput\nH++\n......\ninput\nLo\nLo\nL\u2081\nL2\nL5\nL6\nL7\nLn-1\nL8\nL9\nHHI\noutput\nL10\noutput\nProblem with layer based NAS search space\n\u2022\nSearch space is of the order of layer types and\nassociated parameters\n\u2022\nConv layer: filter size, number of filters, stride\nlength, padding\nFigure 2: An illustration of different architecture spaces. Each node in the graphs cor-\nresponds to a layer in a neural network, e.g., a convolutional or pooling layer.\nDifferent layer types are visualized by different colors. An edge from layer Li to\nlayer L; denotes that L; receives the output of L; as input. Left: an element of a\nchain-structured space. Right: an element of a more complex search space with\nadditional layer types and multiple branches and skip connections.\nElsken et al. Neural Architecture Search: A Survey. JMLR 2019 22\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-4-columbia-Fall2024.pdf": {
        "text": "NAS Search Space: Cell Based\nNormal cell\nH\ninput\nReduction cell\noutput\ninput\nH\nHHH\noutput\nH\noutput\nHHH\nHHHH\ninput\nHHH\nMotivated by hand-crafted architectures consisting\nof repeated motifs (cells/blocks)\n\u2022\nInception blocks: normal cell, reduction cell\nResidual blocks: normal cell reduction cell\nFigure 3: Illustration of the cell search space. Left: Two different cells, e.g., a normal cell\n(top) and a reduction cell (bottom) (Zoph et al., 2018). Right: an architecture\nbuilt by stacking the cells sequentially. Note that cells can also be combined in a\nmore complex manner, such as in multi-branch spaces, by simply replacing layers\nwith cells.\nElsken et al. Neural Architecture Search: A Survey. JMLR 2019\n23\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-4-columbia-Fall2024.pdf": {
        "text": "NAS Search Space: Cell Based\nAdvantages of cell-based NAS search space\n\u2022\n\u2022\nDrastically reduced search space compared to layer based\nEasy adaptability of a network for across datasets of varying size\nand complexity a given domain\nApplicability across different domains, LSTM blocks in RNN,\ninception and residual blocks in CNNs\n\u2022 Macro-architecture search problem: how many cells shall be used\nand how should they be connected to build the actual model?\n24\n14\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-4-columbia-Fall2024.pdf": {
        "text": "NAS Search Strategy\nRandom search\n\u2022 Bayesian optimization\n.\nEvolutionary algorithms (EA)\nReinforcement learning (RL)\n\u2022 Gradient based methods\n25\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-4-columbia-Fall2024.pdf": {
        "text": "EA based NAS search\nTournament selection,\nall, youngest, elitist (best fitness)\nfitness proportion selection\nParent Selection\nParents\nInitialization\nSurvivor\nPopulation\nSelection\nRecombination & Mutation\nOffspring\nFitness Evaluation\nA general framework for evolutionary algorithms\nWitsuba et al. A Survey on Neural Architecture Search. 2019\n26\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-4-columbia-Fall2024.pdf": {
        "text": "RL based NAS search\n\u2022\nSpecify the structure and connectivity of a neural network by using a\n\u2022\nconfiguration string (e.g., [\"Filter Width: 5\", \"Filter Height: 3\", \"Num Filters: 24\"])\nZoph and Le (2017): Use a RNN (\"Controller\") to generate this string that\nspecifies a neural network architecture\n\u2022 Train this architecture (\"Child Network\") to see how well it performs on a\nvalidation set\n\u2022\nUse reinforcement learning to update the parameters of the Controller model\nbased on the accuracy of the child model\n27\n27\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Neural Architecture Search for Convolutional Networks\nSoftmax classifier\nLayer N-1\nController RNN\n[1,3,5,7] [1,3,5,7] [1,2,3] [1,2,3]\n[24,36,48,64]\nNumber\nof Filters:\nFilter\nHeight\nFilter\nWidth\nStride\nHeight\nStride\nWidth\nNumber\nof Filters\nFilter\nHeight\nEmbedding\nLayer N\nLayer N+1\nZoph et al. NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING. ICLR 2017\n28\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Recurrent Cell Prediction from NAS\nTree\nht\nTree\nIndex 2\nAdd Tanh Mun ReLU Mult moid Add ReLU 1\n--------\nsigmoid\nelem_mult\nTree\nIndex 0\nIndex 1\nht-1\nXt\nht-1\n\u2717t\nTree Index 0\nTree Index 1\nTree Index 2\nCell Inject\nCell Indices\ntree that defines the\ncomputation steps\nto be predicted by\ncontroller\nrelu\nrelu\nadd\ntanh\nelem_mult\nadd\nht-1\nCt\nCt-1\nexample set of predictions made by the controller for each\ncomputation step in the tree\ncomputation graph of the\nrecurrent cell constructed\nfrom example predictions\nof the controller\n29\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Computation Steps in Predicted RNN Cell\n\u2022 The controller predicts Add and Tanh for tree index 0, this means we need to compute\na = tanh(W. * t+W2 * ht\u22121).\nao\n\u2022 The controller predicts ElemMult and ReLU for tree index 1, this means we need to\ncompute a\u2081 = ReLU ((W3 *x+) (W4* ht-1)).\n((W3*x+) O\n\u2022 The controller predicts 0 for the second element of the \"Cell Index\", Add and ReLU for\nelements in \"Cell Inject\", which means we need to compute anew = ReLU(ao + ct-1).\nNotice that we don't have any learnable parameters for the internal nodes of the tree.\nThe controller predicts Elem Mult and Sigmoid for tree index 2, this means we need to\ncompute a2 sigmoid (anew a\u2081). Since the maximum index in the tree is 2, ht is set to\na2.\n=\n\u2022 The controller RNN predicts 1 for the first element of the \"Cell Index\u201d, this means that we\nshould set ct to the output of the tree at index 1 before the activation, i.e., ct = (W3 *x+) O\n(W4 * ht\u22121).\n30\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-4-columbia-Fall2024.pdf": {
        "text": "RL based NAS Search\nTraining with REINFORCE (Zoph and Le, 2017)\nThe controller (RNN)\nSample architecture A\nwith probability p\nTrains a child network\nwith architecture\nA to get accuracy R\nExpected reward of controller\nJ(0c) = EP(a1:T;0c)[R]\nGradient of expected reward\nT\nV0J(0c) = P(a1:T;0c) [ V0. log P(at|a(t\u22121):1;0c)R]\nt=1\nCompute gradient of p and\nscale it by R to update\nthe controller\nEmpiricial approximation of gradient of expected reward\n1\nm\nm T\n\u03a3Velog P(at a(t\u22121):1; 0c)Rk\nk=1 t=1\n31\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Scaling NAS\nDistributed Training\nwith asynchronous parameter updates\nParameter\nServer 1\nAccuracy\nR\nController\nReplica 1\nParameters\n\u13be\n\u2022\n\u2022\n\u2022\nParameter server of S shards\nEach controller samples m child networks and train\nthem in parallel\nController then collects gradients according to the\nresults of that minibatch of m architectures at convergence\nand sends them to the parameter server in order to update\nthe weights across all controller replicas\nParameter\nServer 2\nController\nReplica 2\nParameter\nServer S\nController\nReplica K\nChild\nReplica 1\nChild\nReplica 2\nChild\nReplica m\nChild\nReplica 1\nChild\nReplica 2\nChild\nReplica m\nChild\nReplica 1\nChild\nReplica 2\nChild\nReplica m\nZoph et al. NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING. ICLR 2017\n32\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Differentiable Architecture Search\nContinuous relaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent\n1\n0\n0\n0\n0\n3\n3\n(a)\n(b)\n3\n(c)\n3\n(d)\nx(i) = \u03a3o(i,j) (x(i))\nFigure 1: An overview of DARTS: (a) Operations on the edges are initially unknown. (b) Continuous\nrelaxation of the search space by placing a mixture of candidate operations on each edge. (c) Joint\noptimization of the mixing probabilities and the network weights by solving a bilevel optimization\nproblem. (d) Inducing the final architecture from the learned mixing probabilities.\ni<j\nLiu et al. DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH. ICLR 2019\n33\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Details of DARTS\n.\nComputation procedure for an architecture (or a cell in it) is\nrepresented as a directed acyclic graph\nCell-based search\nhttps://arxiv.org/pdf/1806.09055.pdf\nrelax the categorical choice of a particular operation to a\nsoftmax over all possible operations:\no (\u03b1) \u03a3\n()(x) =\nexp(a())\nDEO \u03a3o'\u0404o explai\nwhere the operation mixing weights for a pair of nodes (i, j) are\nparameterized by a vector a (i,j) of dimension |O|.\narchitecture search then reduces to learning a set of\ncontinuous variables \u03b1 = a (i,j). a discrete architecture can be\nobtained by replacing each mixed operation \u014d(i,j)with the most\nlikely operation,\no(i,j)\n= argmax\u3002 a(i,j).\n2\n3\n0\n34\n",
        "links": [
            "https://arxiv.org/pdf/1806.09055.pdf"
        ]
    },
    "./chunks/chunk_35_Lecture-4-columbia-Fall2024.pdf": {
        "text": "DARTS\n\u2022 Goal is to jointly learn the architecture a and the weights w within all\n\u25cf\nthe mixed operations (e.g. weights of the convolution filters)\nThe continuous variable a determines the operation mixing weights for\ndifferent pair of nodes in the network\nLtrain and Lval the training and the validation loss,\n\u2022 Both losses are determined not only by the architecture a, but also the weights\nw in the network.\nmin Lval (w* (a), a)\n\u03b1\ns.t. w* (a) = argmin Ltrain (w, a)\nBi-level optimization problem\na as the upper-level variable\nwas the lower-level variable\n35\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Hyperparameter Optimization\n36\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Hyperparameter optimization\n\u2022 Problem of identifying good hyperparameter configuration(s) from the\nset of possible configurations\n\u2022 Two sub-problems\n\u2022\nConfiguration selection\n\u2022 Efficient selection of good configuration.\nConfiguration evaluation\n\u2022\nAdaptive computation, allocating more resources to promising\nhyperparameter configurations while eliminating poor ones.\n37\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-4-columbia-Fall2024.pdf": {
        "text": "5\n1\n16\n14\n15\n13\n11\n12\n3\n7\n8\n4\n6\n2\n10\nLoss\n0.30\n0.25\n0.20\n0.15\n0.10\n0.05\n0.00\n0\n10\n20\n30\n40\n50\n60\nResources\n(a) Configuration Selection.\n(b) Configuration Evaluation\n(a) The heatmap shows the validation error over a two-dimensional search space\nwith red corresponding to areas with lower validation error. Configuration selection\nmethods adaptively choose new configurations to train, proceeding in a sequential\nmanner as indicated by the numbers. (b) The plot shows the validation error as\na function of the resources allocated to each configuration (i.e. each line in the\nplot). Configuration evaluation methods allocate more resources to promising\nconfigurations.\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Hyperparameter Types\n\u2022 Continuous\n-\nExample: learning rate\nInteger\n- Example: #units\n\u2022 Categorical\n-\nFinite domain, unordered\n\u2022 Example 1: algo E {SVM, RF, NN}\n\u00b0\n\u00b0\nExample 2: activation function = {ReLU, Leaky ReLU, tanh}\nExample 3: operator = {conv3x3, separable conv3x3, max pool, ...}\n- Special case: binary\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Blackbox Hyperparameter Optimization\nDNN hyperparameter\nsetting 1\nBlackbox argmin\u0447(2)\noptimizer \u03bb\u03b5\u039b\nValidation\nperformance 4(\u03bb)\n\u2022 The blackbox function is expensive to evaluate\n\u2192 sample efficiency is important\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Hyperparameter Optimization Formulation\n2 (*) = argmin Ex~G,[L (x; Ax (X (train))).\n2 (*)\n\u03bb\u03b5\u03bb\n\u2248 argmin mean 2 (x; Ax (X (train)\n\u03bb\u03b5\u03bb \u03c7\u03b5\u03c7(valid)\n= argmin Y(\u03bb)\n\u03bb\u03b5\u03bb\nc (x; Ax (x (train)\nargmin 4(\u03bb) =\n\u03bb\u03b5{\u03bb(1)...\u03bb(5)}\ncritical step is to choose the set of trials {2(\u00b9)...\u03bb(S)}\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Techniques for Hyperparameter Optimization\n\u2022 Grid search\n\u2022\nRandom search\nHyperband random configuration search with adaptive resource\nallocation\nBayesian optimization methods\n\u2022 Focus on configuration selection\n\u25cf\nIdentify good configurations more quickly than standard baselines like\nrandom search by selecting configurations in an adaptive manner\nBayesian optimization with adaptive resource allocation\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-4-columbia-Fall2024.pdf": {
        "text": "9\n8\n7\n6\n5\n4\nGrid Search\nEvery combination of a preset list of\nvalues of the hyper-parameters and\nevaluate the model for each combination\n\u2022 Let K be the number of hyperparameters\ngrid search requires that we choose a set\nof values for each variable (L (1) ...\u013d (K))\nNumber of configurations to consider\nS = II (=1 |L (k) |\nSuffers from curse of dimensionality\n3\n2\n1\n0 123456789\nVisual Representation of grid search\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Random Search\n\u2022 Technique where random\ncombinations of the hyperparameters\nare used to find the best solution for\nthe built model\n6\n8\nM\n60\n\u0441\u043b\n5\n4\n.\nEmpirically and theoretically shown\nthat random search is more efficient\nfor parameter optimization than grid\nsearch.\n3\n2\n2\n0\n0 1 2 3 4 5 6 7 8 9\nVisual Representation of Random search\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Grid Search Vs Random Search\nUnimportant parameter\nGrid Layout\nUnimportant parameter\nRandom Layout\nImportant parameter\nImportant parameter\nGrid and random search of nine trials for optimizing a function f(x,y) = g(x)+h(y) \u2248\ng(x) with low effective dimensionality. Above each square g(x) is shown in green, and\nleft of each square h(y) is shown in yellow. With grid search, nine trials only test g(x)\nin three distinct places. With random search, all nine trials explore distinct values of\ng. This failure of grid search is the rule rather than the exception in high dimensional\nhyper-parameter optimization.\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Training resources\n\u2022 Size of training set\n.\nNumber of features\nNumber of iterations for iterative algorithms\n\u2022 Hours of training time\n46\n46\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Validation loss vs Resource allocated\nLoss\n1.6\n1.4\n1.2\n22\n10\n1.0\n0.8\nValidation loss as a function of total resources allocated for two\nconfigurations with terminal validation losse: V1 and V2\nV2\nThe shaded areas bound the maximum distance of the\nintermediate losses from the terminal validation loss and\nmonotonically decrease with the resource.\nPossible to distinguish between the two\nconfigurations when the envelopes no longer overlap\n0.6\n0.4\n0.2\nV1\n0.0\n0\n50\n100\n150\n200\n250\n300\n350\n400\nResources Allocated\nMore resources are needed to differentiate between the two\nconfigurations when either\n(1) the envelope functions are wider or\n(2) the terminal losses are closer together.\n47\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Successive Halving\n\u2022\nUnderlying principle: Even if performance after a small number of iterations is\nvery unrepresentative of the absolute performance of any configuration,\nits relative performance compared with many alternatives trained with the\nsame number of iterations is roughly maintained.\n\u2022 Relative ordering among configurations converges much faster than their true values\nUniformly allocate a budget to a set of hyperparameter configurations, evaluate\nthe performance of all configurations, throw out the worst half, and repeat until\none configuration remains\nAllocates exponentially more resources to more promising configurations\n\u2022 Given some finite budget B and n configurations, it is not clear a priori whether\nwe should\n\u2022 Consider many configurations (large n) with a small average training resources; or\n\u2022 Consider a small number of configurations (small n) with longer average training resources.\nSuccessive Halving suffers from the \"n vs B/n\" trade-off\n48\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-4-columbia-Fall2024.pdf": {
        "text": "n Vs. B/n\n\u2022 Consider a simple strategy\n\u2022\n\u2022\nIf hyper-parameter configurations can be discriminated quickly\n\u2022 n should be chosen large\nIf hyper-parameter configurations are slow to differentiate\n\u2022 B/n should be large\nDrawbacks of the simple strategy\n\u2022 If n is large, then some good configurations which can be slow to converge at\nthe beginning will be killed off early.\n\u2022\nIf B/n is large, then bad configurations will be given a lot of resources, even\nthough they could have been stopped before.\n49\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-4-columbia-Fall2024.pdf": {
        "text": "\u2022\nHyperband\nFormulating hyperparameter optimization as a pure-exploration adaptive\nresource allocation problem addressing how to allocate resources among\nrandomly sampled hyperparameter configurations.\n\u2022 Considers several possible values of n for a fixed B, in essence performing a\ngrid search over feasible value of n\n\u2022 Hedges and loops over varying degrees of the aggressiveness balancing\nbreadth versus depth-based search.\n\u2022 Resource constrained hyperparameter optimization\nLi et al. Hyperband: A Novel Bandit-Based Approach to Hyperparameter\nOptimization. 2018\n50\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Algorithm 1: HYPERBAND algorithm for hyperparameter optimization.\ninput\n: R, n (default n = 3)<\ninitialization: Smax = [logn (R)], B = (Smax + 1)R\n1 for s = {Smax, Smax - 1,..., 0} do\n2\nn =\nB\nR(s + 1)\n\u03b7\u03c2\nr =\nRn-s\n// begin SUCCESSIVEHALVING with (n,r) inner loop\nT=get hyperparameter configuration(n)\nfor i = {0,...,s} do\nR: max amount of resource allocated\nto a single configuration\nn proportion of configurations\ndiscarded in each round of\nSUCCESSIVEHALVING\ntwo inputs dictate how many different\nbrackets are considered\n3\n4\nBracket (outer loop)\n5\nni =\n= [nn\u00afi]\n6\nri = rn\u00b2\nSuccessive\nHalving\n7\nL =\n8\n{run then return_val_loss (t, ri) : t\u0404T}\nT=top k(T, L, [ni/n])\n(inner loop)\n9\nend\n10 end\n11 return Configuration with the smallest intermediate loss seen so far.\n51\n2\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Hyperband\nget_hyperparameter_configuration (n): returns a set of n i.i.d samples from some\ndistribution defined over the hyperparameter configuration space. Uniformly sample the hyperparameters from\na predefined space (hypercube with min and max bounds for each hyperparameter).\nrun_then_return_val_loss (t, r): a function that takes a hyperparameter configuration t\nand resource allocation as input and returns the validation loss after training the configuration for the\nallocated resources.\ntop_k (configs, losses, k): a function that takes a set of configurations as well as their\nassociated losses and returns the top k performing configurations.\n",
        "links": []
    },
    "./chunks/chunk_53_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Hyperband: Sweeping over different s\n\u2022 Each inner loop indexed by s is designed to take B total iterations\nEach value of s takes about the same amount of time on average.\n\u2022 For large values of s the algorithm considers many configurations\n(max_iter at the most) but discards hyperparameters based on just a\nvery small number of iterations which may be undesirable for\nhyperparameters like the learning_rate.\n\u2022 For small values of s the algorithm will not throw out\nhyperparameters until after many iterations have been performed but\nfewer configurations are considered (logeta(max_iter)+1 at the least).\nThe outerloop hedges over all possibilities.\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Hyperband in action\nEach bracket uses B total resources and corresponds to a different tradeoff of n and B/n\nmax_iter = 81\neta = 3\nB = 5*max_iter\nni\nri\nni\nri\n27\n5\n81\n81\nni ri\n081\nni\nni\n1\n27\n3\n1\n27\n234\n93\n39\n9\n9\n93\n27\n62\n6\n3\n27 1\n81\n27\n1\n81\n1\n81\nA larger value of n corresponds to a smaller r and hence more aggressive early-stopping\nA single execution of Hyperband takes a finite budget of (Smax + 1)B\nrandom search\n54\n",
        "links": []
    },
    "./chunks/chunk_55_Lecture-4-columbia-Fall2024.pdf": {
        "text": "AutoML as Hyperparameter Optimization.\nDefinition: Combined Algorithm Selection and Hyperparameter\nOptimization (CASH)\nLet\n\u2022 A = {A(\u00b9)\nA(n)} be a set of algorithms\n\u2022 A (i) denote the hyperparameter space of A(i), for i = 1, . . ., n\n\u00b0\nL(A), Dtrain, Dvalid) denote the loss of A(i), using A E A (i) trained\non Dtrain and evaluated on Dvalid.\nThe Combined Algorithm Selection and Hyperparameter Optimization\n(CASH) problem is to find a combination of algorithm A* =\nA(i) and\nhyperparameter configuration A* EA (i) that minimizes this loss:\narg min L(A), Dtrain, Dvalid)\nA** \u20ac\n\u0391(1) \u0395\u0391,\u03a3\u0395\u039b(4)\n",
        "links": []
    },
    "./chunks/chunk_56_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Bayesian Hyperparameter Optimization\n\u2022\n\u2022\n\u2022\nBuilds a probability model of the objective function and use it to select the most\npromising hyperparameters to evaluate in the true objective function.\nApproach\n1. Fit a probabilistic model to the function evaluations (\u03bb, f (\u03bb))\n2. Use that model to trade off exploration vs. exploitation\nSurrogate probability model for the objective function: p(score | hyperparameters)\nSteps:\n1. Build a surrogate probability model of the objective function\n2.\n3.\n4.\n5.\nFind the hyperparameters that perform best on the surrogate\nApply these hyperparameters to the true objective function\nUpdate the surrogate model incorporating the new results\nRepeat steps 2-4 until max iterations or time is reached\n\u2022 The surrogate probability model is updated after each evaluation of the objective\nfunction\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-4-columbia-Fall2024.pdf": {
        "text": "f(x)\nSurrogate model performance\npred var\npred mean\ntruth\nAfter 2 evaluations\nevaluations\nf(x)\nAfter 8 evaluations\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Sequential Model-Based Optimization\n(SMBO)\nRunning trials one after another, each time trying better hyperparameters\nby applying Bayesian reasoning and updating a probability model\n(surrogate)\n\u2022 Main components\n1. A domain of hyperparameters over which to search\n2.\nAn objective function which takes in hyperparameters and outputs a score that we\nwant to minimize (or maximize)\n3. The surrogate model of the objective function\n4.\nA criteria, called a selection function, for evaluating which hyperparameters to\nchoose next from the surrogate model\n5. A history consisting of (score, hyperparameter) pairs used by the algorithm to\nupdate the surrogate model\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Surrogate model\n\u2022 Gaussian Processes\nRandom Forest Regressions\n\u2022 Tree Parzen Estimators (TPE)\nAccuracy\n$ 0.90\n0.85\n0.80\n0.75\nNumber of Product Terms\nNumber of Iterations\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Selection Function\n\u2022\nExpected Improvement\nEL\u2081. (x) = [\"\u00b0\" (y *\nS\" (y\u00b2\n- y)p(y|x)dy\nscore\n10\n40\n35\n30\n25\n20\n15\n10\n10\nRandom Forest Results\n0\n200\n400\nHere y* is a threshold value of the objective function, x is the proposed set of\nhyperparameters, y is the actual value of the objective function using\nhyperparameters x, and p(y | x) is the surrogate probability model expressing\nthe probability of y given x.\n600\n800\n1000\nn_estimators\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Tree structured Parzen estimator\np(y|x)\n=\np(x \\y) *p(y)\np(x)\np(xy):\n=\nfl(x) if y<y*\n(g(x) if y\u2265 y*\nEly (x):\n=\nprobability\nTPE Probability Distributions\n0.014\n0.012\n0.010\n0.008\n0.006\n0.004\n0.002\n0.000\n0\n100\n200\n300\nvy\" l(x)-l(x) \u221a P(y)dy\n8\nyl(x)+(1-y)g(x)\n\u03b1\n400\n500\n600\n700\nn_estimators\n(y + 9 ) (1-1)) -1\ng(x)\nl(x)\n1(x)\ng(x)\n",
        "links": []
    },
    "./chunks/chunk_62_Lecture-4-columbia-Fall2024.pdf": {
        "text": "ONNX\nOpen Neural Network Exchange\n\u2022 An open format to represent traditional machine learning and deep\nlearning models\n\u2022 ONNX Features\n.\n\u2022\nFramework interoperability\n\u2022 Models trained in one DL framework to be transferred to another for inference\n\u2022 Hardware optimizations\n\u2022 ONNX-compatible runtimes and libraries designed to maximize performance on specific\nDL hardware\n\u2022 Visit http://onnx.ai\n62\n",
        "links": [
            "http://onnx.ai",
            "http://onnx.ai"
        ]
    },
    "./chunks/chunk_63_Lecture-4-columbia-Fall2024.pdf": {
        "text": "ONNX Capabilities\n\u2022 Common set of operators related to ML\n\u2022 Common file format for representing ML models\n\u2022 Supported Tools\n\u2022\nVisit http://onnx.ai/supported-tools\n\u2022 ONNX Tutorials\nVisit https://github.com/onnx/tutorials\n\u2022 Model Zoo\n\u2022 A collection of pre-trained, state-of-the-art models in the ONNX format contributed by\ncommunity members\n\u2022 Visit https://github.com/onnx/models\n63\n",
        "links": [
            "http://onnx.ai/supported-tools",
            "https://github.com/onnx/tutorials",
            "https://github.com/onnx/models",
            "http://onnx.ai/supported-tools"
        ]
    },
    "./chunks/chunk_64_Lecture-4-columbia-Fall2024.pdf": {
        "text": "ONNX in Enterprise: Microsoft\nML used in many products to improve performance and productivity\nMicrosoft 365\nS Skype\nWindows\nMicrosoft Dynamics 365\nBing\nMicrosoft HoloLens\nMicrosoft | Research\nOffice 365\nXBOX\n\u2022\n\u2022\nAl developers work with different frameworks\nDeployment of trained models to production\n\u2022\nDifferent deployment targets: cloud, IoT devices, edge devices\n\u2022\nDifferent hardware: CPUs, GPUs, TPUs, FPGAS\nYoutube Video: Open Neural Network Exchange (ONNX) in the\nenterprise: how Microsoft scales ML\n64\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Multiple ML frameworks + multiple deployment\ntargets\nReality: ML @ Microsoft - Deployment Challenge\nTraining Framework\nK Keras\nPyTorch\nCognitive\nToolkit\nML.NET\nCaffe\nlearn\nDeployment Target\nCPU\nFPGA\nMaintaining multiple frameworks in deployment\n\u2022\nGPU\n\u2022\nNPU\n\u2022\nNot scalable\n\u2022\nHard to maintain\nDegrades application performance\nFrameworks compete for system\nresources\nDecouple training and deployment frameworks\nYoutube Video: Open Neural Network Exchange (ONNX) in the\nenterprise: how Microsoft scales ML\n65\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Bridge Model Training and Deployment\nOpen and Interoperable industry wide standard\nPyTorch\nlearn\nChainer\nK\n\u2192 Caffe2\nmxnet\nBuild and train\nmodel in\nframework A\nCognitive\nToolkit\ndmlc\nXGBoost\nML\nCPU\nONNX\nFPGA\nGPU\nNPU\nPaddlePaddle\nConvert trained\nmodel in\nframework A to\nONNX model\nConvert ONNX\nmodel to\nframework B and\ndeploy\nDeploy ONNX\nmodel using\nONNX Runtime\n66\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-4-columbia-Fall2024.pdf": {
        "text": "\u00b7\nONNX Runtime\nFlexible\nONNX\nRUNTIME\nSupports full ONNX-ML\nspec (v1.2-1.5)\nC#, C, and Python APIs\nCross Platform\nWorks on\n-Mac, Windows, Linux\n-x86, x64, ARM\nAlso built-in to Windows\n10 natively (WinML)\ngithub.com/microsoft/onnxruntime\nExtensible\nExtensible architecture to\nplug-in optimizers and\nhardware accelerators\nONNX Runtime is a cross-platform inference and training machine-learning accelerator.\n\u2022 Available on Mac, Windows, Linux platforms\n\u2022\nSupports full ONNX-ML spec\nOpen-sourced https://github.com/microsoft/onnxruntime\n67\n40\n",
        "links": [
            "https://github.com/microsoft/onnxruntime"
        ]
    },
    "./chunks/chunk_68_Lecture-4-columbia-Fall2024.pdf": {
        "text": "ONNX Runtime Architecture\nComputational dataflow graph\nInput\nData\nONNX\nModel\nIn-Memory\nGraph\nGraph\nPartitioner\nProvider\nRegistry\nParallel, Distributed Graph Runner\nExecution Providers\nCPU\nMKL-\nDNN\nnGraph\nCUDA\nTensorRT\n1\nOutput\nResult\nConv X\nstrides 1,1\npads-0,0,0,0\nkernel shape 1,1\nRelu\nConv XW B\nstrides 1,1\npads 0.0.0.0\nkernel shape 1,1\nY=>X\n\u2611\n>inputs\nY X\nRelu X\nY=> X\nConv\nstrides 1,1\nW\nB\npads 1,1,1,1\nkernel shape 3.3\nConcat inputs\naxis-1\nY=> X\nRelu X\nY->inputs\nconcat result X\nConv XW B\nstrides 1,1\npads-0.0.0.0\nkernel shape 1,1\nY-X\nONNX Runtime has a graph parser which takes ONNX model, parses the graph, applies\nruntime optimizations like fusion ops, executes portions of the graph on specific\nhardware, provides the inference output result\nYoutube Video: Open Neural Network Exchange (ONNX) in the\nenterprise: how Microsoft scales ML\nRelu X\nFrameworks provide implementations of ONNX\noperators\n68\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Example\nhttps://github.com/huggingface/notebooks/blob/main/examples/onnx\n-export.ipynb\n.\nExport Transformers Models to ONNX\nLeverage ONNX runtime for inference over an ONNX graph\nBenchmark Pytorch and ONNX model on CPU\n69\n",
        "links": [
            "https://github.com/huggingface/notebooks/blob/main/examples/onnx"
        ]
    },
    "./chunks/chunk_70_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Learning with limited data\n\u2022\n\u2022\nBuilding good machine learning models require lot of training data\n\u2022 To capture robust representation of unknown input distribution\nSmall training jobs are common and labeled data is scarce in many\ndomains\n\u2022 In commercial VR service (Bhatta et al. 2019), average number of\nimages submitted is 250 and average number of classes are 5; ~50\nimages per class\n\u2022 Can we leverage knowledge learnt from related tasks for target task?\n70\n10\n",
        "links": []
    },
    "./chunks/chunk_71_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Transfer Learning\nLearning Process of Traditional Machine Learning\nDifferent Tasks\nLearning Process of Transfer Learning\nSource Tasks\nLearning System\nLearning System\nLearning System\nKnowledge\nTarget Task\nLearning System\n(a) Traditional Machine Learning\n(b) Transfer Learning\nTransfer learning is a class of techniques to reuse knowledge gathered\nfrom \"source\" tasks (with sufficiently rich set of labeled data) for a \"target\ntask (with few labeled data)\n",
        "links": []
    },
    "./chunks/chunk_72_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Inductive Transfer Learning Approaches\n\u2022 Common intuition: Networks which have learned compact representations of a\n\"source\" task, can reuse these representations to achieve higher performance\non a related \"target\" task.\n\u2022 Instance based approaches attempt to identify appropriate data used in the\nsource task to supplement target task training\n\u2022 Source and target domain data use exactly the same set of features and\nlabels, but the distributions of the data in the two domains are different\nSource\nsamples\nTarget\nSamples\nReweighting\nLow\nHigh\nColor bar shows the weights\n72\n12\n",
        "links": []
    },
    "./chunks/chunk_73_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Inductive Transfer Learning Approaches\n\u2022 Feature representation based approaches attempt to leverage source task\nweight matrices\n\u2022 Trained weights in source network have captured a representation of the\ninput that can be transferred by fine-tuning the weights or retraining the\nfinal dense layer of the network on the new task.\nRandom Weights\nSource Domain\n+\nTraining\nTransferring the learned structure\n(Weights of the network)\nTarget Domain\nLearned Weights on\nSource Samples\nLearned Weights on\nSource Samples\nFine Tuned Weights on\nTarget Samples\nTraining\n73\n",
        "links": []
    },
    "./chunks/chunk_74_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Learn to Transfer\n\u2022\n\u2022\nWhich source model to use?\n\u2022\nCaffe Model Zoo has several pretrained models for different tasks\n\u2022\nTensorFlow 2 Detection Model Zoo has several models pretrained on COCO 2017 dataset\nWhich method to use?\n\u2022\nShallow learning driving SVMs\n\u2022\nFine tuning\n\u2022\nSingle source or ensemble\nWhich layers to freeze and finetune?\nResNet101 has 101 layers\nGoogleNet has 22 layers\n\u2022\n\u2022\nVGG16 has 16 layers\nWhat is the performance requirement?\n\u2022\nLatency\n\u2022\nMemory Footprint\n\u2022\nTarget domain accuracy\nWhat curriculum should I follow?\n\u2022\nImagenet1K (1000 classes, ~1.3M images,) -> Food 101 (101 classes, 101000 images) -> Greek Food\nImagenet22K (22K classes,~15M images,) -> Greek Food\n74\n",
        "links": []
    },
    "./chunks/chunk_75_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Transfer Learning: Basic Finetuning\n\u2022 Take almost any deep network pre-trained on a large dataset of your choice\n\u2022 Model zoo of pretrained models\n\u2022 TensorFlow: https://github.com/tensorflow/models\n\u2022\nPyTorch: https://github.com/pytorch/vision\n\u2022 Replace the last (classification) layer with a randomly initialized one\n\u2022 Train only the new layer's weights, using the \"frozen\" embedding\n\u25a0This baseline method\n1. Train on Imagenet\n2. Small Dataset (C classes)\n3. Bigger dataset\nFC-1000\nFC-4096\nFC-C\nFC-4096\nworks well in many\nReinitialize\nFC-4096\nFC-4096\nFC-C\nFC-4096\nFC-4096\nTrain these\nthis and train\nMaxPool\nMaxPool\nMaxPool\nsettings...\nConv-512\nConv-512\nConv-512\nWith bigger\nConv-512\nConv-512\nConv-512\ndataset, train\nMaxPool\nMaxPool\nMaxPool\nmore layers\n\u25a0.... Can we improve on it?\nConv-512\nConv-512\nConv-512\nConv-512\nConv-512\nConv-512\nMaxPool\nMaxPool\nFreeze these\nMaxPool\nConv-256\nConv-256\nConv-256\nFreeze these\nConv-256\nConv-256\nConv-256\nMaxPool\nMaxPool\nMaxPool\nConv-128\nConv-128\nConv-128\nLower learning rate\nConv-128\nConv-128\nConv-128\nwhen finetuning;\nMaxPool\nMaxPool\nMaxPool\n1/10 of original LR\nConv-64\nConv-64\nConv-64\nConv-64\nConv-64\nConv-64\nis good starting\npoint\nImage\nImage\nImage\n75\n",
        "links": [
            "https://github.com/tensorflow/models",
            "https://github.com/pytorch/vision"
        ]
    },
    "./chunks/chunk_76_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Improving Transferability in Transfer\nLearning\n\u2022 Selection of source model: What is the best \"source\" task/model to\ntransfer knowledge for a \"target\" task?\n\u2022\nHow to measure \"similarity\" between source and target task?\nDevelop similarity measures between source and target datasets in feature space\n.\n\u2022 Does size always matter?\n\u2022\n\u2022 Will a source model trained on huge datasets will always outperform source models trained\non smaller but more related datasets ?\nImproper choice of a base dataset/model for a target could result in degraded\nperformance compared to not using transfer learning (negative transfer)\nDegree of finetuning\n\u2022 Which layers to finetune and which to freeze?\n\u2022 What should be the learning rate of layers to be finetuned ?\n\u2022\nHigher learning rate loose information from source task\n76\n",
        "links": []
    },
    "./chunks/chunk_77_Lecture-4-columbia-Fall2024.pdf": {
        "text": "labels\nA\nbaseA\nTransferability Experiments\nWA1 WA2 WA3 WA4 WA5\nWA6 WA7\nWAS\ninput\nA\nOOOOOOO \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\ninput\nB\nWB1\nWB2\n\u17e1\u17e1\nOOOOOOO\nWB3\n\u17e1\nT\nOOOOOOO \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\ngenerality versus specificity of neurons\nlabels\nbaseB\nB\nB3B\nand\nB3B+\n0000000\nOOOOOOO\naa\nA3B\nand\nA3B+\n77\n12\n",
        "links": []
    },
    "./chunks/chunk_78_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Transferability\nEach marker in the figure represents\nthe average accuracy over the\nvalidation set for a trained network.\nThe white circles above n = 0 represent\nthe accuracy of baseB. There are eight\npoints, because we tested on four\nseparate random A/B splits. Each dark\nblue dot represents a BnB network.\nLight blue points represent BnB+\nnetworks, or fine-tuned versions of\nBnB. Dark red diamonds are AnB\nnetworks, and light red diamonds are\nthe fine-tuned AnB+ versions.\nTop-1 accuracy (higher is better)\nTop-1 accuracy (higher is better)\n0.66\n0.64\n0.62\n0.60\n0.58\n0.56\nOO\n\u25cf \u25cb\nbaseB\nselffer BnB\n\u25cf\n+\n0.54\nselffer BnB +\ntransfer AnB\ntransfer AnB\n0.52\n0\n3\n4\nU\n4\n5\n6\n\u05db\n0\n0.64\n0.62\n0.60\n5: Transfer + fine-tuning improves generalization\n3: Fine-tuning recovers co-adapted interactions\n2: Performance drops\ndue to fragile\nco-adaptation\n4: Performance\ndrops due to\nrepresentation\nspecificity\n0.58\n0.56\n0.54\n0\n1\n2\n3\n4\n5\n6\n7\nLayer n at which network is chopped and retrained\nLines connecting the means of each treatment. 78\n",
        "links": []
    },
    "./chunks/chunk_79_Lecture-4-columbia-Fall2024.pdf": {
        "text": "Impact of base model data size on transfer\nlearning\nIMAGENET\nInstrumentality, instrumentation\nThe Food-101 Data Set\nImagenet1K\n(1.3 Million images)\nor\nImagenet22K\n(14 Million images)\nGoogleNet\nInner layers\nOuter layer\nTop-1 accuracy\n0.82\nImageNet 22k\n0,80\nImageNet 1k\n0.78\n0.76-\n0.74\n0.72\nLarge model trained with small data\ncould over learn the training set\n0.70\n0,68\n0.66\n0.001\n0.01\n0.1\n1\nIr = 0.01\nIr = * 0.01\nFine-tuning results on Food-101 dataset with varying numbers of learning\nrate multipliers for layers with pre-trained weights\nBhattacharjee et al, \"Distributed learning of deep feature embedding for visual recognition tasks\", IBM J of R & D, 2017\n79\n",
        "links": []
    },
    "./chunks/chunk_1_Lecture-8-Columbia.pdf": {
        "text": "Lecture 8\nParijat Dube, Chen Wang\n+\n[COMSE6998-015] Fall\n2024\nIntroduction to Deep\nLearning and LLM based\nGenerative Al Systems\n+\n1\n",
        "links": []
    },
    "./chunks/chunk_2_Lecture-8-Columbia.pdf": {
        "text": "Agenda\nRAG & Agent\n.\nRAG\n\u2022 Semantic Search, Keyword Search,\nEmbeddings, Hybrid Search, ReRank,\nGenerating Answers\nVectorDB\n\u2022 KNN, ANN, HNSW\nFunctions, Tools and Agents\nGuest Lecture, Bo Wen from IBM\nResearch\n\u2022 LangChain & LlamaIndex Notebooks\n",
        "links": []
    },
    "./chunks/chunk_3_Lecture-8-Columbia.pdf": {
        "text": "Retrieval Augmented Generation\nWhat is RAG?\n\u2022 \"Retrieval Augmented Generation (RAG) is a sophisticated approach that\nenhances the capabilities of large language models (LLMs) by integrating\nretrieval mechanisms with generative models. This synergy allows the model\nto access a wealth of external knowledge, significantly improving the\nrelevance and accuracy of generated responses [1]\".\nWhy we need RAG?\n.\n\u25cf\nImproves accuracy and relevance of LLM outputs\nAllows access to current information beyond LLM training data\nCost-effective alternative to fine-tuning or retraining LLMs\n[1] https://www.restack.io/p/retrieval-augmented-generation-answer-rag-vs-semantic-cat-ai\n",
        "links": [
            "https://www.restack.io/p/retrieval-augmented-generation-answer-rag-vs-semantic-cat-ai"
        ]
    },
    "./chunks/chunk_4_Lecture-8-Columbia.pdf": {
        "text": "Semantic\nSearch\nTraditional Semantic Search vs. RAG\n\u2022 Semantic Search: Retrieves relevant\ndocuments based on meaning and intent\n\u2022 RAG: Retrieves relevant information AND uses\nit to generate new, contextual responses\nKey Differences\n\u2022 Semantic Search focuses on retrieval;\n\u2022 RAG combines retrieval with generation; RAG\nprovides more dynamic and tailored responses\n",
        "links": []
    },
    "./chunks/chunk_5_Lecture-8-Columbia.pdf": {
        "text": "Semantic Search\nSearch is crucial to how we\nnavigate the world\nMosoft Bing\n\u0644\u0647\nGoogle\n$ 3\nGoogle Search\ncoffee\nRelevance\nOpen now\nTop-rated\nplaystation\nPlayStation\ndavid bowie\nTop\nArtists Songs Albums Playlists\nLa Col Latest f\n4.6\nCoffee s\nDavid Bowie\nOpen C\nArtist\nTrendy\npastries\nFeaturing David Bowie\nMe\nDavid Bowie\nDevil Renie\nNostalgie+Rock\nYOU\nAl chatbot to search the web\nGrego\n44\nStart typing to search.\nearch the web privately\nbrave\n9\nCoffee s\nOpen C\nHigh tec\npastries\nThe\nCar This is David\nPlay Bowie\nDavid Bowie Radio Nostalgia + Rock\nThe Rise and Fall of Ziggy Stardust and t..\nAlbum David Bowie\nBlackstar\nAlbum - David Bowie\nSpace Oddity-2015 Remaster\nSong David Bowie\nDavid Bowie greatest hits\nFriend\nBOSE NO TOO H\nJake Tapper Ret\nIID\nMame\nSearch\nYour Library\n",
        "links": []
    },
    "./chunks/chunk_6_Lecture-8-Columbia.pdf": {
        "text": "Keyword\nSearch\nQuery\nWhat color is the grass?\nResponses\nTomorrow is Saturday\nThe grass is green\nThe capital of Canada is Ottawa\nThe sky is blue\nA whale is a mammal\nNumber of words\nin common\n1\n3\n2\n2\n1\n",
        "links": []
    },
    "./chunks/chunk_7_Lecture-8-Columbia.pdf": {
        "text": "Search at a\nhigh level\nQuery\nDocument\nArchive\nResults\n1. search result\n2. search result\nSearch\nSystem\n3. search result\n4. search result\n5. search result\n",
        "links": []
    },
    "./chunks/chunk_8_Lecture-8-Columbia.pdf": {
        "text": "Keyword Search (Internals)\nQuery\nWhat color is the sky?\ninverted index Algorithm\nID\nDOCUMENT/TEAM\nDOCUMENT/TEAM\nID\nResults\n1\nCAT\nCAT\n1\n3 6\n1. search result\n2\nDOG\nDOG\n2\n5\n2. search result\n3\nSearch\nSystem\nReranking\nCAT\nCAT\n4\nRetrieval\nSecond\n3. search result\nFirst Stage\n4\nFISH\nStage\n4. search result\n5\nDOG\n5. search result\n60\nCAT\nInverted\nIndex\nBM25 Algorithm\nKeyword\nabacus\nDocument IDs\n1,38,18\nColor\n23, 804\nSky\n804, 922\nBM25 Algorithm, https://www.geeksforgeeks.org/keyword-\nsearching-algorithms-for-search-engines/\n",
        "links": [
            "https://www.geeksforgeeks.org/keywordsearching-algorithms-for-search-engines/"
        ]
    },
    "./chunks/chunk_9_Lecture-8-Columbia.pdf": {
        "text": "Limitation of\nkeyword\nmatching\nQuery\nStrong pain in the side of\nthe head\nNo shared\nkeywords\nDocument\nArchive\nSharp temple headache\nLanguage\nModel\n",
        "links": []
    },
    "./chunks/chunk_10_Lecture-8-Columbia.pdf": {
        "text": "Language Models Can Improve Both Search Stages\nQuery\nWhat color is the sky?\nResults\n1. search result\nSearch\nSystem\nRetrieval\nFirst Stage\nReranking\nSecond\nStage\n2. search result\n3. search result\n4. search result\n5. search result\nGeneration\nLanguage\nModel\nLanguage\nModel\nLanguage\nModel\nco.embed()\nco.rerank()\nco.generate()\n",
        "links": []
    },
    "./chunks/chunk_11_Lecture-8-Columbia.pdf": {
        "text": "\u2022\nEmbeddings\n\u2022\nObject 1\nEmbeddings\n\u2022\nare dense numerical representations of various types of data such as text,\nimages, or audio, transforming them into a low-dimensional space suitable for\nmachine learning.\nKey Points:\n\u25cf\n\u2022\nDimensionality Reduction: They map high-dimensional data to a lower-\ndimensional space, capturing important features.\nSemantic Similarity: Similar items have embeddings close together, allowing\ntasks like nearest neighbor search.\n\u2022 Applications: Used in NLP, computer vision, recommendation systems.\n\u2022\nLearning from Data: Generated through algorithms like neural networks that learn\nfrom input data.\n0.6 0.3 0.1\nObject 2\nEmbedding Model\n0.8 0.5 0.3\nObject 3\nSet of Objects\n0.4 0.2 0.9\nObjects as Vectors\n",
        "links": []
    },
    "./chunks/chunk_12_Lecture-8-Columbia.pdf": {
        "text": "Embeddings\n5\n3\n2\n1\nQuiz: Where would you put the word \"apple\"?\nWord\nNumbers\nApple\n?\n?\nBanana\n6\n5\nStrawberry\n5\n4\nCherry\n6\n4\nSoccer\n0\n6\nBasketball\n1\n6\nTennis\n1\n5\nCastle\n1\n2\n\u5be9\n1 B\nHouse\n2\n2\nBuilding\n2\n1\nBicycle\n5\n1\n1\n2\n3\n4 5 6\nTruck\nCar\n60\n99\n1\n6\n0\n",
        "links": []
    },
    "./chunks/chunk_13_Lecture-8-Columbia.pdf": {
        "text": "Word\nEmbeddings\nWord\nWord\nNumbers\nApple\n5 5\nSoccer\n0\n6\nHouse\n2\n2\nCar\n6\n0\nNumbers\nA\n-0.82\n-0.32\n0.23\nAardvark\n0.42\n42 1.2\n-0.06\nZygote\n-0.74\n-1.02\nHundreds\n1.35\n",
        "links": []
    },
    "./chunks/chunk_14_Lecture-8-Columbia.pdf": {
        "text": "Text Embeddings\nSentence\nHello, how are you?\n0.39\n0.49\nI'm going to school today\n-0.79\n-0.05\nOnce upon a time\n3.23\n-0.23\nNumbers\n...\n-1.01\n-0.72\n-0.94\n2.71\n-1.45\n0.82\nHi, how's it going?\n0.41\n0.48\n-0.98\n-0.66\n",
        "links": []
    },
    "./chunks/chunk_15_Lecture-8-Columbia.pdf": {
        "text": "Dense Retrieval\nQuery\n\u2610 The clouds are white\nWhat is the capital of Canada?\nThe gras is grean The sky is blue\nResponses\nThe capital of Canada is Ottawa\nThe capital of France is Paris\nThe sky is blue\nThe clouds are white\nThe grass is green\nWhat is the capital of Canada?\nThe capital of France is Paris\nThe capital of Canada is Ottawa\n",
        "links": []
    },
    "./chunks/chunk_16_Lecture-8-Columbia.pdf": {
        "text": "Chunking\nTransformer (machine learning model)\nArticle Talk\nFrom Wikipedia, the free encyclopedia\nA Transformer is a deep learning architecture that relies on the attention mechanism.!!! It is notable for\nrequiring less training time compared to previous recurrent neural architectures, such as long short-term\nmemory (LSTM), and has been prevalently adopted for training large language models on large (language)\ndatasets, such as the Wikipedia Corpus and Common Crawl, by virtue of the parallelized processing of input\nsequence. More specifically, the model takes in tokenized (byte pair encoding) input tokens, and at each\nlayer, contextualizes each token with other (unmasked) input tokens in parallel via attention mechanism.\nThough the Transformer model came out in 2017, the core attention mechanism was proposed earlier in\n2014 by Bahdanau, Cho, and Bengio for machine translation. (415) This architecture is now used not only in\nnatural language processing, computer vision,16) but also in audio, and multi-modal processing. It has also\nled to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and\nBERT (Bidirectional Encoder Representations from Transformers).\nBackground [edit]\nBefore transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTMs and gated\nrecurrent units (GRUS), with various attention mechanisms added to them. Unlike RNNs, transformers do not\nhave a recurrent structure. Provided with enough training data, their attention mechanisms alone can match\nthe performance of RNNs with attention added.[1]\nPrevious work [edit]\nIn 1992, J\u00fcrgen Schmidhuber published the fast weight controller as an alternative to RNNs that can learn\n\"internal spotlights of attention,(10) and experimented with using it to learn variable binding.[11]\nIn a fast weight controller, a feedforward neural network (\"slow\") learns by gradient descent to control the\nweights of another neural network (\"fast\") through outer products of self-generated activation patterns called\n\"FROM\" and \"TO\" which corresponds to \"key\" and \"value\" in the attention mechanism.[12] This fast weight is\napplied to queries. The attention mechanism may be obtained by interposing a softmax operator and three\nlinear operators (one for each of query, key, and value), [12][13]\nFlawed sequential processing [edit]\nModels, used before the transformer models prevailed, processed the input tokens sequentially, maintaining\na state vector representing all the tokens up to the current token in the input. To process the nth token, such\na model combined the vector representing the sentence up to token n-1 with the information of the new\ntoken to create a new state, representing the sentence up to token n. Theoretically, the information from one\ntoken can propagate arbitrarily far down the sequence, if at every point the state continues to encode\ncontextual information about the token.\nQuery\nDo transformers have a\nrecurrent structure?\n",
        "links": []
    },
    "./chunks/chunk_17_Lecture-8-Columbia.pdf": {
        "text": "Vector 1\nVector 2\nVector 3\nSearch for\nSimilar\nVectors\nVector N\nSearch\nAlgorithm\nQuery Vector\n17\nNearest Neighbor Vector\n10\n11:15\n16\n18\n20 Vectors\nFind \"K\" nearest neighbors to a search query - K-NN\n12\n",
        "links": []
    },
    "./chunks/chunk_18_Lecture-8-Columbia.pdf": {
        "text": "'brute force'\nsearch\nalgorithm\n'brute force' search algorithm\n1115\n1. Measure the L2 distance between the Query\nand each vector\n2. Sort all those distances\n3. Return the top k matches. These are the most\nsemantically similar points.\n",
        "links": []
    },
    "./chunks/chunk_19_Lecture-8-Columbia.pdf": {
        "text": "Runtime\nComplexity\nof KNN\nRuntime for a Query\n\u2022 O(dN) runtime complexity for search\nRuntime Complexity for KNN\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n0\n10000\n20000\n30000\n40000\n50000\nNumber of Data Points\n",
        "links": []
    },
    "./chunks/chunk_20_Lecture-8-Columbia.pdf": {
        "text": "Not a big\nproblem at\nsmall scale\nNearest Neighbor Vector\n11 15\n2\n14\n18\n20 Vectors\n10\n8\n13\n7\n19\n16\nQuery Vector\n3\n12\n",
        "links": []
    },
    "./chunks/chunk_21_Lecture-8-Columbia.pdf": {
        "text": "Big problem\nat scale\nNearest Neighbor Vector\n?\n1,000 Vectors\nQuery Vector\n",
        "links": []
    },
    "./chunks/chunk_22_Lecture-8-Columbia.pdf": {
        "text": "small world\nSmall\nWorld\nObserved phenomenon in human social networks\nwhere everyone is closely connected.\n\u2022 Six Degrees of Separation: Popularized concept that\nany two individuals are on average separated by six\nacquaintance links.\n\u2022 High Clustering: Nodes form tightly-knit groups, with\nmembers of groups connecting to one another.\nShort Path Lengths: Despite high clustering, only a\nfew steps are needed to connect any two nodes.\n",
        "links": []
    },
    "./chunks/chunk_23_Lecture-8-Columbia.pdf": {
        "text": "Navigate\nSmall World\n(Constructing)\nVector (node) is added\nVector (node) is not added yet.\n8 Vectors\n(6)\n4\n2\n1\n0\n5\n3\n(7)\n",
        "links": []
    },
    "./chunks/chunk_24_Lecture-8-Columbia.pdf": {
        "text": "Navigate\nSmall World\n(Constructing)\n6\n8 Vectors\n(3)\n4\n7\n(2)\n1\n\u2191\n0\n5\n",
        "links": []
    },
    "./chunks/chunk_25_Lecture-8-Columbia.pdf": {
        "text": "Navigate\nSmall World\n(Constructing)\n(6)\n1\n8 Vectors\n4\n7\n2\n0\n5\n3\n",
        "links": []
    },
    "./chunks/chunk_26_Lecture-8-Columbia.pdf": {
        "text": "8 Vectors\n4\n7\nNavigate\nSmall World\n(Constructing)\n6\n2\n1\n0\n5\n3\n",
        "links": []
    },
    "./chunks/chunk_27_Lecture-8-Columbia.pdf": {
        "text": "8 Vectors\n4\n7\nNavigate\nSmall World\n(Constructing)\n(6)\n2\n1\n0\n5\n3\n",
        "links": []
    },
    "./chunks/chunk_28_Lecture-8-Columbia.pdf": {
        "text": "8 Vectors\n4\n7\nNavigate\nSmall World\n(Constructing)\n6\n2\n1\n5\n3\n",
        "links": []
    },
    "./chunks/chunk_29_Lecture-8-Columbia.pdf": {
        "text": "8 Vectors\n4\n(7\nNavigate\nSmall World\n(Constructing)\n6\n2\n1\n0\n5\n3\n",
        "links": []
    },
    "./chunks/chunk_30_Lecture-8-Columbia.pdf": {
        "text": "8 Vectors\n4\n7\nNavigate\nSmall World\n(Constructing)\n6\n2\n1\n0\n5\n3\n",
        "links": []
    },
    "./chunks/chunk_31_Lecture-8-Columbia.pdf": {
        "text": "8 Vectors\n4\n7\nNavigate\nSmall World\n(Constructing)\n6\n2\n1\n0\n5\n3\n",
        "links": []
    },
    "./chunks/chunk_32_Lecture-8-Columbia.pdf": {
        "text": "Nearest Neighbor Vector\nNSW -\nSearch 1:\nNearest\nQuery\nVector\n4\n6\n2\nNeighbor\nQ\n1\n0\n5\n8 Vectors\n7\n3\nEntry Node\n",
        "links": []
    },
    "./chunks/chunk_33_Lecture-8-Columbia.pdf": {
        "text": "NSW -\nSearch\nQ\n8 Vectors\nQuery\nVector\n4\n7\n1\n6\n2\n0\n5\n3\nEntry Node\n",
        "links": []
    },
    "./chunks/chunk_34_Lecture-8-Columbia.pdf": {
        "text": "8 Vectors\nNSW -\nSearch\nQuery\nVector\n6\n4\n7\nQ\n1\n0\n5\n3\nEntry Node\n",
        "links": []
    },
    "./chunks/chunk_35_Lecture-8-Columbia.pdf": {
        "text": "Query\nNSW -\nSearch\nVector\n1\n8 Vectors\n4\n7\n6\n2\n0\n5\n3\nEntry Node\n",
        "links": []
    },
    "./chunks/chunk_36_Lecture-8-Columbia.pdf": {
        "text": "8 Vectors\n3\nNSW -\nSearch\nQuery\nVector\n4\n6\n2\n7\nEntry Node\n1\n0\n5\n",
        "links": []
    },
    "./chunks/chunk_37_Lecture-8-Columbia.pdf": {
        "text": "Nearest Neighbor Vector\nNSW -\nSearch\nQuery\nVector\n4\n6\n2\nQ\n1\n0\n5\n8 Vectors\n7\n3\nEntry Node\n",
        "links": []
    },
    "./chunks/chunk_38_Lecture-8-Columbia.pdf": {
        "text": "Nearest Neighbor Vector\nNSW - Search\n2\nQuery\nVector\n4\nApproximate\nNearest\nNeighbor\n2\n1\n0\n5\nEntry Node\n8 Vectors\n7\n3\n",
        "links": []
    },
    "./chunks/chunk_39_Lecture-8-Columbia.pdf": {
        "text": "NSW - Search\n2\nApproximate\nNearest\nNeighbor\nQuery\nVector\n4\n6\n2\n5\n1\n0\nEntry Node\n8 Vectors\n7\n3\n",
        "links": []
    },
    "./chunks/chunk_40_Lecture-8-Columbia.pdf": {
        "text": "NSW - Search\n2\nApproximate\nNearest\nNeighbor\nNearest Neighbor Vector\nQuery\nVector\n4\n2\n0\n5\nEntry Node\nApproximate Nearest Neighbor Vector\n8 Vectors\n7\n3\n",
        "links": []
    },
    "./chunks/chunk_41_Lecture-8-Columbia.pdf": {
        "text": "Hierarchical\nNavigable\nSmall World\n(HNSW) -\nSearch\nLayer 2\nLayer 1\nLayer 0\n18\n(15)\n(23)\n(13)\n25 Vectors\n4\n17\n2\n8\n(21\n12\n(19\n(11)\n(16)\n3\n(24)\nEntry\nNode\n1\n6\nQ\n10\n(14)\n-\n0\n22\n(20)\n5\n",
        "links": []
    },
    "./chunks/chunk_42_Lecture-8-Columbia.pdf": {
        "text": "Hierarchical\nNavigable\nSmall World\n(HNSW) -\nSearch\nLayer 2\nLayer 1\nLayer 0\n18\n(15)\n23\n(13)\n33\n21\n8\n2\n(17)\n19\n(11)\n16\n9\n22\n20\n25 Vectors\n4\n12\n24\n(14)\n(5\n",
        "links": []
    },
    "./chunks/chunk_43_Lecture-8-Columbia.pdf": {
        "text": "Layer 2\nLayer 1\nLayer 0\n18\n15\n23\n13\nHierarchical\nNavigable\nSmall World\n(HNSW) -\nSearch\n17\n25 Vectors\n21\n12\n8\nQuery Vector\n19\n11\n16\n6\n7\n24\nQ\n14\n22\n20\n9\n5\nApproximate NN\n",
        "links": []
    },
    "./chunks/chunk_44_Lecture-8-Columbia.pdf": {
        "text": "HNSW -\nConstruction\nVec 1\nVec 2\nVec 1 Max Layer\n-\nVec 2 - Max Layer\nVec 3\nVec 3 Max Layer\nRandom Generator\nVec N\nVec N - Max Layer\nMax Layer\n0\nNode is present at layer 0\nMax Layer\n1\nNode is present at layers 0,1\nMax Layer\n2\nNode is present at layers 0, 1, 2\nMax Layer\nM\nNode is present at layers 0, 1, 2, ...., M\n",
        "links": []
    },
    "./chunks/chunk_45_Lecture-8-Columbia.pdf": {
        "text": "HNSW\nRuntime\nLow likelihood in Higher Levels\n\u2022 The likelihood of a vector being found on the higher up levels\nof our HNSW graph goes down exponentially\n\u2022 Query time increases Logarithmically\nThis means that as the number of datapoints increases the\nnumber of comparisons to perform vector search only goes\nup logarithmically.\nO(log(N)) runtime complexity\n\u3002 As a results we have a O(log(N)) runtime complexity for the\nHNSW algorithm\nRuntime Complexity for ANN HNSW\nRuntime for a Query\n4\n6\n14\n12-\n10\n8\n2\n0.0\n0.2\n0.4\n0.6\nNumber of Data Points\n0.8\n1.0\n1e6\n",
        "links": []
    },
    "./chunks/chunk_46_Lecture-8-Columbia.pdf": {
        "text": "VectorDB\n\u2022\n\u2022\nA vector database is a specialized database designed to\nstore, manage, and query high-dimensional vector data\nefficiently\nOptimized for storing and retrieving vector\nembeddings, which are crucial for Al and machine\nlearning applications\nWhy Vector Databases are Useful for Dense Retrieval\nand RAG\n\u2022\n\u2022\nEfficient storage and retrieval of high-dimensional\ndata\nOptimized for similarity search operations\n\u2022\nScalability to handle large datasets\n\u2022\nLow-latency queries for real-time applications\n\u2022\nSeamless integration with Al and machine learning\nworkflows\n",
        "links": []
    },
    "./chunks/chunk_47_Lecture-8-Columbia.pdf": {
        "text": "On-prem (self-hosted)\nIn-memory only, single machine\nWeaviate\nchroma\ndrant\nLanceDB\nredis\nWeaviate\ndrant\n\u2023 milvus\nvespa\npgvector\nelasticsearch\nEmbedded\n(Serverless)\nzilliz\nLanceDB\nzilliz\nchroma\nPinecone\nclient-server\npgvector\nThrough various third-parties\ndrant\nWeaviate\nVald\nvespa\nredis\nelasticsearch\nCloud-native (managed)\n",
        "links": []
    },
    "./chunks/chunk_48_Lecture-8-Columbia.pdf": {
        "text": "Indexing\nHash-based\nTree-based\nGraph-based\nInverted file\nSpherical\nhashing\nSpectral\nhashing\nTrinary\nprojection\nLSH\ntrees\n(Locally\nHNSW\nsensitive\nDT-ST\nNGT\n(Neighbourhood Graph\nand Tree)\nScalable\nKNN graph\nIVMF\n(Inverted multi-\nindex file)\nIVF\n(Inverted file)\nhashing)\nAnnoy\nVamana\n",
        "links": []
    },
    "./chunks/chunk_49_Lecture-8-Columbia.pdf": {
        "text": "Sparse vs Dense Search\nDense Search (Semantic Search)\n0\nUses vector embedding representation of data to\nperform search.\n(as described in the previous lessons)\n\u3002 This types of search allows one to capture and return\nsemantically similar objects.\n\"Baby dogs\"\n\"Here is content on puppies!\"\n",
        "links": []
    },
    "./chunks/chunk_50_Lecture-8-Columbia.pdf": {
        "text": "Dense\nSearch\nChallenge\nProduct with a Serial Number\nSearching for seemingly random data\n(like serial numbers) will also yield poor results.\n\"BB43300\"\n\u201cErrrm... Bumble Bee?\"\n\u2022 String or Word Matching\n\u3002 Here we would be better off doing exact string or\nword matching to see where which product has the\nsame serial number as the input serial number.\nThis is known as keyword search or sparse search!\n",
        "links": []
    },
    "./chunks/chunk_51_Lecture-8-Columbia.pdf": {
        "text": "Sparse vs\nDense\nSearch\n\u2022 Bag of Words\n\u3002 The easiest way to do keyword matching is using Bag\nof Words to count how many times a word occurs in\nthe query and the data vector and then return objects\nwith the highest matching word frequencies.\n\u2022 This is known as Sparse Search\nbecause the text is embedded into vectors by\ncounting how many times every unique word in your\nvocabulary occurs in the query and stored sentences.\nMostly Zeroes (Sparse Embedding)\nSince the likelihood of any given sentence containing\nevery word in your vocabulary is quite low the\nembeddings is mostly zeroes and thus is known as a\nsparse embedding.\nRaw Text\nBag-of-words\nvector\nit is a puppy and it\nis extremely cute\n...\n9---9--ON\n",
        "links": []
    },
    "./chunks/chunk_52_Lecture-8-Columbia.pdf": {
        "text": "BM25\nBest Matching 25 (BM25): In practice when performing\nkeyword search we use a modification of simple word\nfrequencies called best matching 25\nscore (D, Q) = \u03a3 IDF (qi) \u00b7\ni=1\nf(qi, D) \u00b7 (k\u2081 + 1)\nf (qi, D) + k\u2081 \u00b7 (1 \u2212 b + b\u22c5\n|D\navgdl\nLearn more about BM25: https://en.wikipedia.org/wiki/Okapi_BM25\n\"If you can look into\nthe seeds of time,\nAnd say which\ngrain will grow and\nwhich will not.\u201d\n[0.,0.,0.,0.33052881,0..., 0.,0.20961694,0.,0.,., 0.,0.,0.,\n0.,.,0.,0.,0.20961694,0.20961694,.,0.20961694,0.209\n61694,0.,0.20961694,.20961694,0.20961694,0.,0.,0.,.\n,0.,0.,0.,0.20961694,0.,0.20961694,0.10938682,.,0.,0.\n20961694,0.41923388,0.41923388,0.,0.,0.20961694]\n",
        "links": [
            "https://en.wikipedia.org/wiki/Okapi_BM25"
        ]
    },
    "./chunks/chunk_53_Lecture-8-Columbia.pdf": {
        "text": "Hybrid\nSearch\nWhat is Hybrid Search?\n\u3002 Hybrid search is the process of performing both\nvector/dense search and keyword/sparse search and\nthen combining the results\nCombination based on a Scoring System\n\u3002 This combination can be done based on a scoring\nsystem that measures how well each objects matches\nthe query using both dense and sparse searches.\nSCORE\nRANK\nID\n1.9\n1\n8743\n1.6\n2\n5623\n1.5\n1.5\n3\n1965\n1.3\n4\n0998\n22\n1.2\n5\n7863\n1.1\n6\n4446\n0.8\n0.7\nD\nVector search BM25 search\n",
        "links": []
    },
    "./chunks/chunk_54_Lecture-8-Columbia.pdf": {
        "text": "Hybrid\nSearch\nQuery\nWho said, \"to be or not to be\"?\nLanguage\nKeyword\nSearch\nVector\nSearch\nModel\nOther signals\nco.embed()\nDocument #\nKeyword Search\nRelevance Score\nVector Search\nRelevance Score\nSignal #3 Score\n1\n2\n3\nText\nRelevance\nReranker\nLanguage\nModel\nco.rerank()\n",
        "links": []
    },
    "./chunks/chunk_55_Lecture-8-Columbia.pdf": {
        "text": "Query\nWhat is the capital of Canada?\nDense\nRetrieval is\nnot Perfect\nResponses\nThe capital of Canada is Ottawa\nToronto is in Canada\nThe capital of France is Paris\nThe capital of Canada is Sydney\nThe capital of Ontario is Toronto\nToronto is in Canada\nThe capital of France is Paris\nWhat is the capital of Canada?\nThe capital of Ontario is Toronto\nThe capital of Canada is Ottawa\nThe capital of Canada is Sydney\n",
        "links": []
    },
    "./chunks/chunk_56_Lecture-8-Columbia.pdf": {
        "text": "Query\nWhat is the capital of Canada?\nSolution:\nReRank\nTop Responses\nEurope is a continent\nThe capital of France is Paris\nThe grass is green\nThe sky is blue\nToronto is in Canada\nTomorrow is Sunday\nThe capital of Canada is Ottawa\nThe capital of Canada is Sydney\nMost apples are red\nThe capital of Ontario is Toronto\nReRank\nRelevance\nThe capital of France is Paris\nToronto is in Canada\nThe capital of Canada is Ottawa\nThe capital of Canada is Sydney\n0.2\n0.3\n0.9\n0.6\nThe capital of Ontario is Toronto\n0.5\n",
        "links": []
    },
    "./chunks/chunk_57_Lecture-8-Columbia.pdf": {
        "text": "ReRank is\ntrained on\nMany queries with correct answers:\nWhat is the capital of Canada?\nWhat is the capital of France?\nThe capital of Canada is Ottawa\nThe capital of France is Paris\nWhat color is the sky?\nThe sky is blue\nMany queries with wrong answers:\nWhat is the capital of Canada?\nToronto is in Canada\nWhat is the capital of France?\nThe capital of France is \u00cele-de-France\nWhat color is the sky?\nThe sky is red\n",
        "links": []
    },
    "./chunks/chunk_58_Lecture-8-Columbia.pdf": {
        "text": "Search can help LLMS in multiple ways\nWhat is the most viewed\ntelevised event?\nLarge\nLanguage\nModel\nRetrieval can help LLMs with:\n\u2022\nFactual information\nPrivate information\n\u2022 Updated information\n",
        "links": []
    },
    "./chunks/chunk_59_Lecture-8-Columbia.pdf": {
        "text": "Where is the\ninformation stored?\nLanguage\nWhat is the most viewed\ntelevised event?\nskills\nWorld\nInformation\nLarge Language Model\n",
        "links": []
    },
    "./chunks/chunk_60_Lecture-8-Columbia.pdf": {
        "text": "Search can add some context\nContext:\nFIFA World Cup\nSuperbowl\nQuestion:\nWhat is the most viewed televised\nevent?\nLanguage\nskills\nLarge Language Model\n",
        "links": []
    },
    "./chunks/chunk_61_Lecture-8-Columbia.pdf": {
        "text": "Generating\nAnswers\nQuery\nWhat is the most viewed\ntelevised event?\nJustthe LLM\nResponse\nGenerative\nModel\nWorld cup\nLLM powered by Semantic Search\nQuery\nWhat is the most\nviewed televised event?\nSearch\nModel\nGenerative\nModel\nResponse\nThe most viewed\ntelevised event\nin history was...\n= WIKIPEDIA\nSuper Bowl XXXVIII\n= WIKIPEDIA\n2010 FIFA World Cup\n",
        "links": []
    },
    "./chunks/chunk_62_Lecture-8-Columbia.pdf": {
        "text": "Function Call\nOpenAl has fine tuned GPT-3.5-turbo and GPT-4 models to\nAccept additional arguments through which users can pass in\ndescriptions of functions.\nIf it is relevant, return the name of the function to use, along with a JSON\nobject with the appropriate input parameters.\n\u2022 Notebook example:\nhttps://drive.google.com/file/d/1U-\nCb_ED3anGlfUvqm19ZKvVdBTBJb4KM/view?usp=sharing\n",
        "links": [
            "https://drive.google.com/file/d/1UCb_ED3anGlfUvqm19ZKvVdBTBJb4KM/view?usp=sharing"
        ]
    },
    "./chunks/chunk_63_Lecture-8-Columbia.pdf": {
        "text": "LangChain composes chains of components\nLCEL and the runnable protocol define:\nMeans of modifying\nparameters at\nruntime\nbind,...\nLangChain\nExpression\nLanguage\n(LCEL)\nprompt\nAn allowed set of\ninput types\n11m\nParser\nRequired\nAnd output\nmethods:\ntypes\ninvoke\nstream\nbatch\nComposition can now use the Linux pipe syntax:\nChain = prompt | 11m | OutputParser\n",
        "links": []
    },
    "./chunks/chunk_64_Lecture-8-Columbia.pdf": {
        "text": "Interface\nComponents implement \"Runnable\" protocol\n\u2022 Common methods include:\ninvoke [ainvoke]\no stream [astream]\no\nbatch [abatch]\n\u2022 Common properties\no input_schema, output_schema\n\u2022 Common 1/o\nComponent\nInput Type\nOutput Type\nPrompt\nDictionary\nPrompt Value\nRetriever\nSingle String\nList of Documents\nLLM\nString, list of messages or\nPrompt Value\nString\nChatModel\nString, list of messages or\nChatMessage\nTool\nPrompt Value\nString/Dictionary\nTool dependent\nOutput Parser\nOutput of LLM or ChatModel Parser dependent\n",
        "links": []
    },
    "./chunks/chunk_65_Lecture-8-Columbia.pdf": {
        "text": "LangChain\nExpression\nLanguage\n(LCEL)\n\u2022 Runnables support:\n\u25cf\nAsync, Batch and Streaming Support\n\u2022 Fallbacks\n\u2022 Parallelism\n\u2022\n\u2022 LLM calls can be time consuming\n\u2022\nAny components that can be run in parallel are!\nLogging is built in\n",
        "links": []
    },
    "./chunks/chunk_66_Lecture-8-Columbia.pdf": {
        "text": "Pydantic\nPydantic is a 'data validation library' for python.\n\u2022 Works with python type annotations. But\nrather than static type checking, they are\nactively used at runtime for data validation\nand conversion.\n\u2022 Provides built-in methods to\nserialize/deserialize models to/from JSON,\ndictionaries, etc.\n\u2022 LangChain leverages Pydantic to create JSON\nScheme describing function.\n",
        "links": []
    },
    "./chunks/chunk_67_Lecture-8-Columbia.pdf": {
        "text": "Pydantic\nclass pUser (BaseModel):\nname: str\nage: int\nemail: str\nfoo_p = pUser(name=\"Jane\", age=32, email=\"jane@gmail.com\")\nfoo_p.name\n'Jane'\nfoo_p=pUser(name=\"Jane\", age=\"bar\", email=\"jane@gmail.com\")\nValidationError\nst)\nCell In [20], line 1\nTraceback (most recent call la\n----> 1 foo_p = pUser (name=\"Jane\", age=\"bar\", email=\"jane@gmail.com\")\nFile ~/Documents/GitHub/SC-langchain-openai/.venv_LC3/lib/python3.9/site\n-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__()\nValidationError: 1 validation error for pUser\nage\nvalue is not a valid integer (type=type_error.integer)\n\u2191 \u5c71\nIn normal python you would create a class like\nthis:\nclass User:\ndef __init__(self, name: str, age: int, email: str):\nself.name = name\nself.age = age\nself.email = email\nUsing Pydantic, this is:\nfrom pydantic import BaseModel,\nclass User(BaseModel):\nname: str\nage: int\nemail: EmailStr\n",
        "links": []
    },
    "./chunks/chunk_68_Lecture-8-Columbia.pdf": {
        "text": "Pydantic with\nLangChain for\nOpenAl\nfunction calls\nUsing multiple functions\nEven better, we can pass a set of function and let the LLM decide which to use based\non the question context.\nclass ArtistSearch (BaseModel):\n\"Call this to get the names of songs by a particular artist\"\"\"\nartist_name: str = Field (description=\"name of artist to look up\")\nn: int = Field(description=\"number of results\")\nfunctions = [\n]\nconvert_pydantic_to_openai_function (WeatherSearch),\nconvert_pydantic_to_openai_function (ArtistSearch),\nmodel_with_functions = model.bind(functions=functions)\nmodel_with_functions.invoke(\"what is the weather in sf?\")\nAIMessage(content=\"', additional_kwargs={'function_call': {'name': 'Weat\nherSearch', 'arguments': '{\\n \"airport_code\": \"SFO\"\\n}'}}}\nmodel_with_functions.invoke(\"what are three songs by taylor swift?\")\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'Arti\nstSearch', 'arguments': '{\\n \"artist_name\": \"Taylor Swift\", \\n \"n\": 3\n\\n}'}})\n",
        "links": []
    },
    "./chunks/chunk_69_Lecture-8-Columbia.pdf": {
        "text": "Tagging and Extraction Using OpenAI\nfunctions\nimport os\nimport openai\nfrom dotenv import load_dotenv, find_dotenv\n=\nload_dotenv(find_dotenv()) # read local .env file\nopenai.api_key = os.environ['OPENAI_API_KEY']\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom langchain.utils.openai_functions import convert_pydantic_to_openai_f\nclass Tagging (BaseModel):\n\"\"\"Tag the piece of text with particular info.\"\"\nsentiment: str = Field (description=\"sentiment of text, should be `pos\nlanguage: str = Field(description=\"language of text (should be ISO 63\nconvert_pydantic_to_openai_function (Tagging)\n{'name': 'Tagging',\n'description': 'Tag the piece of text with particular info.',\n'parameters': {'title': 'Tagging',\n'description': 'Tag the piece of text with particular info.',\n'type': 'object',\n'properties': {'sentiment': {'title': 'Sentiment',\n'description': 'sentiment of text, should be `pos`, `neg`, or `neutr\nal'',\n'type': 'string'},\n'language': {'title': 'Language',\n'type': 'string'}},\n'required': ['sentiment', 'language']}}\n'description': 'language of text (should be ISO 639-1 code)',\nTagging\nWe have seen the LLM, given a function\ndescription, select arguments from the input\ntext generate a structured output forming a\nfunction call\n\u2022 More generally, the LLM can evaluate the\ninput text and generate structured output.\nLLM\nstructure\ndescription\n{\nsentiment: positive,\nlanguage: Spanish\n... }\n",
        "links": []
    },
    "./chunks/chunk_70_Lecture-8-Columbia.pdf": {
        "text": "Tvill Cangchain.prompts impor Chatri vilip li tip late\nfrom langchain.chat_models import ChatOpenAI\nmodel = ChatOpenAI (temperature=0)\ntagging_functions =\n[convert_pydantic_to_openai_function(Tagging)]\nprompt = ChatPromptTemplate.from_messages( [\n])\n(\"system\", \"Think carefully, and then tag the text as instructed\"),\n(\"user\", \"{input}\")\nmodel_with_functions\n= model.bind(\nfunctions=tagging_functions,\nfunction_call={\"name\": \"Tagging\"}\nTagging\nWe have seen the LLM, given a function\ndescription, select arguments from the input\ntext generate a structured output forming a\nfunction call\n\u2022 More generally, the LLM can evaluate the\ninput text and generate structured output.\n)\ntagging_chain = prompt | model_with_functions\ntagging_chain.invoke({\"input\": \"I love langchain\"})\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'Tagg\ning', 'arguments': '{\\n \"sentiment\": \"pos\", \\n \"language\":\"en\"\\n}'}}}\ntagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})\nAIMessage (content='', additional_kwargs={'function_call': {'name': 'Tagg\ning', 'arguments': '{\\n \"sentiment\": \"neg\",\\n \"language\": \"it\"\\n}'}}}\nLLM\nstructure\ndescription\n{\nsentiment: positive,\nlanguage: Spanish\n... }\n",
        "links": []
    },
    "./chunks/chunk_71_Lecture-8-Columbia.pdf": {
        "text": "Extraction\n\u2022 When given an input JSON schema, the LLM has\nbeen fine tuned to find and fill in the parameters\nof that schema.\n\u2022 The capability is not limited to function schema.\n\u2022 This can be used for general purpose extraction.\nLLM\nstructure\ndescription\n]\n[ { \u2026..\nfirst name: Lang,\nlast name: Chain,\nlanguage: Python\n... },\nExtraction\nExtraction is similar to tagging, but used for extracting multiple pieces of information.\nfrom typing import Optional\nclass Person (BaseModel):\n\"\"\"Information about a person.\nIIIIII\nname: str = Field (description=\"person's name\")\nage: Optional[int] = Field(description=\"person's age\")\nclass Information (BaseModel):\n\"\"\"Information to extract.\"\"\"\npeople: List [Person] =\nField(description=\"List of info about people\")\nconvert_pydantic_to_openai_function (Information)\n{'name': 'Information',\n'description': 'Information to extract.',\n'parameters': {'title': 'Information',\n'description': 'Information to extract.',\n'type': 'object',\n'properties': {'people': {'title': 'People',\n'description': 'List of info about people',\n'type': 'array',\n'items': {'title': 'Person',\n'description': 'Information about a person.',\n'type': 'object',\n'properties': {'name': {'title': 'Name',\n'description': \"person's name\",\n'type': 'string'},\n'age': {'title': 'Age',\n'description': \"person's age\",\n'type': 'integer'}},\n'required': ['name']}}},\n'required': ['people']}}\n",
        "links": []
    },
    "./chunks/chunk_72_Lecture-8-Columbia.pdf": {
        "text": "Extraction\n\u2022 When given an input JSON schema, the LLM has\nbeen fine tuned to find and fill in the parameters\nof that schema.\n\u2022 The capability is not limited to function schema.\n\u2022 This can be used for general purpose extraction.\n'type': 'object',\n'properties': {'name': {'title': 'Name',\n'description': \"person's name\",\n'type': 'string'},\n'age': {'title': 'Age',\n'description': \"person's age\",\n'type': 'integer'}},\n'required': ['name']}}},\n'required': ['people']}}\nextraction_functions = [convert_pydantic_to_openai_function (Information)]\nextraction_model = model.bind(functions=extraction_functions, function_ca\nextraction_model.invoke(\"Joe is 30, his mom is Martha\")\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'Info\n{\\n\nrmation', 'arguments': '{\\n \"people\": [\\n\n\"name\": \"Joe\", \\n\n\"age\": 30\\n },\\n {\\n \"name\": \"Martha\", \\n \"age\": 0\\n }\n\\n \\n}'}})\nLLM\nstructure\n[ { \u2026..\nfirst name: Lang,\nlast name: Chain,\nlanguage: Python\n... },\ndescription\n]\nprompt = ChatPromptTemplate.from_messages ( [\n(\"system\", \"Extract the relevant information, if not explicitly provi\n(\"human\", \"{input}\")\n1)\nextraction_chain = prompt extraction_model\nextraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'Info\nrmation', 'arguments': '{\\n \"people\": [\\n {\\n\n\"age\": 30\\n }, \\n {\\n\n\"name\": \"Martha\"\\n\n\"name\": \"Joe\", \\n\n}\\n \\n}'}})\nextraction_chain = prompt | extraction_model | JsonOutputFunctions Parser()\nextraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})\n",
        "links": []
    },
    "./chunks/chunk_73_Lecture-8-Columbia.pdf": {
        "text": "Tools and Routing\n\u2022\nFunctions and services an LLM can\nutilize to extend its capabilities are\nnamed \"tools\" in LangChain\nLangChain has many tools available\n\u2022 Search tools\nMath tools\nSQL tools\nimport requests\nfrom pydantic import BaseModel, Field\nimport datetime\n# Define the input schema\nclass OpenMeteo Input (BaseModel):\n+\n\u2191\u2193\u9996\nlatitude: float = Field (..., description=\"Latitude of the location to\nlongitude: float = Field (..., description=\"Longitude of the location\n@tool(args_schema-OpenMeteoInput)\ndef get_current_temperature (latitude: float, longitude: float) -> dict:\n\"\"\"Fetch current temperature for given coordinates.\"\"\"\nBASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n# Parameters for the request\nparams = {\n}\n'latitude': latitude,\n'longitude': longitude,\n'hourly': 'temperature_2m',\n'forecast_days': 1,\nI\n# Make the request\nresponse = requests.get(BASE_URL, params=params)\nif response.status_code\nelse:\n==\n200:\nresults = response.json()\nraise Exception (f\"API Request failed with status code: {response.\ncurrent_utc_time = datetime.datetime.utcnow()\ntime_list = [datetime.datetime.fromisoformat (time_str.replace('Z',\ntemperature_list = results['hourly'] ['temperature_2m']\n+\nclosest_time_index = min (range(len(time_list)), key-lambda i: abs(tim\ncurrent temperature\ntemperature listiclocest time indev1\n",
        "links": [
            "https://api.open-meteo.com/v1/forecast\""
        ]
    },
    "./chunks/chunk_74_Lecture-8-Columbia.pdf": {
        "text": "import wikipedia\n@tool\ndef search_wikipedia (query: str) -> str:\n\"\"\"Run Wikipedia search and get page summaries.\"\"\"\npage_titles wikipedia.search(query)\nsummaries = [ ]\nfor page_title in page_titles [: 3]:\ntry:\nwiki_page =\nwikipedia.page(title=page_title, auto_suggest=Fa\nfunctions = [\nformat_tool_to_openai_function (f) for f in [\nsearch_wikipedia, get_current_temperature\n]\nmodel = ChatOpenAI (temperature=0).bind(functions=functions)\nmodel.invoke(\"what is the weather in sf right now\")\nsummaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.su\nexcept (\nself.wiki_client.exceptions.PageError,\nself.wiki_client.exceptions. DisambiguationError,\n):\npass\nif not summaries:\nreturn \"No good Wikipedia Search Result was found\"\nreturn \"\\n\\n\".join(summaries)\nsearch_wikipedia name\n'search_wikipedia'\nsearch_wikipedia.description\n'search_wikipedia (query: str) -> str\nsummaries.'\n-\nRun Wikipedia search and get page\nformat_tool_to_openai_function(search_wikipedia)\n{'name': 'search_wikipedia',\n'description': 'search_wikipedia (query: str) -> str\nrch and get page summaries.',\n-\nRun Wikipedia sea\n'parameters': {'title': 'search_wikipediaSchemaSchema',\n'type': 'object',\n'properties': {'query': {'title': 'Query', 'type': 'string'}},\n\u0413\n\u05d5\u05e0\u05d9...\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'get_\ncurrent_temperature', 'arguments': '{\\n \"latitude\": 37.7749,\\n\nude\": -122.4194\\n}'}}}\nmodel.invoke(\"what is langchain\")\n\"longit\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'sear\nch_wikipedia', 'arguments': '{\\n \"query\": \"langchain\"\\n}'}}}\nfrom langchain.prompts import ChatPromptTemplate\nprompt = Chat Prompt Template.from_messages ( [\n(\"system\", \"You are helpful but sassy assistant\"),\n(\"user\", \"{input}\"),\n])\nchain prompt | model\nchain.invoke({\"input\": \"what is the weather in sf right now\"})\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'get_\ncurrent_temperature', 'arguments': '{\\n \"latitude\": 37.7749, \\n \"longit\nude\": -122.4194\\n}'}})\n\u2191\u2193\u66f1\n",
        "links": []
    },
    "./chunks/chunk_75_Lecture-8-Columbia.pdf": {
        "text": "chain.invoke({\"input\": \"what is the weather in sf right now\"})\nAIMessage(content='', additional_kwargs={'function_call': {'name': 'get_\ncurrent_temperature', 'arguments': '{\\n \"latitude\": 37.7749, \\n \"longit\nude\": -122.4194\\n}'}}}\nfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputPar\nchain =\nprompt | model | OpenAIFunctionsAgentOutputParser()\nresult = chain.invoke({\"input\": \"what is the weather in sf right now\"})\ntype (result)\nlangchain.schema.agent.AgentActionMessageLog\n\u03b9 \u03c3\u03ba\u03b9\u03bc\u03bf\u03b9. HIC CLU: IIUW Cali\naurel you Luuay: J\nfrom langchain.schema.agent import Agent Finish\ndef route(result):\nif isinstance(result, AgentFinish):\nelse:\nreturn result. return_values['output']\ntools = {\n}\n\"search_wikipedia\": search_wikipedia,\n\"get_current_temperature\": get_current_temperature,\nreturn tools [result.tool].run(result.tool_input)\nchain = prompt | model | OpenAIFunctionsAgentOutputParser() | route\nresult = chain.invoke({\"input\": \"What is the weather in san francisco rig\nresult.tool\n'get_current_temperature'\nresult.tool_input\n{'latitude': 37.7749, 'longitude': -122.4194}\nget_current_temperature(result.tool_input)\n'The current temperature is 22.9\u00b0C'\nresult\n'The current temperature is 22.9\u00b0C'\nresult = chain.invoke({\"input\": \"What is langchain?\"})\nresult\n'Page: LangChain Summary: LangChain is a framework designed to simplify\nthe creation of applications using large language models (LLMS). As a la\nnguage model integration framework, LangChain\\'s use-cases largely overl\nap with those of language models in general, including document analysis\nand summarization, chatbots, and code analysis. \\n\\nPage: Prompt engineer\ning\\nSummary: Prompt engineering is the process of structuring text that\ncan be interpreted and understood by a generative AI model. A prompt is\nnatural language text describing the task that an AI should perform. A pr\nompt for a text-to-text model can be a query such as \"what is Fermat\\'s\nlittle theorem?\", a command such as \"write a poem about leaves falling\",\na short statement of feedback (for example, \"too verbose\", \"too formal\",\n\"rephrase again\", \"omit this word\") or a longer statement including cont\n",
        "links": []
    },
    "./chunks/chunk_76_Lecture-8-Columbia.pdf": {
        "text": "Agent Basics\n\u25cf\nAgents\n\u2022 Are a combination of LLMs and code\nLLMs reason about what steps to take and call for actions.\nAgent loop\n\u25cf\n\u25cf\nChoose a tool to use\nObserve the output of the tool\nRepeat until a stopping condition is met\nStopping conditions can be:\n\u25cf\nLLM determined\nHardcoded rules\n",
        "links": []
    }
}