{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac77f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae8312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a919f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_clean_text(url):\n",
    "    \"\"\"\n",
    "    Fetches and cleans text from the given URL.\n",
    "    :param url: The URL to fetch text from.\n",
    "    :return: Cleaned text or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make an HTTP GET request\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract the main text content\n",
    "        # We can focus on specific tags (e.g., <p>, <div>) or use the whole text\n",
    "        text_elements = soup.find_all([\"p\", \"div\"])\n",
    "        text = \" \".join(element.get_text() for element in text_elements)\n",
    "        \n",
    "        # Clean the text\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "        text = text.strip()  # Remove leading/trailing whitespace\n",
    "        \n",
    "        # Handle empty text scenario\n",
    "        if not text:\n",
    "            return f\"Error: No extractable text found at {url}\"\n",
    "        return text\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle HTTP and connection errors\n",
    "        return f\"Error: Unable to fetch content from {url}. Exception: {e}\"\n",
    "    except Exception as e:\n",
    "        # Handle other unexpected errors\n",
    "        return f\"Error: Unexpected error while processing {url}. Exception: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f78bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_links(links):\n",
    "    \"\"\"\n",
    "    Processes a list of links, extracting and cleaning text content.\n",
    "    :param links: List of URLs.\n",
    "    :return: Dictionary with URLs as keys and cleaned text (or error messages) as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for url in links:\n",
    "        print(f\"Processing: {url}\")\n",
    "        text = fetch_and_clean_text(url)\n",
    "        results[url] = text\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18580894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://ai.stanford.edu/blog/longer-sequences-next-leap-ai/\n"
     ]
    }
   ],
   "source": [
    "links = [\n",
    "    'https://ai.stanford.edu/blog/longer-sequences-next-leap-ai/',\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Process the links\n",
    "extracted_data = process_links(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "114cab86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://ai.stanford.edu/blog/longer-sequences-next-leap-ai/': 'The Stanford AI Lab Blog About Posts All Conferences Computer Vision Robotics NLP Machine Learning Reinforcement Learning Subscribe SAIL The Stanford AI Lab Blog About Posts All Conferences Computer Vision Robotics NLP Machine Learning Reinforcement Learning Subscribe SAIL All Conferences Computer Vision Robotics NLP Machine Learning Reinforcement Learning Can Longer Sequences Help Take the Next Leap in AI? Chris Ré, Tri Dao, Dan Fu, Karan Goel June 9, 2022 Deep learning has revolutionized machine learning. To a first approximation, deeper has been better. However, there is another dimension to scale these models: the size of the input. Even the world’s most impressive models can only process long-form content by dismembering it into isolated, disconnected chunks of a few hundred words to fit their length requirements. There is a good reason: the ubiquitous Transformer model is an absolute wonder, but it is difficult to scale in sequence length for both performance and quality reasons. For complexity nerds, the Transformer’s runtime is quadratic in the input sequence length. As a result, Transformers get too expensive to train for long inputs. Also, the folklore is that these models can become unstable during training and struggle to learn long-range dependencies. Improvements on both of these dimensions are really exciting–and we aren’t alone thinking this. The great Long Range Arena benchmark from Google was created for exactly these reasons, and it has inspired a great deal of our current work. The goal of this blog post is to share why we are excited about this seemingly narrow topic of sequence length, which is sometimes only obvious to the subcommunity working on it–and point to some new work123 in these directions. Bridging new capabilities. Impoverished context means that most paragraphs–let alone books, plays, or instruction manuals, are difficult for modern models to understand, or even train on. Longer-range themes may be difficult or even impossible for models to pick up. During training, they can see isolated sentences from a variety of different sources with no connection at all. Hopefully, models with larger contexts could enable higher quality and new capabilities. For exciting new paradigms like in-context learning, we might be able to use larger contexts to enable foundation models to learn entirely new skills, just by feeding them an instruction manual! We might be able to generate entire stories instead of isolated posts. It’s possible that longer sequences could lead to entirely new skills the way in-context learning has emerged–we can condition on so much more information! In part, we just don’t know–and that’s why it’s exciting! Closing the reality gap. Sequence length is a limiter outside of just text processing. In computer vision, sequence length is intimately related to resolution. Not surprisingly, higher resolution in images can lead to better, more robust insights. The gap between today’s best models and the data we have access to is huge: computer vision is confined to resolutions that are 10 or 100x smaller than the default resolution of pictures from your iPhone or Android–let alone the much higher resolutions available from satellites, and medical imaging. In some ways, our current vision models see the world through thick, foggy glasses–they are amazing, but they might get much better! Multimodal models that mix text and images like DALL-E (2)45 and Imagen6 are some of the most exciting in AI! They can generate remarkable images from text descriptions and have sequence models at their core. What might these models do with even larger context? Opening new areas. There are huge application areas like time series, audio, and video where deep learning requires heavy manual hand engineering–or where classical, manual techniques are still preferred. We think a large part is because the data are naturally modeled as sequences of millions of steps, and today’s architectures cannot learn from this data automatically. We could enable entirely new training modalities, and we’ve started to work on things like the imaging of the brain (fMRI7 - sequences of high-resolution 3D brain images over time) and much more! What would a foundation model from fMRI data reveal about what we can learn? Can the machine learn directly from our telemetry? Maybe?! Who knows, it’s research. IoT devices generate orders of magnitude more data per year than the totality of the internet. Could machines learn in new and unexpected ways from this data? Do these structures transfer across machines? We’ve been thinking about observational supervision: as we type and interact with our machines, we generate digital exhaust. Could we learn from interaction at many different time scales to get something exciting? Pragmatically, we and other groups have observed that new methods for simply increasing the sequence length can already lead to improvements on benchmarks and applications. This has been invaluable to our work. One major task is Path-X8: given an image, decide if the image is of a path that is connected or not (at various resolutions and lengths). This task is challenging, and in the first two years of the benchmark, no model did better than random chance! In recent work, Tri Dao and Dan Fu created FlashAttention9, an IO-Aware exact Attention block with associated sparsity ideas that we used to learn from much longer sequences than previously possible [GitHub]. By fusing the attention kernel and not writing the intermediate attention matrix to GPU memory, FlashAttention reduces runtime by 2-4x and memory footprint by 10-20x. On the Path-X task in the Long Range Arena benchmark, all previous Transformers have had performance at chance (50%). FlashAttention showed that Transformers could perform better than chance (62%) simply by modeling longer sequences. For the past few years, Albert Gu and Karan Goel, along with many others, have been working on a new architecture called S410, which naturally enables training on much longer sequence lengths. S4 is based on classical signal processing ideas (structured state space models). The key insight is that modeling the underlying signal along with careful initialization can lead to much better performance on long sequences. These architectures have shown a remarkable ability – 20%+ better on LRA, and 96% on the Path-X task! See the Github for an overview of this work and applications to music generation, video, and more. This blog post was intended to share our excitement about this seemingly small issue of sequence length in deep learning models, and why we think it can both supercharge today’s text and image models–and equally importantly, open up entirely new vistas for deep learning. We’ve kept this short in the hopes our transformer friends can give us feedback on how they view these challenges, and where it’s exciting to see more scale. If you have exciting ideas for long-range sequence data, let us know! Tri Dao: trid@stanford.edu; Dan Fu: danfu@cs.stanford.edu; Karan Goel: krng@stanford.edu Gu, A., Dao, T., Ermon, S., Rudra, A., & Ré, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. Advances in Neural Information Processing Systems, 33, 1474-1487. ↩ Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., & Ré, C. (2021). Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers. Advances in Neural Information Processing Systems, 34. ↩ Goel, K., Gu, A., Donahue, C., & Ré, C. (2022). It’s Raw! Audio Generation with State-Space Models. arXiv preprint arXiv:2202.09729. ↩ Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., … & Sutskever, I. (2021, July). Zero-Shot Text-to-Image Generation. In International Conference on Machine Learning (pp. 8821-8831). PMLR. ↩ Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv preprint arXiv:2204.06125. ↩ Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … & Norouzi, M. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv preprint arXiv:2205.11487. ↩ Thomas, A. W., Ré, C., & Poldrack, R. A. (2021). Challenges for Cognitive Decoding Using Deep Learning Methods. arXiv preprint arXiv:2108.06896. ↩ Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., … & Metzler, D. (2020). Long Range Arena: A Benchmark for Efficient Transformers. arXiv preprint arXiv:2011.04006. ↩ Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv preprint arXiv:2205.14135. ↩ Gu, A., Goel, K., & Ré, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396. ↩ Keep on top of the latest SAIL Blog posts via RSS , Twitter , or email: Share Share on Facebook Tweet Add to Pocket Share on Reddit Email Tags ai artificial intelligence long sequences machine learning ml sequence learning sequence modeling sequences Previous post LinkBERT: Improving Language Model Training with Document Link Next post Stanford AI Lab Robotics Papers (ICRA and RSS 2022) Chris Ré, Tri Dao, Dan Fu, Karan Goel June 9, 2022 June 9, 2022 Deep learning has revolutionized machine learning. To a first approximation, deeper has been better. However, there is another dimension to scale these models: the size of the input. Even the world’s most impressive models can only process long-form content by dismembering it into isolated, disconnected chunks of a few hundred words to fit their length requirements. There is a good reason: the ubiquitous Transformer model is an absolute wonder, but it is difficult to scale in sequence length for both performance and quality reasons. For complexity nerds, the Transformer’s runtime is quadratic in the input sequence length. As a result, Transformers get too expensive to train for long inputs. Also, the folklore is that these models can become unstable during training and struggle to learn long-range dependencies. Improvements on both of these dimensions are really exciting–and we aren’t alone thinking this. The great Long Range Arena benchmark from Google was created for exactly these reasons, and it has inspired a great deal of our current work. The goal of this blog post is to share why we are excited about this seemingly narrow topic of sequence length, which is sometimes only obvious to the subcommunity working on it–and point to some new work123 in these directions. Bridging new capabilities. Impoverished context means that most paragraphs–let alone books, plays, or instruction manuals, are difficult for modern models to understand, or even train on. Longer-range themes may be difficult or even impossible for models to pick up. During training, they can see isolated sentences from a variety of different sources with no connection at all. Hopefully, models with larger contexts could enable higher quality and new capabilities. Closing the reality gap. Sequence length is a limiter outside of just text processing. In computer vision, sequence length is intimately related to resolution. Not surprisingly, higher resolution in images can lead to better, more robust insights. The gap between today’s best models and the data we have access to is huge: computer vision is confined to resolutions that are 10 or 100x smaller than the default resolution of pictures from your iPhone or Android–let alone the much higher resolutions available from satellites, and medical imaging. In some ways, our current vision models see the world through thick, foggy glasses–they are amazing, but they might get much better! Multimodal models that mix text and images like DALL-E (2)45 and Imagen6 are some of the most exciting in AI! They can generate remarkable images from text descriptions and have sequence models at their core. What might these models do with even larger context? Opening new areas. There are huge application areas like time series, audio, and video where deep learning requires heavy manual hand engineering–or where classical, manual techniques are still preferred. We think a large part is because the data are naturally modeled as sequences of millions of steps, and today’s architectures cannot learn from this data automatically. Pragmatically, we and other groups have observed that new methods for simply increasing the sequence length can already lead to improvements on benchmarks and applications. This has been invaluable to our work. In recent work, Tri Dao and Dan Fu created FlashAttention9, an IO-Aware exact Attention block with associated sparsity ideas that we used to learn from much longer sequences than previously possible [GitHub]. For the past few years, Albert Gu and Karan Goel, along with many others, have been working on a new architecture called S410, which naturally enables training on much longer sequence lengths. S4 is based on classical signal processing ideas (structured state space models). The key insight is that modeling the underlying signal along with careful initialization can lead to much better performance on long sequences. These architectures have shown a remarkable ability – 20%+ better on LRA, and 96% on the Path-X task! See the Github for an overview of this work and applications to music generation, video, and more. This blog post was intended to share our excitement about this seemingly small issue of sequence length in deep learning models, and why we think it can both supercharge today’s text and image models–and equally importantly, open up entirely new vistas for deep learning. We’ve kept this short in the hopes our transformer friends can give us feedback on how they view these challenges, and where it’s exciting to see more scale. If you have exciting ideas for long-range sequence data, let us know! Gu, A., Dao, T., Ermon, S., Rudra, A., & Ré, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. Advances in Neural Information Processing Systems, 33, 1474-1487. ↩ Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., & Ré, C. (2021). Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers. Advances in Neural Information Processing Systems, 34. ↩ Goel, K., Gu, A., Donahue, C., & Ré, C. (2022). It’s Raw! Audio Generation with State-Space Models. arXiv preprint arXiv:2202.09729. ↩ Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., … & Sutskever, I. (2021, July). Zero-Shot Text-to-Image Generation. In International Conference on Machine Learning (pp. 8821-8831). PMLR. ↩ Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv preprint arXiv:2204.06125. ↩ Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … & Norouzi, M. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv preprint arXiv:2205.11487. ↩ Thomas, A. W., Ré, C., & Poldrack, R. A. (2021). Challenges for Cognitive Decoding Using Deep Learning Methods. arXiv preprint arXiv:2108.06896. ↩ Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., … & Metzler, D. (2020). Long Range Arena: A Benchmark for Efficient Transformers. arXiv preprint arXiv:2011.04006. ↩ Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv preprint arXiv:2205.14135. ↩ Gu, A., Goel, K., & Ré, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396. ↩ Gu, A., Dao, T., Ermon, S., Rudra, A., & Ré, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. Advances in Neural Information Processing Systems, 33, 1474-1487. ↩ Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., & Ré, C. (2021). Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers. Advances in Neural Information Processing Systems, 34. ↩ Goel, K., Gu, A., Donahue, C., & Ré, C. (2022). It’s Raw! Audio Generation with State-Space Models. arXiv preprint arXiv:2202.09729. ↩ Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., … & Sutskever, I. (2021, July). Zero-Shot Text-to-Image Generation. In International Conference on Machine Learning (pp. 8821-8831). PMLR. ↩ Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv preprint arXiv:2204.06125. ↩ Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … & Norouzi, M. (2022). Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv preprint arXiv:2205.11487. ↩ Thomas, A. W., Ré, C., & Poldrack, R. A. (2021). Challenges for Cognitive Decoding Using Deep Learning Methods. arXiv preprint arXiv:2108.06896. ↩ Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., … & Metzler, D. (2020). Long Range Arena: A Benchmark for Efficient Transformers. arXiv preprint arXiv:2011.04006. ↩ Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv preprint arXiv:2205.14135. ↩ Gu, A., Goel, K., & Ré, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396. ↩ Share Share on Facebook Tweet Add to Pocket Share on Reddit Email Share Tags ai artificial intelligence long sequences machine learning ml sequence learning sequence modeling sequences Tags ai artificial intelligence long sequences machine learning ml sequence learning sequence modeling sequences Previous post LinkBERT: Improving Language Model Training with Document Link Next post Stanford AI Lab Robotics Papers (ICRA and RSS 2022) Previous post LinkBERT: Improving Language Model Training with Document Link Previous post Next post Stanford AI Lab Robotics Papers (ICRA and RSS 2022) Next post © 2021 Stanford AI Lab'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e38956",
   "metadata": {},
   "source": [
    "<h1>Prompting a QA dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"I am trying to create a dataset of quiz questions and answers I can use to fine-tune a model. I want you to create that set of 5 quiz questions and answers using the data I give you below\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here is the data I want you to make quiz questions and answers from: {text}.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please format the output as a list of python dictionaries where each dictionary represents one question answer pair. Here is an example of the structure [{'question':extracted question, 'answer':extracted answer}]\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Please return the format in a way that I can easily parse it into a python dictionary\"}\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
